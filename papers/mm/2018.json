{"0": {"title": "session details: ff-1.", "url": "https://dl.acm.org/citation.cfm?id=3286915", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "1": {"title": "scratch: a scalable discrete matrix factorization hashing for cross-modal retrieval.", "url": "https://doi.org/10.1145/3240508.3240547", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "2": {"title": "predicting visual context for unsupervised event segmentation in continuous photo-streams.", "url": "https://doi.org/10.1145/3240508.3240624", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "3": {"title": "video-to-video translation with global temporal consistency.", "url": "https://doi.org/10.1145/3240508.3240708", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "4": {"title": "shared linear encoder-based gaussian process latent variable model for visual classification.", "url": "https://doi.org/10.1145/3240508.3240520", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "5": {"title": "step-by-step erasion, one-by-one collection: a weakly supervised temporal action detector.", "url": "https://doi.org/10.1145/3240508.3240511", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "6": {"title": "multi-human parsing machines.", "url": "https://doi.org/10.1145/3240508.3240515", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "7": {"title": "fast parameter adaptation for few-shot image captioning and visual question answering.", "url": "https://doi.org/10.1145/3240508.3240527", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "8": {"title": "hierarchical memory modelling for video captioning.", "url": "https://doi.org/10.1145/3240508.3240538", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "9": {"title": "incremental deep hidden attribute learning.", "url": "https://doi.org/10.1145/3240508.3240510", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "10": {"title": "cropnet: real-time thumbnailing.", "url": "https://doi.org/10.1145/3240508.3240517", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "11": {"title": "learning to transfer: generalizable attribute learning with multitask neural model search.", "url": "https://doi.org/10.1145/3240508.3240518", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "12": {"title": "attention-based pyramid aggregation network for visual place recognition.", "url": "https://doi.org/10.1145/3240508.3240525", "abstract": "Visual place recognition is challenging in the urban environment and is usually viewed as a large sc\nale image retrieval task. The intrinsic challenges in place recognition exist that the confusing obj\nects such as cars and trees frequently occur in the complex urban scene, and buildings with repetiti\nve structures may cause over-counting and the burstiness problem degrading the image representations\n. To address these problems, we present an Attention-based Pyramid Aggregation Network (APANet), whi\nch is trained in an end-to-end manner for place recognition. One main component of APANet, the spati\nal pyramid pooling, can effectively encode the multi-size buildings containing geo-information. The \nother one, the attention block, is adopted as a region evaluator for suppressing the confusing regio\nnal features while highlighting the discriminative ones. When testing, we further propose a simple y\net effective PCA power whitening strategy, which significantly improves the widely used PCA whitenin\ng by reasonably limiting the impact of over-counting. Experimental evaluations demonstrate that the \nproposed APANet outperforms the state-of-the-art methods on two place recognition benchmarks, and ge\nneralizes well on standard image retrieval datasets.", "cite_num": 7, "conf": "mm", "time": "2018"}, "13": {"title": "semi-supervised deep generative modelling of incomplete multi-modality emotional data.", "url": "https://doi.org/10.1145/3240508.3240528", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "14": {"title": "twitter sentiment analysis via bi-sense emoji embedding and attention-based lstm.", "url": "https://doi.org/10.1145/3240508.3240533", "abstract": "Sentiment analysis on large-scale social media data is important to bridge the gaps between social m\nedia contents and real world activities including political election prediction, individual and publ\nic emotional status monitoring and analysis, and so on. Although textual sentiment analysis has been\n well studied based on platforms such as Twitter and Instagram, analysis of the role of extensive em\noji uses in sentiment analysis remains light. In this paper, we propose a novel scheme for Twitter s\nentiment analysis with extra attention on emojis. We first learn bi-sense emoji embeddings under pos\nitive and negative sentimental tweets individually, and then train a sentiment classifier by attendi\nng on these bi-sense emoji embeddings with an attention-based long short-term memory network (LSTM).\n Our experiments show that the bi-sense embedding is effective for extracting sentiment-aware embedd\nings of emojis and outperforms the state-of-the-art models. We also visualize the attentions to show\n that the bi-sense emoji embedding provides better guidance on the attention mechanism to obtain a m\nore robust understanding of the semantics and sentiments.", "cite_num": 3, "conf": "mm", "time": "2018"}, "15": {"title": "facial expression recognition in the wild: a cycle-consistent adversarial attention transfer approach.", "url": "https://doi.org/10.1145/3240508.3240574", "abstract": "Facial expression recognition (FER) is a very challenging problem due to different expressions under\n arbitrary poses. Most conventional approaches mainly perform FER under laboratory controlled enviro\nnment. Different from existing methods, in this paper, we formulate the FER in the wild as a domain \nadaptation problem, and propose a novel auxiliary domain guided Cycle-consistent adversarial Attenti\non Transfer model (CycleAT) for simultaneous facial image synthesis and facial expression recognitio\nn in the wild. The proposed model utilizes large-scale unlabeled web facial images as an auxiliary d\nomain to reduce the gap between source domain and target domain based on generative adversarial netw\norks (GAN) embedded with an effective attention transfer module, which enjoys several merits. First,\n the GAN-based method can automatically generate labeled facial images in the wild through harnessin\ng information from labeled facial images in source domain and unlabeled web facial images in auxilia\nry domain. Second, the class-discriminative spatial attention maps from the classifier in source dom\nain are leveraged to boost the performance of the classifier in target domain. Third, it can effecti\nvely preserve the structural consistency of local pixels and global attributes in the synthesized fa\ncial images through pixel cycle-consistency and discriminative loss. Quantitative and qualitative ev\naluations on two challenging in-the-wild datasets demonstrate that the proposed model performs favor\nably against state-of-the-art methods.", "cite_num": 0, "conf": "mm", "time": "2018"}, "16": {"title": "inferring user emotive state changes in realistic human-computer conversational dialogs.", "url": "https://doi.org/10.1145/3240508.3240575", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "17": {"title": "self-boosted gesture interactive system with st-net.", "url": "https://doi.org/10.1145/3240508.3240530", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "18": {"title": "slackliner - an interactive slackline training assistant.", "url": "https://doi.org/10.1145/3240508.3240537", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "19": {"title": "a unified generative adversarial framework for image generation and person re-identification.", "url": "https://doi.org/10.1145/3240508.3240573", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "20": {"title": "fov-aware edge caching for adaptive 360\u00b0 video streaming.", "url": "https://doi.org/10.1145/3240508.3240680", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "21": {"title": "session details: keynote 1.", "url": "https://dl.acm.org/citation.cfm?id=3286916", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "22": {"title": "don't just look - smell, taste, and feel the interaction.", "url": "https://doi.org/10.1145/3240508.3267343", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "23": {"title": "session details: ff-2.", "url": "https://dl.acm.org/citation.cfm?id=3286917", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "24": {"title": "style separation and synthesis via generative adversarial networks.", "url": "https://doi.org/10.1145/3240508.3240524", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "25": {"title": "group re-identification: leveraging and integrating multi-grain information.", "url": "https://doi.org/10.1145/3240508.3240539", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "26": {"title": "osmo: online specific models for occlusion in multiple object tracking under surveillance scene.", "url": "https://doi.org/10.1145/3240508.3240548", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "27": {"title": "video forecasting with forward-backward-net: delving deeper into spatiotemporal consistency.", "url": "https://doi.org/10.1145/3240508.3240551", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "28": {"title": "feature constrained by pixel: hierarchical adversarial deep domain adaptation.", "url": "https://doi.org/10.1145/3240508.3240562", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "29": {"title": "fast and light manifold cnn based 3d facial expression recognition across pose variations.", "url": "https://doi.org/10.1145/3240508.3240568", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "30": {"title": "explore multi-step reasoning in video question answering.", "url": "https://doi.org/10.1145/3240508.3240563", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "31": {"title": "attention and language ensemble for scene text recognition with convolutional sequence modeling.", "url": "https://doi.org/10.1145/3240508.3240571", "abstract": "Recent dominant approaches for scene text recognition are mainly based on convolutional neural netwo\nrk (CNN) and recurrent neural network (RNN), where the CNN processes images and the RNN generates ch\naracter sequences. Different from these methods, we propose an attention-based architecture1 which i\ns completely based on CNNs. The distinctive characteristics of our method include: (1) the method fo\nllows encoder-decoder architecture, in which the encoder is a two-dimensional residual CNN and the d\necoder is a deep one-dimensional CNN. (2) An attention module that captures visual cues, and a langu\nage module that models linguistic rules are designed equally in the decoder. Therefore the attention\n and language can be viewed as an ensemble to boost predictions jointly. (3) Instead of using a sing\nle loss from language aspect, multiple losses from attention and language are accumulated for traini\nng the networks in an end-to-end way. We conduct experiments on standard datasets for scene text rec\nognition, including Street View Text, IIIT5K and ICDAR datasets. The experimental results show our C\nNN-based method has achieved state-of-the-art performance on several benchmark datasets, even withou\nt the use of RNN.", "cite_num": 3, "conf": "mm", "time": "2018"}, "32": {"title": "temporal sequence distillation: towards few-frame action recognition in videos.", "url": "https://doi.org/10.1145/3240508.3240534", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "33": {"title": "previewer for multi-scale object detector.", "url": "https://doi.org/10.1145/3240508.3240544", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "34": {"title": "learning discriminative features with multiple granularities for person re-identification.", "url": "https://doi.org/10.1145/3240508.3240552", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "35": {"title": "stripnet: towards topology consistent strip structure segmentation.", "url": "https://doi.org/10.1145/3240508.3240553", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "36": {"title": "emotion recognition in speech using cross-modal transfer in the wild.", "url": "https://doi.org/10.1145/3240508.3240578", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "37": {"title": "personalized multiple facial action unit recognition through generative adversarial recognition network.", "url": "https://doi.org/10.1145/3240508.3240613", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "38": {"title": "investigation of small group social interactions using deep visual activity-based nonverbal features.", "url": "https://doi.org/10.1145/3240508.3240685", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "39": {"title": "cross-species learning: a low-cost approach to learning human fight from animal fight.", "url": "https://doi.org/10.1145/3240508.3240710", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "40": {"title": "personalized serious games for cognitive intervention with lifelog visual analytics.", "url": "https://doi.org/10.1145/3240508.3240598", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "41": {"title": "drawing in a virtual 3d space - introducing vr drawing in elementary school art education.", "url": "https://doi.org/10.1145/3240508.3240692", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "42": {"title": "circe: real-time caching for instance recognition on cloud environments and multi-core architectures.", "url": "https://doi.org/10.1145/3240508.3240697", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "43": {"title": "jaguar: low latency mobile augmented reality with flexible tracking.", "url": "https://doi.org/10.1145/3240508.3240561", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "44": {"title": "session details: keynote 2.", "url": "https://dl.acm.org/citation.cfm?id=3286918", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "45": {"title": "challenges and practices of large scale visual intelligence in the real-world.", "url": "https://doi.org/10.1145/3240508.3267342", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "46": {"title": "session details: deep-1 (image translation).", "url": "https://dl.acm.org/citation.cfm?id=3286919", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "47": {"title": "structure guided photorealistic style transfer.", "url": "https://doi.org/10.1145/3240508.3240637", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "48": {"title": "crossing-domain generative adversarial networks for unsupervised multi-domain image-to-image translation.", "url": "https://doi.org/10.1145/3240508.3240716", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "49": {"title": "multi-view image generation from a single-view.", "url": "https://doi.org/10.1145/3240508.3240536", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "50": {"title": "sparsely grouped multi-task generative adversarial networks for facial attribute manipulation.", "url": "https://doi.org/10.1145/3240508.3240594", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "51": {"title": "session details: vision-1 (machine learning).", "url": "https://dl.acm.org/citation.cfm?id=3286920", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "52": {"title": "visual domain adaptation with manifold embedded distribution alignment.", "url": "https://doi.org/10.1145/3240508.3240512", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "53": {"title": "causally regularized learning with agnostic data selection bias.", "url": "https://doi.org/10.1145/3240508.3240577", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "54": {"title": "robust correlation filter tracking with shepherded instance-aware proposals.", "url": "https://doi.org/10.1145/3240508.3240709", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "55": {"title": "a unified framework for multimodal domain adaptation.", "url": "https://doi.org/10.1145/3240508.3240633", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "56": {"title": "session details: multimedia-1 (multimedia recommendation & discovery).", "url": "https://dl.acm.org/citation.cfm?id=3286921", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "57": {"title": "what dress fits me best?: fashion recommendation on the clothing style for personal body shape.", "url": "https://doi.org/10.1145/3240508.3240546", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "58": {"title": "csan: contextual self-attention network for user sequential recommendation.", "url": "https://doi.org/10.1145/3240508.3240609", "abstract": "The sequential recommendation is an important task for online user-oriented services, such as purcha\nsing products, watching videos, and social media consumption. Recent work usually used RNN-based met\nhods to derive an overall embedding of the whole behavior sequence, which fails to discriminate the \nsignificance of individual user behaviors and thus decreases the recommendation performance. Besides\n, RNN-based encoding has fixed size and makes further recommendation application inefficient and inf\nlexible. The online sequential behaviors of a user are generally heterogeneous, polysemous, and dyna\nmically context-dependent. In this paper, we propose a unified Contextual Self-Attention Network (CS\nAN) to address the three properties. Heterogeneous user behaviors are considered in our model that a\nre projected into a common latent semantic space. Then the output is fed into the feature-wise self-\nattention network to capture the polysemy of user behaviors. In addition, the forward and backward p\nosition encoding matrices are proposed to model dynamic contextual dependency. Through extensive exp\neriments on two real-world datasets, we demonstrate the superior performance of the proposed model c\nompared with other state-of-the-art algorithms.", "cite_num": 2, "conf": "mm", "time": "2018"}, "59": {"title": "attentive interactive convolutional matching for community question answering in social multimedia.", "url": "https://doi.org/10.1145/3240508.3240626", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "60": {"title": "beyond the product: discovering image posts for brands in social media.", "url": "https://doi.org/10.1145/3240508.3240689", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "61": {"title": "session details: vision-2 (object & scene understanding).", "url": "https://dl.acm.org/citation.cfm?id=3286922", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "62": {"title": "collaborative annotation of semantic objects in images with multi-granularity supervisions.", "url": "https://doi.org/10.1145/3240508.3240540", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "63": {"title": "graphnet: learning image pseudo annotations for weakly-supervised semantic segmentation.", "url": "https://doi.org/10.1145/3240508.3240542", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "64": {"title": "boosting scene parsing performance via reliable scale prediction.", "url": "https://doi.org/10.1145/3240508.3240657", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "65": {"title": "learning to synthesize 3d indoor scenes from monocular images.", "url": "https://doi.org/10.1145/3240508.3240700", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "66": {"title": "session details: multimodal-1 (multimodal reasoning).", "url": "https://dl.acm.org/citation.cfm?id=3286923", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "67": {"title": "visual spatial attention network for relationship detection.", "url": "https://doi.org/10.1145/3240508.3240611", "abstract": "Visual relationship detection, which aims to predict a   triplet with the detected objects, has attr\nacted increasing attention in the scene understanding study. During tackling this problem, dealing w\nith varying scales of the subjects and objects is of great importance, which has been less studied. \nTo overcome this challenge, we propose a novel Vision Spatial Attention Network (VSA-Net), which emp\nloys a two-dimensional normal distribution attention scheme to effectively model small objects. In a\nddition, we design a Subject-Object-layer (SO-layer) to distinguish between the subject and object t\no attain more precise results. To the best of our knowledge, VSA-Net is the first end-to-end attenti\non mechanism based visual relationship detection model. Extensive experiments on the benchmark datas\nets (VRD and VG) show that, by using pure vision information, our VSA-Net achieves state-of-the-art \nperformance for predicate detection, phrase detection, and relationship detection.", "cite_num": 2, "conf": "mm", "time": "2018"}, "68": {"title": "object-difference attention: a simple relational attention for visual question answering.", "url": "https://doi.org/10.1145/3240508.3240513", "abstract": "Attention mechanism has greatly promoted the development of Visual Question Answering (VQA). Attenti\non distribution, which weights differently on objects (such as image regions or bounding boxes) in a\nn image according to their importance for answering a question, plays a crucial role in attention me\nchanism. Most of the existing work focuses on fusing image features and text features to calculate t\nhe attention distribution without comparisons between different image objects. As a major property o\nf attention, selectivity depends on comparisons between different objects. Comparisons provide more \ninformation for assigning attentions better. For achieving this, we propose an object-difference att\nention (ODA) which calculates the probability of attention by implementing difference operator betwe\nen different image objects in an image under the guidance of questions in hand. Experimental results\n on three publicly available datasets show our ODA based VQA model achieves the state-of-the-art res\nults. Furthermore, a general form of relational attention is proposed. Besides ODA, several other re\nlational attentions are given. Experimental results show those relational attentions have strengths \non different types of questions.", "cite_num": 2, "conf": "mm", "time": "2018"}, "69": {"title": "life-long cross-media correlation learning.", "url": "https://doi.org/10.1145/3240508.3240558", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "70": {"title": "human conversation analysis using attentive multimodal networks with hierarchical encoder-decoder.", "url": "https://doi.org/10.1145/3240508.3240714", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "71": {"title": "session details: system-1 (video analysis & streaming).", "url": "https://dl.acm.org/citation.cfm?id=3286924", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "72": {"title": "end-to-end blind quality assessment of compressed videos using deep neural networks.", "url": "https://doi.org/10.1145/3240508.3240643", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "73": {"title": "flexstream: towards flexible adaptive video streaming on end devices using extreme sdn.", "url": "https://doi.org/10.1145/3240508.3240676", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "74": {"title": "cls: a cross-user learning based system for improving qoe in 360-degree video adaptive streaming.", "url": "https://doi.org/10.1145/3240508.3240556", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "75": {"title": "a distributed approach for bitrate selection in http adaptive streaming.", "url": "https://doi.org/10.1145/3240508.3240589", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "76": {"title": "session details: ff-3.", "url": "https://dl.acm.org/citation.cfm?id=3286925", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "77": {"title": "high-quality exposure correction of underexposed photos.", "url": "https://doi.org/10.1145/3240508.3240595", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "78": {"title": "a margin-based mle for crowdsourced partial ranking.", "url": "https://doi.org/10.1145/3240508.3240597", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "79": {"title": "phd-gifs: personalized highlight detection for automatic gif creation.", "url": "https://doi.org/10.1145/3240508.3240599", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "80": {"title": "cross-domain adversarial feature learning for sketch re-identification.", "url": "https://doi.org/10.1145/3240508.3240606", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "81": {"title": "semantic human matting.", "url": "https://doi.org/10.1145/3240508.3240610", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "82": {"title": "geometry guided adversarial facial expression synthesis.", "url": "https://doi.org/10.1145/3240508.3240612", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "83": {"title": "detecting abnormality without knowing normality: a two-stage approach for unsupervised video abnormal event detection.", "url": "https://doi.org/10.1145/3240508.3240615", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "84": {"title": "beautygan: instance-level facial makeup transfer with deep generative adversarial network.", "url": "https://doi.org/10.1145/3240508.3240618", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "85": {"title": "trusted guidance pyramid network for human parsing.", "url": "https://doi.org/10.1145/3240508.3240634", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "86": {"title": "i read, i saw, i tell: texts assisted fine-grained visual classification.", "url": "https://doi.org/10.1145/3240508.3240579", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "87": {"title": "look deeper see richer: depth-aware image paragraph captioning.", "url": "https://doi.org/10.1145/3240508.3240583", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "88": {"title": "learning multimodal taxonomy via variational deep graph embedding and clustering.", "url": "https://doi.org/10.1145/3240508.3240586", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "89": {"title": "watch, think and attend: end-to-end video classification via dynamic knowledge evolution modeling.", "url": "https://doi.org/10.1145/3240508.3240566", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "90": {"title": "multi-label image classification via knowledge distillation from weakly-supervised detection.", "url": "https://doi.org/10.1145/3240508.3240567", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "91": {"title": "unregularized auto-encoder with generative adversarial networks for image generation.", "url": "https://doi.org/10.1145/3240508.3240569", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "92": {"title": "when to learn what: deep cognitive subspace clustering.", "url": "https://doi.org/10.1145/3240508.3240582", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "93": {"title": "depth structure preserving scene image generation.", "url": "https://doi.org/10.1145/3240508.3240584", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "94": {"title": "ca3net: contextual-attentional attribute-appearance network for person re-identification.", "url": "https://doi.org/10.1145/3240508.3240585", "abstract": "Person re-identification aims to identify the same pedestrian across non-overlapping camera views. D\neep learning techniques have been applied for person re-identification recently, towards learning re\npresentation of pedestrian appearance. This paper presents a novel Contextual-Attentional Attribute-\nAppearance Network (CA3Net) for person re-identification. The CA3Net simultaneously exploits the com\nplementarity between semantic attributes and visual appearance, the semantic context among attribute\ns, visual attention on attributes as well as spatial dependencies among body parts, leading to discr\niminative and robust pedestrian representation. Specifically, an attribute network within CA3Net is \ndesigned with an Attention-LSTM module. It concentrates the network on latent image regions related \nto each attribute as well as exploits the semantic context among attributes by a LSTM module. An app\nearance network is developed to learn appearance features from the full body, horizontal and vertica\nl body parts of pedestrians with spatial dependencies among body parts. The CA3Net jointly learns th\ne attribute and appearance features in a multi-task learning manner, generating comprehensive repres\nentation of pedestrians. Extensive experiments on two challenging benchmarks, i.e., Market-1501 and \nDukeMTMC-reID datasets, have demonstrated the effectiveness of the proposed approach.", "cite_num": 6, "conf": "mm", "time": "2018"}, "95": {"title": "rgcnn: regularized graph cnn for point cloud segmentation.", "url": "https://doi.org/10.1145/3240508.3240621", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "96": {"title": "deep triplet quantization.", "url": "https://doi.org/10.1145/3240508.3240516", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "97": {"title": "session details: keynote 3.", "url": "https://dl.acm.org/citation.cfm?id=3286926", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "98": {"title": "what has art got to do with it?", "url": "https://doi.org/10.1145/3240508.3267345", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "99": {"title": "session details: best paper session.", "url": "https://dl.acm.org/citation.cfm?id=3286927", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "100": {"title": "gesturegan for hand gesture-to-gesture translation in the wild.", "url": "https://doi.org/10.1145/3240508.3240704", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "101": {"title": "beyond narrative description: generating poetry from images by multi-adversarial training.", "url": "https://doi.org/10.1145/3240508.3240587", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "102": {"title": "understanding humans in crowded scenes: deep nested adversarial learning and a new benchmark for multi-human parsing.", "url": "https://doi.org/10.1145/3240508.3240509", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "103": {"title": "knowledge-aware multimodal dialogue systems.", "url": "https://doi.org/10.1145/3240508.3240605", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "104": {"title": "session details: doctoral symposium.", "url": "https://dl.acm.org/citation.cfm?id=3286928", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "105": {"title": "end2end semantic segmentation for 3d indoor scenes.", "url": "https://doi.org/10.1145/3240508.3243933", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "106": {"title": "on reducing effort in evaluating laparoscopic skills.", "url": "https://doi.org/10.1145/3240508.3243934", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "107": {"title": "decode human life from social media.", "url": "https://doi.org/10.1145/3240508.3243935", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "108": {"title": "session details: ff-4.", "url": "https://dl.acm.org/citation.cfm?id=3286929", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "109": {"title": "learning semantic structure-preserved embeddings for cross-modal retrieval.", "url": "https://doi.org/10.1145/3240508.3240521", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "110": {"title": "post tuned hashing: a new approach to indexing high-dimensional data.", "url": "https://doi.org/10.1145/3240508.3240529", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "111": {"title": "cross-modal moment localization in videos.", "url": "https://doi.org/10.1145/3240508.3240549", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "112": {"title": "multi-scale correlation for sequential cross-modal hashing learning.", "url": "https://doi.org/10.1145/3240508.3240560", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "113": {"title": "generative adversarial product quantisation.", "url": "https://doi.org/10.1145/3240508.3240590", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "114": {"title": "aesthetic-driven image enhancement by adversarial learning.", "url": "https://doi.org/10.1145/3240508.3240531", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "115": {"title": "attention-based multi-patch aggregation for image aesthetic assessment.", "url": "https://doi.org/10.1145/3240508.3240554", "abstract": "Aggregation structures with explicit information, such as image attributes and scene semantics, are \neffective and popular for intelligent systems for assessing aesthetics of visual data. However, usef\nul information may not be available due to the high cost of manual annotation and expert design. In \nthis paper, we present a novel multi-patch (MP) aggregation method for image aesthetic assessment. D\nifferent from state-of-the-art methods, which augment an MP aggregation network with various visual \nattributes, we train the model in an end-to-end manner with aesthetic labels only (i.e., aesthetical\nly positive or negative). We achieve the goal by resorting to an attention-based mechanism that adap\ntively adjusts the weight of each patch during the training process to improve learning efficiency. \nIn addition, we propose a set of objectives with three typical attention mechanisms (i.e., average, \nminimum, and adaptive) and evaluate their effectiveness on the Aesthetic Visual Analysis (AVA) bench\nmark. Numerical results show that our approach outperforms existing methods by a large margin. We fu\nrther verify the effectiveness of the proposed attention-based objectives via ablation studies and s\nhed light on the design of aesthetic assessment systems.", "cite_num": 2, "conf": "mm", "time": "2018"}, "116": {"title": "an end-to-end quadrilateral regression network for comic panel extraction.", "url": "https://doi.org/10.1145/3240508.3240555", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "117": {"title": "monocular camera based real-time dense mapping using generative adversarial network.", "url": "https://doi.org/10.1145/3240508.3240564", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "118": {"title": "jpeg decompression in the homomorphic encryption domain.", "url": "https://doi.org/10.1145/3240508.3240672", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "119": {"title": "miniview layout for bandwidth-efficient 360-degree video.", "url": "https://doi.org/10.1145/3240508.3240705", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "120": {"title": "real-time 3d face-eye performance capture of a person wearing vr headset.", "url": "https://doi.org/10.1145/3240508.3240570", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "121": {"title": "bridge the gap between vqa and human behavior on omnidirectional video: a large-scale dataset and a deep learning model.", "url": "https://doi.org/10.1145/3240508.3240581", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "122": {"title": "tracking-assisted weakly supervised online visual object segmentation in unconstrained videos.", "url": "https://doi.org/10.1145/3240508.3240638", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "123": {"title": "thoughtviz: visualizing human thoughts using generative adversarial network.", "url": "https://doi.org/10.1145/3240508.3240641", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "124": {"title": "a feature-adaptive semi-supervised framework for co-saliency detection.", "url": "https://doi.org/10.1145/3240508.3240648", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "125": {"title": "ispa-net: iterative semantic pose alignment network.", "url": "https://doi.org/10.1145/3240508.3240650", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "126": {"title": "extractive video summarizer with memory augmented neural networks.", "url": "https://doi.org/10.1145/3240508.3240651", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "127": {"title": "fully point-wise convolutional neural network for modeling statistical regularities in natural images.", "url": "https://doi.org/10.1145/3240508.3240653", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "128": {"title": "online action tube detection via resolving the spatio-temporal context pattern.", "url": "https://doi.org/10.1145/3240508.3240659", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "129": {"title": "enhancing visual question answering using dropout.", "url": "https://doi.org/10.1145/3240508.3240662", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "130": {"title": "face-voice matching using cross-modal embeddings.", "url": "https://doi.org/10.1145/3240508.3240601", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "131": {"title": "deep understanding of cooking procedure for cross-modal recipe retrieval.", "url": "https://doi.org/10.1145/3240508.3240627", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "132": {"title": "decoupled novel object captioner.", "url": "https://doi.org/10.1145/3240508.3240640", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "133": {"title": "temporal cross-media retrieval with soft-smoothing.", "url": "https://doi.org/10.1145/3240508.3240665", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "134": {"title": "photo squarization by deep multi-operator retargeting.", "url": "https://doi.org/10.1145/3240508.3240623", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "135": {"title": "non-locally enhanced encoder-decoder network for single image de-raining.", "url": "https://doi.org/10.1145/3240508.3240636", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "136": {"title": "an admm-based universal framework for adversarial attacks on deep neural networks.", "url": "https://doi.org/10.1145/3240508.3240639", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "137": {"title": "local convolutional neural networks for person re-identification.", "url": "https://doi.org/10.1145/3240508.3240645", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "138": {"title": "conditional expression synthesis with face parsing transformation.", "url": "https://doi.org/10.1145/3240508.3240647", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "139": {"title": "attentive recurrent neural network for weak-supervised multi-label image classification.", "url": "https://doi.org/10.1145/3240508.3240649", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "140": {"title": "deep cross modal learning for caricature verification and identification (cavinet).", "url": "https://doi.org/10.1145/3240508.3240658", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "141": {"title": "few-shot adaptation for multimedia semantic indexing.", "url": "https://doi.org/10.1145/3240508.3240592", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "142": {"title": "fashion sensitive clothing recommendation using hierarchical collocation model.", "url": "https://doi.org/10.1145/3240508.3240596", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "143": {"title": "multi-scale context attention network for image retrieval.", "url": "https://doi.org/10.1145/3240508.3240602", "abstract": "Recent attempts on the Convolutional Neural Network (CNN) based image retrieval usually adopt the ou\ntput of a specific convolutional or fully connected layer as feature representation. Though superior\n representation capability has yielded better retrieval performance, the scale variation and clutter\n distracting remain to be two challenging problems in CNN based image retrieval. In this work, we pr\nopose a Multi-Scale Context Attention Network (MSCAN) to generate global descriptors, which is able \nto selectively focus on the informative regions with the assistance of multi-scale context informati\non. We model the multi-scale context information by an improved Long Short-Term Memory (LSTM) networ\nk across different layers. As such, the proposed global descriptor is equipped with the scale aware \nattention capability. Experimental results show that our proposed method can effectively capture the\n informative regions in images and retain reliable attention responses when encountering scale varia\ntion and clutter distracting. Moreover, we compare the performance of the proposed scheme with the s\ntate-of-the-art global descriptors, and extensive results verify that the proposed MSCAN can achieve\n superior performance on several image retrieval benchmarks.", "cite_num": 0, "conf": "mm", "time": "2018"}, "144": {"title": "comprehensive distance-preserving autoencoders for cross-modal retrieval.", "url": "https://doi.org/10.1145/3240508.3240607", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "145": {"title": "temporal hierarchical attention at category- and item-level for micro-video click-through prediction.", "url": "https://doi.org/10.1145/3240508.3240617", "abstract": "... but we have temporarily restricted your access to the Digital Library.\nYour activity appears to \nbe coming from some type of automated process.\nTo ensure the availability of the Digital Library we \ncan not allow these types of requests to continue.\nThe restriction will be removed automatically onc\ne this activity stops.\n", "cite_num": 0, "conf": "mm", "time": "2018"}, "146": {"title": "historical context-based style classification of painting images via label distribution learning.", "url": "https://doi.org/10.1145/3240508.3240593", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "147": {"title": "direction-aware neural style transfer.", "url": "https://doi.org/10.1145/3240508.3240629", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "148": {"title": "chipgan: a generative adversarial network for chinese ink wash painting style transfer.", "url": "https://doi.org/10.1145/3240508.3240655", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "149": {"title": "cloudvr: cloud accelerated interactive mobile virtual reality.", "url": "https://doi.org/10.1145/3240508.3240620", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "150": {"title": "your attention is unique: detecting 360-degree video saliency in head-mounted display for head movement prediction.", "url": "https://doi.org/10.1145/3240508.3240669", "abstract": "Head movement prediction is the key enabler for the emerging 360-degree videos since it can enhance \nboth streaming and rendering efficiency. To achieve accurate head movement prediction, it becomes im\nperative to understand user's visual attention on 360-degree videos under head-mounted display (HMD)\n. Despite the rich history of saliency detection research, we observe that traditional models are de\nsigned for regular images/videos fixed at a single viewport and would introduce problems such as cen\ntral bias and multi-object confusion when applied to the multi-viewport 360-degree videos switched b\ny user interaction. To fill in this gap, this paper shifts the traditional single-viewport saliency \nmodels that have been extensively studied for decades to a fresh panoramic saliency detection specif\nically tailored for 360-degree videos, and thus maximally enhances the head movement prediction perf\normance. The proposed head movement prediction framework is empowered by a newly created dataset for\n 360-degree video saliency, a panoramic saliency detection model and an integration of saliency and \nhead tracking history for the ultimate head movement prediction. Experimental results demonstrate th\ne measurable gain of both the proposed panoramic saliency detection and head movement prediction ove\nr traditional models for regular images/videos.", "cite_num": 8, "conf": "mm", "time": "2018"}, "151": {"title": "hybrid point cloud attribute compression using slice-based layered structure and block-based intra prediction.", "url": "https://doi.org/10.1145/3240508.3240696", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "152": {"title": "qarc: video quality aware rate control for real-time video streaming based on deep reinforcement learning.", "url": "https://doi.org/10.1145/3240508.3240545", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "153": {"title": "optimizing personalized interaction experience in crowd-interactive livecast: a cloud-edge approach.", "url": "https://doi.org/10.1145/3240508.3240642", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "154": {"title": "session details: demo + video + makers' program.", "url": "https://dl.acm.org/citation.cfm?id=3286930", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "155": {"title": "give me one portrait image, i will tell you your emotion and personality.", "url": "https://doi.org/10.1145/3240508.3241384", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "156": {"title": "demo: phase-based acoustic localization and motion tracking for mobile interaction.", "url": "https://doi.org/10.1145/3240508.3241385", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "157": {"title": "ai painting: an aesthetic painting generation system.", "url": "https://doi.org/10.1145/3240508.3241386", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "158": {"title": "somin.ai: social multimedia influencer discovery marketplace.", "url": "https://doi.org/10.1145/3240508.3241387", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "159": {"title": "anidance: real-time dance motion synthesize to the song.", "url": "https://doi.org/10.1145/3240508.3241388", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "160": {"title": "artsight: an artistic data exploration engine.", "url": "https://doi.org/10.1145/3240508.3241389", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "161": {"title": "meet ar-bot: meeting anywhere, anytime with movable spatial ar robot.", "url": "https://doi.org/10.1145/3240508.3241390", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "162": {"title": "magical rice bowl: a real-time food category changer.", "url": "https://doi.org/10.1145/3240508.3241391", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "163": {"title": "exploring temporal communities in mass media archives.", "url": "https://doi.org/10.1145/3240508.3241392", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "164": {"title": "sonicontrol - a mobile ultrasonic firewall.", "url": "https://doi.org/10.1145/3240508.3241393", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "165": {"title": "musicmapp: a deep learning based solution for music exploration and visual interaction.", "url": "https://doi.org/10.1145/3240508.3241394", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "166": {"title": "demonstration of an open source framework for qualitative evaluation of cbir systems.", "url": "https://doi.org/10.1145/3240508.3241395", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "167": {"title": "a demonstration of an intelligent storytelling system.", "url": "https://doi.org/10.1145/3240508.3241396", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "168": {"title": "icoobook: when the picture book for children encounters aesthetics of interaction.", "url": "https://doi.org/10.1145/3240508.3241397", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "169": {"title": "an implementation of a dash client for browsing networked virtual environment.", "url": "https://doi.org/10.1145/3240508.3241398", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "170": {"title": "knowledge-aware multimodal fashion chatbot.", "url": "https://doi.org/10.1145/3240508.3241399", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "171": {"title": "svias: scene-segmented video information annotation system.", "url": "https://doi.org/10.1145/3240508.3241400", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "172": {"title": "interactive story maker: tagged video retrieval system for video re-creation service.", "url": "https://doi.org/10.1145/3240508.3241401", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "173": {"title": "heterstyle: a heterogeneous video style transfer application.", "url": "https://doi.org/10.1145/3240508.3241402", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "174": {"title": "pami: projection augmented meeting interface for video conferencing.", "url": "https://doi.org/10.1145/3240508.3241359", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "175": {"title": "childar-bot: educational playing projection-based ar robot for children.", "url": "https://doi.org/10.1145/3240508.3241362", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "176": {"title": "session details: deep-2 (recognition).", "url": "https://dl.acm.org/citation.cfm?id=3286931", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "177": {"title": "mining semantics-preserving attention for group activity recognition.", "url": "https://doi.org/10.1145/3240508.3240576", "abstract": "In this paper, we propose a Semantics-Preserving Teacher-Student (SPTS) model for group activity rec\nognition in videos, which aims to mine the semantics-preserving attention to automatically seek the \nkey people and discard the misleading people. Conventional methods usually aggregate the features ex\ntracted from individual persons by pooling operations, which cannot fully explore the contextual inf\normation for group activity recognition. To address this, our SPTS networks first learn a Teacher Ne\ntwork in semantic domain, which classifies the word of group activity based on the words of individu\nal actions. Then we carefully design a Student Network in vision domain, which recognizes the group \nactivity according to the input videos, and enforce the Student Network to mimic the Teacher Network\n during the learning process. In this way, we allocate semantics-preserving attention to different p\neople, which adequately explores the contextual information of different people and requires no extr\na labelled data. Experimental results on two widely used benchmarks for group activity recognition c\nlearly show the superior performance of our method in comparisons with the state-of-the-arts.", "cite_num": 1, "conf": "mm", "time": "2018"}, "178": {"title": "participation-contributed temporal dynamic model for group activity recognition.", "url": "https://doi.org/10.1145/3240508.3240572", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "179": {"title": "wildfish: a large benchmark for fish recognition in the wild.", "url": "https://doi.org/10.1145/3240508.3240616", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "180": {"title": "pvnet: a joint convolutional network of point cloud and multi-view for 3d shape recognition.", "url": "https://doi.org/10.1145/3240508.3240702", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "181": {"title": "session details: multimedia-2 (socical & emotional multimedia).", "url": "https://dl.acm.org/citation.cfm?id=3286932", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "182": {"title": "emotiongan: unsupervised domain adaptation for learning discrete probability distributions of image emotions.", "url": "https://doi.org/10.1145/3240508.3240591", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "183": {"title": "usar: an interactive user-specific aesthetic ranking framework for images.", "url": "https://doi.org/10.1145/3240508.3240635", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "184": {"title": "deep multimodal image-repurposing detection.", "url": "https://doi.org/10.1145/3240508.3240707", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "185": {"title": "facial expression recognition enhanced by thermal images through adversarial learning.", "url": "https://doi.org/10.1145/3240508.3240608", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "186": {"title": "session details: panel-1.", "url": "https://dl.acm.org/citation.cfm?id=3286933", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "187": {"title": "deep learning for multimedia: science or technology?", "url": "https://doi.org/10.1145/3240508.3243931", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "188": {"title": "session details: open source software competition.", "url": "https://dl.acm.org/citation.cfm?id=3286934", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "189": {"title": "vivid: virtual environment for visual deep learning.", "url": "https://doi.org/10.1145/3240508.3243653", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "190": {"title": "a general-purpose distributed programming system using data-parallel streams.", "url": "https://doi.org/10.1145/3240508.3243654", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "191": {"title": "cilantro: a lean, versatile, and efficient library for point cloud data processing.", "url": "https://doi.org/10.1145/3240508.3243655", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "192": {"title": "web-based configurable image annotations.", "url": "https://doi.org/10.1145/3240508.3243656", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "193": {"title": "session details: vision-3 (applications in multimedia).", "url": "https://dl.acm.org/citation.cfm?id=3286935", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "194": {"title": "only learn one sample: fine-grained visual categorization with one sample training.", "url": "https://doi.org/10.1145/3240508.3240557", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "195": {"title": "la-net: layout-aware dense network for monocular depth estimation.", "url": "https://doi.org/10.1145/3240508.3240628", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "196": {"title": "robustness and discrimination oriented hashing combining texture and invariant vector distance.", "url": "https://doi.org/10.1145/3240508.3240690", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "197": {"title": "joint global and co-attentive representation learning for image-sentence retrieval.", "url": "https://doi.org/10.1145/3240508.3240535", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "198": {"title": "session details: multimodal-2 (cross-modal translation).", "url": "https://dl.acm.org/citation.cfm?id=3286936", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "199": {"title": "text-to-image synthesis via symmetrical distillation networks.", "url": "https://doi.org/10.1145/3240508.3240559", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "200": {"title": "context-aware visual policy network for sequence-level image captioning.", "url": "https://doi.org/10.1145/3240508.3240632", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "201": {"title": "sibnet: sibling convolutional encoder for video captioning.", "url": "https://doi.org/10.1145/3240508.3240667", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "202": {"title": "paragraph generation network with visual relationship detection.", "url": "https://doi.org/10.1145/3240508.3240695", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "203": {"title": "session details: panel-2.", "url": "https://dl.acm.org/citation.cfm?id=3286937", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "204": {"title": "ai + multimedia make better life?", "url": "https://doi.org/10.1145/3240508.3243932", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "205": {"title": "session details: ff-5.", "url": "https://dl.acm.org/citation.cfm?id=3286938", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "206": {"title": "online inter-camera trajectory association exploiting person re-identification and camera topology.", "url": "https://doi.org/10.1145/3240508.3240663", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "207": {"title": "learning local descriptors with adversarial enhancer from volumetric geometry patches.", "url": "https://doi.org/10.1145/3240508.3240666", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "208": {"title": "context-dependent diffusion network for visual relationship detection.", "url": "https://doi.org/10.1145/3240508.3240668", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "209": {"title": "connectionist temporal fusion for sign language translation.", "url": "https://doi.org/10.1145/3240508.3240671", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "210": {"title": "support neighbor loss for person re-identification.", "url": "https://doi.org/10.1145/3240508.3240674", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "211": {"title": "perceptual temporal incoherence aware stereo video retargeting.", "url": "https://doi.org/10.1145/3240508.3240682", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "212": {"title": "a large-scale rgb-d database for arbitrary-view human action recognition.", "url": "https://doi.org/10.1145/3240508.3240675", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "213": {"title": "spotting and aggregating salient regions for video captioning.", "url": "https://doi.org/10.1145/3240508.3240677", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "214": {"title": "adaptive temporal encoding network for video instance-level human parsing.", "url": "https://doi.org/10.1145/3240508.3240660", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "215": {"title": "user-guided deep anime line art colorization with conditional adversarial networks.", "url": "https://doi.org/10.1145/3240508.3240661", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "216": {"title": "bitstream: efficient computing architecture for real-time low-power inference of binary neural networks on cpus.", "url": "https://doi.org/10.1145/3240508.3240673", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "217": {"title": "attentive crowd flow machines.", "url": "https://doi.org/10.1145/3240508.3240681", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "218": {"title": "video-based person re-identification via self-paced learning and deep reinforcement learning framework.", "url": "https://doi.org/10.1145/3240508.3240622", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "219": {"title": "interpretable multimodal retrieval for fashion products.", "url": "https://doi.org/10.1145/3240508.3240646", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "220": {"title": "generating defensive plays in basketball games.", "url": "https://doi.org/10.1145/3240508.3240670", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "221": {"title": "dense auto-encoder hashing for robust cross-modality retrieval.", "url": "https://doi.org/10.1145/3240508.3240684", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "222": {"title": "dance with melody: an lstm-autoencoder approach to music-oriented dance synthesis.", "url": "https://doi.org/10.1145/3240508.3240526", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "223": {"title": "musicality-novelty generative adversarial nets for algorithmic composition.", "url": "https://doi.org/10.1145/3240508.3240604", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "224": {"title": "improving qoe of abr streaming sessions through quic retransmissions.", "url": "https://doi.org/10.1145/3240508.3240664", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "225": {"title": "from data to knowledge: deep learning model compression, transmission and communication.", "url": "https://doi.org/10.1145/3240508.3240654", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "226": {"title": "session details: keynote 4.", "url": "https://dl.acm.org/citation.cfm?id=3286939", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "227": {"title": "living with ai in connected devices for valuable experience.", "url": "https://doi.org/10.1145/3240508.3267344", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "228": {"title": "session details: multimedia -3 (multimedia search).", "url": "https://dl.acm.org/citation.cfm?id=3286940", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "229": {"title": "supervised online hashing via hadamard codebook learning.", "url": "https://doi.org/10.1145/3240508.3240519", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "230": {"title": "cascaded feature augmentation with diffusion for image retrieval.", "url": "https://doi.org/10.1145/3240508.3240532", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "231": {"title": "deep priority hashing.", "url": "https://doi.org/10.1145/3240508.3240543", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "232": {"title": "fast discrete cross-modal hashing with regressing from semantic labels.", "url": "https://doi.org/10.1145/3240508.3240683", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "233": {"title": "session details: experience-1 (multimedia entertainment and experience).", "url": "https://dl.acm.org/citation.cfm?id=3286941", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "234": {"title": "modanet: a large-scale street fashion dataset with polygon annotations.", "url": "https://doi.org/10.1145/3240508.3240652", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "235": {"title": "slions: a karaoke application to enhance foreign language learning.", "url": "https://doi.org/10.1145/3240508.3240691", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "236": {"title": "context-aware unsupervised text stylization.", "url": "https://doi.org/10.1145/3240508.3240580", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "237": {"title": "songle sync: a large-scale web-based platform for controlling various devices in synchronization with music.", "url": "https://doi.org/10.1145/3240508.3240619", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "238": {"title": "session details: system-2 (smart multimedia systems).", "url": "https://dl.acm.org/citation.cfm?id=3286942", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "239": {"title": "fine-grained grocery product recognition by one-shot learning.", "url": "https://doi.org/10.1145/3240508.3240522", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "240": {"title": "reconfigurable inverted index.", "url": "https://doi.org/10.1145/3240508.3240630", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "241": {"title": "robust billboard-based, free-viewpoint video synthesis algorithm to overcome occlusions under challenging outdoor sport scenes.", "url": "https://doi.org/10.1145/3240508.3240514", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "242": {"title": "ihuman3d: intelligent human body 3d reconstruction using a single flying camera.", "url": "https://doi.org/10.1145/3240508.3240600", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "243": {"title": "session details: ff-6.", "url": "https://dl.acm.org/citation.cfm?id=3286943", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "244": {"title": "examine before you answer: multi-task learning with adaptive-attentions for multiple-choice vqa.", "url": "https://doi.org/10.1145/3240508.3240687", "abstract": "Multiple-choice (MC) Visual Question Answering (VQA) is a similar but essentially different task to \nopen-ended VQA because the answer options are provided. Most of existing works tackle them in a unif\nied pipeline by solving a multi-class problem to infer the best answer from a predefined answer set.\n The option that matches the best answer is selected for MC VQA. Nevertheless, this violates human t\nhinking logics. Normally, people examine the questions, answer options and the reference image befor\ne inferring a MC VQA. For MC VQA, human either rely on the question and answer options to directly d\neduce a correct answer if the question is not image-related, or read the question and answer options\n and then purposefully search for answers in a reference image. Therefore, we propose a novel approa\nch, namely Multi-task Learning with Adaptive-attention (MTA), to simulate human logics for MC VQA. S\npecifically, we first fuse the answer options and question features, and then adaptively attend to t\nhe visual features for inferring a MC VQA. Furthermore, we design our model as a multi-task learning\n architecture by integrating the open-ended VQA task to further boost the performance of MC VQA. We \nevaluate our approach on two standard benchmark datasets: VQA and Visual7W and our approach sets new\n records on both datasets for MC VQA task, reaching 73.5% and 65.9% average accuracy respectively.", "cite_num": 1, "conf": "mm", "time": "2018"}, "245": {"title": "residual-guide network for single image deraining.", "url": "https://doi.org/10.1145/3240508.3240694", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "246": {"title": "from volcano to toyshop: adaptive discriminative region discovery for scene recognition.", "url": "https://doi.org/10.1145/3240508.3240698", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "247": {"title": "the effect of foveation on high dynamic range video perception.", "url": "https://doi.org/10.1145/3240508.3240703", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "248": {"title": "an efficient deep quantized compressed sensing coding framework of natural images.", "url": "https://doi.org/10.1145/3240508.3240706", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "249": {"title": "pob: toward reasoning patterns of beauty in image data.", "url": "https://doi.org/10.1145/3240508.3240711", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "250": {"title": "partial multi-view subspace clustering.", "url": "https://doi.org/10.1145/3240508.3240679", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "251": {"title": "pseudo transfer with marginalized corrupted attribute for zero-shot learning.", "url": "https://doi.org/10.1145/3240508.3240715", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "252": {"title": "semi-supervised dff: decoupling detection and feature flow for video object detectors.", "url": "https://doi.org/10.1145/3240508.3240693", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "253": {"title": "unsupervised learning of 3d model reconstruction from hand-drawn sketches.", "url": "https://doi.org/10.1145/3240508.3240699", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "254": {"title": "deep adaptive temporal pooling for activity recognition.", "url": "https://doi.org/10.1145/3240508.3240713", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "255": {"title": "person re-identification with hierarchical deep learning feature and efficient xqda metric.", "url": "https://doi.org/10.1145/3240508.3240717", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "256": {"title": "cumulative nets for edge detection.", "url": "https://doi.org/10.1145/3240508.3240688", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "257": {"title": "webly supervised joint embedding for cross-modal image-text retrieval.", "url": "https://doi.org/10.1145/3240508.3240712", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "258": {"title": "multi-modal preference modeling for product search.", "url": "https://doi.org/10.1145/3240508.3240541", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "259": {"title": "learning joint multimodal representation with adversarial attention networks.", "url": "https://doi.org/10.1145/3240508.3240614", "abstract": "Recently, learning a joint representation for the multimodal data (e.g., containing both visual cont\nent and text description) has attracted extensive research interests. Usually, the features of diffe\nrent modalities are correlational and compositive, and thus a joint representation capturing the cor\nrelation is more effective than a subset of the features. Most of existing multimodal representation\n learning methods suffer from lack of additional constraints to enhance the robustness of the learne\nd representations. In this paper, a novel Adversarial Attention Networks (AAN) is proposed to incorp\norate both the attention mechanism and the adversarial networks for effective and robust multimodal \nrepresentation learning. Specifically, a visual-semantic attention model with siamese learning strat\negy is proposed to encode the fine-grained correlation between visual and textual modalities. Meanwh\nile, the adversarial learning model is employed to regularize the generated representation by matchi\nng the posterior distribution of the representation to the given priors. Then, the two modules are i\nncorporated into a integrated learning framework to learn the joint multimodal representation. Exper\nimental results in two tasks, i.e., multi-label classification and tag recommendation, show that the\n proposed model outperforms state-of-the-art representation learning methods.", "cite_num": 3, "conf": "mm", "time": "2018"}, "260": {"title": "dest-resnet: a deep spatiotemporal residual network for hotspot traffic speed prediction.", "url": "https://doi.org/10.1145/3240508.3240656", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "261": {"title": "learning and fusing multimodal deep features for acoustic scene categorization.", "url": "https://doi.org/10.1145/3240508.3240631", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "262": {"title": "dynamic sound field synthesis for speech and music optimization.", "url": "https://doi.org/10.1145/3240508.3240644", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "263": {"title": "dash for 3d networked virtual environment.", "url": "https://doi.org/10.1145/3240508.3240701", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "264": {"title": "session details: keynote 5.", "url": "https://dl.acm.org/citation.cfm?id=3286944", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "265": {"title": "transforming retailing experiences with artificial intelligence.", "url": "https://doi.org/10.1145/3240508.3267341", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "266": {"title": "session details: deep-3 (image processing-inpainting, super-resolution, deblurring).", "url": "https://dl.acm.org/citation.cfm?id=3286945", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "267": {"title": "learning collaborative generation correction modules for blind image deblurring and beyond.", "url": "https://doi.org/10.1145/3240508.3240565", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "268": {"title": "when deep fool meets deep prior: adversarial attack on super-resolution network.", "url": "https://doi.org/10.1145/3240508.3240603", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "269": {"title": "semantic image inpainting with progressive generative networks.", "url": "https://doi.org/10.1145/3240508.3240625", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "270": {"title": "structural inpainting.", "url": "https://doi.org/10.1145/3240508.3240678", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "271": {"title": "session details: brand new ideas.", "url": "https://dl.acm.org/citation.cfm?id=3286946", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "272": {"title": "fluid annotation: a human-machine collaboration interface for full image annotation.", "url": "https://doi.org/10.1145/3240508.3241916", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "273": {"title": "images2poem: generating chinese poetry from image streams.", "url": "https://doi.org/10.1145/3240508.3241910", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "274": {"title": "harnessing ai for speech reconstruction using multi-view silent video feed.", "url": "https://doi.org/10.1145/3240508.3241911", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "275": {"title": "alert: adding a secure layer in decision support for advanced driver assistance system (adas).", "url": "https://doi.org/10.1145/3240508.3241912", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "276": {"title": "cross-modal health state estimation.", "url": "https://doi.org/10.1145/3240508.3241913", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "277": {"title": "session details: grand challenge-1.", "url": "https://dl.acm.org/citation.cfm?id=3286947", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "278": {"title": "an effective text-based characterization combined with numerical features for social media headline prediction.", "url": "https://doi.org/10.1145/3240508.3266438", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "279": {"title": "an iterative refinement approach for social media headline prediction.", "url": "https://doi.org/10.1145/3240508.3266443", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "280": {"title": "random forest exploiting post-related and user-related features for social media popularity prediction.", "url": "https://doi.org/10.1145/3240508.3266439", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "281": {"title": "content-based video relevance prediction with second-order relevance and attention modeling.", "url": "https://doi.org/10.1145/3240508.3266434", "abstract": "This paper describes our proposed method for the Content-Based Video Relevance Prediction (CBVRP) ch\nallenge. Our method is based on deep learning, i.e. we train a deep network to predict the relevance\n between two video sequences from their features. We explore the usage of second-order relevance, bo\nth in preparing training data, and in extending the deep network. Second-order relevance refers to e\n.g. the relevance between x and z if x is relevant to y and y is relevant to z. In our proposed meth\nod, we use second-order relevance to increase positive samples and decrease negative samples, when p\nreparing training data. We further extend the deep network with an attention module, where the atten\ntion mechanism is designed for second-order relevant video sequences. We verify the effectiveness of\n our method on the validation set of the CBVRP challenge.", "cite_num": 0, "conf": "mm", "time": "2018"}, "282": {"title": "session details: vision-4 (representation learning).", "url": "https://dl.acm.org/citation.cfm?id=3286948", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "283": {"title": "fine-grained representation learning and recognition by exploiting hierarchical semantic embedding.", "url": "https://doi.org/10.1145/3240508.3240523", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "284": {"title": "dissimilarity representation learning for generalized zero-shot recognition.", "url": "https://doi.org/10.1145/3240508.3240686", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "285": {"title": "attribute-aware attention model for fine-grained representation learning.", "url": "https://doi.org/10.1145/3240508.3240550", "abstract": "How to learn a discriminative fine-grained representation is a key point in many computer vision app\nlications, such as person re-identification, fine-grained classification, fine-grained image retriev\nal, etc. Most of the previous methods focus on learning metrics or ensemble to derive better global \nrepresentation, which are usually lack of local information. Based on the considerations above, we p\nropose a novel Attribute-Aware Attention Model ($A^3M$), which can learn local attribute representat\nion and global category representation simultaneously in an end-to-end manner. The proposed model co\nntains two attention models: attribute-guided attention module uses attribute information to help se\nlect category features in different regions, at the same time, category-guided attention module sele\ncts local features of different attributes with the help of category cues. Through this attribute-ca\ntegory reciprocal process, local and global features benefit from each other. Finally, the resulting\n feature contains more intrinsic information for image recognition instead of the noisy and irreleva\nnt features. Extensive experiments conducted on Market-1501, CompCars, CUB-200-2011 and CARS196 demo\nnstrate the effectiveness of our $A^3M$.", "cite_num": 4, "conf": "mm", "time": "2018"}, "286": {"title": "gnas: a greedy neural architecture search method for multi-attribute learning.", "url": "https://doi.org/10.1145/3240508.3240588", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "287": {"title": "session details: grand challenge-2.", "url": "https://dl.acm.org/citation.cfm?id=3286949", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "288": {"title": "feature re-learning with data augmentation for content-based video recommendation.", "url": "https://doi.org/10.1145/3240508.3266441", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "289": {"title": "beauty product image retrieval based on multi-feature fusion and feature aggregation.", "url": "https://doi.org/10.1145/3240508.3266431", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "290": {"title": "unprecedented usage of pre-trained cnns on beauty product.", "url": "https://doi.org/10.1145/3240508.3266433", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "291": {"title": "regional maximum activations of convolutions with attention for cross-domain beauty and personal care product retrieval.", "url": "https://doi.org/10.1145/3240508.3266436", "abstract": "Cross-domain beauty and personal care product image retrieval is a challenging problem due to data v\nariations (e.g., brightness, viewpoint, and scale), and the rich types of items. In this paper, we p\nresent a regional maximum activations of convolutions with attention (RA-MAC) descriptor to extract \nimage features for retrieval. RA-MAC improves the regional maximum activations of convolutions (R-MA\nC) descriptor considering the influence of background in cross-domain images (i.e., shopper domain a\nnd seller domain). More specifically, RA-MAC utilizes the characteristics of the convolutional layer\n to find the attention of an image, and reduces the influence of the unimportant regions in an unsup\nervised manner. Furthermore, a few strategies have been exploited to improve the performance, such a\ns multiple features fusion, query expansion, and database augmentation. Extensive experiments conduc\nted on a dataset consisting of half a million images of beauty care products (Perfect-500K) manifest\n the effectiveness of RA-MAC. Our approach achieves the 2nd place in the leader board of the Grand C\nhallenge of AI Meets Beauty in ACM Multimedia 2018. Our code is available at: https://github.com/Ret\nrainIt/Perfect-Half-Million-Beauty-Product-Image-Recognition-Challenge.", "cite_num": 3, "conf": "mm", "time": "2018"}, "292": {"title": "session details: interactive art.", "url": "https://dl.acm.org/citation.cfm?id=3286950", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "293": {"title": "shadow calligraphy of dance: an image-based interactive installation for capturing flowing human figures.", "url": "https://doi.org/10.1145/3240508.3264576", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "294": {"title": "cellular music: an interactive game of life sequencer.", "url": "https://doi.org/10.1145/3240508.3264577", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "295": {"title": "tagapp visualization: an application based visual art installation.", "url": "https://doi.org/10.1145/3240508.3264580", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "296": {"title": "similarity-based processing of motion capture data.", "url": "https://doi.org/10.1145/3240508.3241468", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "297": {"title": "structured deep learning for pixel-level understanding.", "url": "https://doi.org/10.1145/3240508.3241469", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "298": {"title": "social and political event analysis based on rich media.", "url": "https://doi.org/10.1145/3240508.3241470", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "299": {"title": "to recognize families in the wild: a machine vision tutorial.", "url": "https://doi.org/10.1145/3240508.3241471", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "300": {"title": "deep learning interpretation.", "url": "https://doi.org/10.1145/3240508.3241472", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "301": {"title": "interactive video search: where is the user in the age of deep learning?", "url": "https://doi.org/10.1145/3240508.3241473", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "302": {"title": "human behavior understanding: from action recognition to complex event detection.", "url": "https://doi.org/10.1145/3240508.3241474", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "303": {"title": "the importance of medical multimedia.", "url": "https://doi.org/10.1145/3240508.3241475", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "304": {"title": "altmm 2018 - 3rd international workshop on multimedia alternate realities.", "url": "https://doi.org/10.1145/3240508.3243718", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "305": {"title": "summary for avec 2018: bipolar disorder and cross-cultural affect recognition.", "url": "https://doi.org/10.1145/3240508.3243719", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "306": {"title": "coview'18: the 1st workshop and challenge on comprehensive video understanding in the wild.", "url": "https://doi.org/10.1145/3240508.3243720", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "307": {"title": "healthmedia 2018: third international workshop on multimedia for personal health and health care.", "url": "https://doi.org/10.1145/3240508.3243722", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "308": {"title": "mahci 2018: the 1st workshop on multimedia for accessible human computer interface.", "url": "https://doi.org/10.1145/3240508.3243723", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "309": {"title": "asmmc-mmac 2018: the joint workshop of 4th the workshop on affective social multimedia computing and first multi-modal affective computing of large-scale multimedia data workshop.", "url": "https://doi.org/10.1145/3240508.3243724", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "310": {"title": "avsu: workshop on audio-visual scene understanding for immersive multimedia.", "url": "https://doi.org/10.1145/3240508.3243725", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "311": {"title": "1st acm international workshop on multimedia content analysis in sports.", "url": "https://doi.org/10.1145/3240508.3243726", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}, "312": {"title": "ee-usad: acm mm 2018workshop on understandingsubjective attributes of data focus on evoked emotions.", "url": "https://doi.org/10.1145/3240508.3243721", "abstract": "", "cite_num": -1, "conf": "mm", "time": "2018"}}
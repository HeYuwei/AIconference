{"52": {"title": "beautygan: instance-level facial makeup transfer with deep generative adversarial network.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240618", "abstract": "", "cite_num": -1}, "68": {"title": "session details: keynote 5.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286944", "abstract": "", "cite_num": -1}, "285": {"title": "facial expression recognition enhanced by thermal images through adversarial learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240608", "abstract": "", "cite_num": -1}, "266": {"title": "hybrid point cloud attribute compression using slice-based layered structure and block-based intra prediction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240696", "abstract": "", "cite_num": -1}, "124": {"title": "bitstream: efficient computing architecture for real-time low-power inference of binary neural networks on cpus.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240673", "abstract": "", "cite_num": -1}, "307": {"title": "shadow calligraphy of dance: an image-based interactive installation for capturing flowing human figures.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3264576", "abstract": "", "cite_num": -1}, "38": {"title": "pvnet: a joint convolutional network of point cloud and multi-view for 3d shape recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240702", "abstract": "", "cite_num": -1}, "291": {"title": "geometry guided adversarial facial expression synthesis.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240612", "abstract": "", "cite_num": -1}, "306": {"title": "childar-bot: educational playing projection-based ar robot for children.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241362", "abstract": "", "cite_num": -1}, "284": {"title": "beauty product image retrieval based on multi-feature fusion and feature aggregation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3266431", "abstract": "", "cite_num": -1}, "305": {"title": "songle sync: a large-scale web-based platform for controlling various devices in synchronization with music.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240619", "abstract": "", "cite_num": -1}, "8": {"title": "phd-gifs: personalized highlight detection for automatic gif creation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240599", "abstract": "", "cite_num": -1}, "197": {"title": "pami: projection augmented meeting interface for video conferencing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241359", "abstract": "", "cite_num": -1}, "274": {"title": "session details: vision-1 (machine learning).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286920", "abstract": "", "cite_num": -1}, "29": {"title": "osmo: online specific models for occlusion in multiple object tracking under surveillance scene.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240548", "abstract": "", "cite_num": -1}, "252": {"title": "learning and fusing multimodal deep features for acoustic scene categorization.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240631", "abstract": "Convolutional Neural Networks (CNNs) have been widely applied to audio classification recently where\n promising results have been obtained. Previous CNN-based systems mostly learn from two-dimensional \ntime-frequency representations such as MFCC and spectrograms, which may tend to emphasize more on th\ne background noise of the scene. To learn the key acoustic events, we introduce a three-dimensional \nCNN to emphasize on the different spectral characteristics from neighboring regions in spatial-tempo\nral domain. A novel acoustic scene classification system based on multimodal deep feature fusion is \nproposed in this paper, where three CNNs have been presented to perform 1D raw waveform modeling, 2D\n time-frequency image modeling, and 3D spatial-temporal dynamics modeling, respectively. The learnt \nfeatures are shown to be highly complementary to each other, which are next combined in a feature fu\nsion network to obtain significantly improved classification predictions. Comprehensive experiments \nhave been conducted on two large-scale acoustic scene datasets, namely the DCASE16 dataset and the L\nITIS Rouen dataset. Experimental results demonstrate the effectiveness of our proposed approach, as \nour solution achieves state-of-the-art classification rates and improves the average classification \naccuracy by 1.5% - 8.2% compared to the top ranked systems in the DCASE16 challenge.", "cite_num": 4}, "269": {"title": "shared linear encoder-based gaussian process latent variable model for visual classification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240520", "abstract": "", "cite_num": -1}, "256": {"title": "depth structure preserving scene image generation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240584", "abstract": "", "cite_num": -1}, "110": {"title": "beyond the product: discovering image posts for brands in social media.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240689", "abstract": "", "cite_num": -1}, "218": {"title": "extractive video summarizer with memory augmented neural networks.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240651", "abstract": "", "cite_num": -1}, "239": {"title": "on reducing effort in evaluating laparoscopic skills.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243934", "abstract": "", "cite_num": -1}, "35": {"title": "session details: ff-3.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286925", "abstract": "", "cite_num": -1}, "59": {"title": "partial multi-view subspace clustering.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240679", "abstract": "", "cite_num": -1}, "221": {"title": "deep understanding of cooking procedure for cross-modal recipe retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240627", "abstract": "", "cite_num": -1}, "23": {"title": "demo: phase-based acoustic localization and motion tracking for mobile interaction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241385", "abstract": "", "cite_num": -1}, "116": {"title": "session details: multimodal-2 (cross-modal translation).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286936", "abstract": "... but we have temporarily restricted your access to the Digital Library.\nYour activity appears to \nbe coming from some type of automated process.\nTo ensure the availability of the Digital Library we \ncan not allow these types of requests to continue.\nThe restriction will be removed automatically onc\ne this activity stops.\n", "cite_num": 0}, "242": {"title": "fluid annotation: a human-machine collaboration interface for full image annotation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241916", "abstract": "", "cite_num": -1}, "123": {"title": "online inter-camera trajectory association exploiting person re-identification and camera topology.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240663", "abstract": "", "cite_num": -1}, "136": {"title": "session details: ff-6.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286943", "abstract": "", "cite_num": -1}, "205": {"title": "session details: system-2 (smart multimedia systems).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286942", "abstract": "", "cite_num": -1}, "240": {"title": "deep cross modal learning for caricature verification and identification (cavinet).", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240658", "abstract": "", "cite_num": -1}, "138": {"title": "an implementation of a dash client for browsing networked virtual environment.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241398", "abstract": "", "cite_num": -1}, "276": {"title": "semi-supervised deep generative modelling of incomplete multi-modality emotional data.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240528", "abstract": "There are threefold challenges in emotion recognition. First, it is difficult to recognize human's e\nmotional states only considering a single modality. Second, it is expensive to manually annotate the\n emotional data. Third, emotional data often suffers from missing modalities due to unforeseeable se\nnsor malfunction or configuration issues. In this paper, we address all these problems under a novel\n multi-view deep generative framework. Specifically, we propose to model the statistical relationshi\nps of multi-modality emotional data using multiple modality-specific generative networks with a shar\ned latent space. By imposing a Gaussian mixture assumption on the posterior approximation of the sha\nred latent variables, our framework can learn the joint deep representation from multiple modalities\n and evaluate the importance of each modality simultaneously. To solve the labeled-data-scarcity pro\nblem, we extend our multi-view model to semi-supervised learning scenario by casting the semi-superv\nised classification problem as a specialized missing data imputation task. To address the missing-mo\ndality problem, we further extend our semi-supervised multi-view model to deal with incomplete data,\n where a missing view is treated as a latent variable and integrated out during inference. This way,\n the proposed overall framework can utilize all available (both labeled and unlabeled, as well as bo\nth complete and incomplete) data to improve its generalization ability. The experiments conducted on\n two real multi-modal emotion datasets demonstrated the superiority of our framework.", "cite_num": 0}, "112": {"title": "learning to synthesize 3d indoor scenes from monocular images.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240700", "abstract": "", "cite_num": -1}, "296": {"title": "session details: ff-1.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286915", "abstract": "", "cite_num": -1}, "147": {"title": "ca3net: contextual-attentional attribute-appearance network for person re-identification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240585", "abstract": "Person re-identification aims to identify the same pedestrian across non-overlapping camera views. D\neep learning techniques have been applied for person re-identification recently, towards learning re\npresentation of pedestrian appearance. This paper presents a novel Contextual-Attentional Attribute-\nAppearance Network (CA3Net) for person re-identification. The CA3Net simultaneously exploits the com\nplementarity between semantic attributes and visual appearance, the semantic context among attribute\ns, visual attention on attributes as well as spatial dependencies among body parts, leading to discr\niminative and robust pedestrian representation. Specifically, an attribute network within CA3Net is \ndesigned with an Attention-LSTM module. It concentrates the network on latent image regions related \nto each attribute as well as exploits the semantic context among attributes by a LSTM module. An app\nearance network is developed to learn appearance features from the full body, horizontal and vertica\nl body parts of pedestrians with spatial dependencies among body parts. The CA3Net jointly learns th\ne attribute and appearance features in a multi-task learning manner, generating comprehensive repres\nentation of pedestrians. Extensive experiments on two challenging benchmarks, i.e., Market-1501 and \nDukeMTMC-reID datasets, have demonstrated the effectiveness of the proposed approach.", "cite_num": -1}, "45": {"title": "wildfish: a large benchmark for fish recognition in the wild.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240616", "abstract": "", "cite_num": -1}, "60": {"title": "1st acm international workshop on multimedia content analysis in sports.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243726", "abstract": "", "cite_num": -1}, "258": {"title": "local convolutional neural networks for person re-identification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240645", "abstract": "", "cite_num": -1}, "233": {"title": "feature constrained by pixel: hierarchical adversarial deep domain adaptation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240562", "abstract": "", "cite_num": -1}, "169": {"title": "online action tube detection via resolving the spatio-temporal context pattern.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240659", "abstract": "", "cite_num": -1}, "67": {"title": "social and political event analysis based on rich media.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241470", "abstract": "", "cite_num": -1}, "100": {"title": "musicmapp: a deep learning based solution for music exploration and visual interaction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241394", "abstract": "", "cite_num": -1}, "193": {"title": "direction-aware neural style transfer.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240629", "abstract": "", "cite_num": -1}, "236": {"title": "interactive video search: where is the user in the age of deep learning?", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241473", "abstract": "", "cite_num": -1}, "166": {"title": "ai painting: an aesthetic painting generation system.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241386", "abstract": "", "cite_num": -1}, "113": {"title": "your attention is unique: detecting 360-degree video saliency in head-mounted display for head movement prediction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240669", "abstract": "Head movement prediction is the key enabler for the emerging 360-degree videos since it can enhance \nboth streaming and rendering efficiency. To achieve accurate head movement prediction, it becomes im\nperative to understand user's visual attention on 360-degree videos under head-mounted display (HMD)\n. Despite the rich history of saliency detection research, we observe that traditional models are de\nsigned for regular images/videos fixed at a single viewport and would introduce problems such as cen\ntral bias and multi-object confusion when applied to the multi-viewport 360-degree videos switched b\ny user interaction. To fill in this gap, this paper shifts the traditional single-viewport saliency \nmodels that have been extensively studied for decades to a fresh panoramic saliency detection specif\nically tailored for 360-degree videos, and thus maximally enhances the head movement prediction perf\normance. The proposed head movement prediction framework is empowered by a newly created dataset for\n 360-degree video saliency, a panoramic saliency detection model and an integration of saliency and \nhead tracking history for the ultimate head movement prediction. Experimental results demonstrate th\ne measurable gain of both the proposed panoramic saliency detection and head movement prediction ove\nr traditional models for regular images/videos.", "cite_num": -1}, "297": {"title": "temporal sequence distillation: towards few-frame action recognition in videos.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240534", "abstract": "", "cite_num": -1}, "220": {"title": "semantic human matting.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240610", "abstract": "", "cite_num": -1}, "4": {"title": "fine-grained representation learning and recognition by exploiting hierarchical semantic embedding.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240523", "abstract": "", "cite_num": -1}, "263": {"title": "a feature-adaptive semi-supervised framework for co-saliency detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240648", "abstract": "", "cite_num": -1}, "244": {"title": "living with ai in connected devices for valuable experience.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3267344", "abstract": "", "cite_num": -1}, "190": {"title": "enhancing visual question answering using dropout.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240662", "abstract": "", "cite_num": -1}, "199": {"title": "historical context-based style classification of painting images via label distribution learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240593", "abstract": "", "cite_num": -1}, "36": {"title": "session details: multimedia-2 (socical & emotional multimedia).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286932", "abstract": "", "cite_num": -1}, "99": {"title": "challenges and practices of large scale visual intelligence in the real-world.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3267342", "abstract": "", "cite_num": -1}, "14": {"title": "inferring user emotive state changes in realistic human-computer conversational dialogs.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240575", "abstract": "", "cite_num": -1}, "288": {"title": "harnessing ai for speech reconstruction using multi-view silent video feed.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241911", "abstract": "", "cite_num": -1}, "201": {"title": "stripnet: towards topology consistent strip structure segmentation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240553", "abstract": "", "cite_num": -1}, "102": {"title": "improving qoe of abr streaming sessions through quic retransmissions.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240664", "abstract": "", "cite_num": -1}, "106": {"title": "session details: experience-1 (multimedia entertainment and experience).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286941", "abstract": "", "cite_num": -1}, "130": {"title": "context-aware unsupervised text stylization.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240580", "abstract": "", "cite_num": -1}, "192": {"title": "spotting and aggregating salient regions for video captioning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240677", "abstract": "", "cite_num": -1}, "103": {"title": "learning collaborative generation correction modules for blind image deblurring and beyond.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240565", "abstract": "", "cite_num": -1}, "155": {"title": "fully point-wise convolutional neural network for modeling statistical regularities in natural images.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240653", "abstract": "", "cite_num": -1}, "54": {"title": "artsight: an artistic data exploration engine.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241389", "abstract": "", "cite_num": -1}, "57": {"title": "visual spatial attention network for relationship detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240611", "abstract": "Visual relationship detection, which aims to predict a   triplet with the detected objects, has attr\nacted increasing attention in the scene understanding study. During tackling this problem, dealing w\nith varying scales of the subjects and objects is of great importance, which has been less studied. \nTo overcome this challenge, we propose a novel Vision Spatial Attention Network (VSA-Net), which emp\nloys a two-dimensional normal distribution attention scheme to effectively model small objects. In a\nddition, we design a Subject-Object-layer (SO-layer) to distinguish between the subject and object t\no attain more precise results. To the best of our knowledge, VSA-Net is the first end-to-end attenti\non mechanism based visual relationship detection model. Extensive experiments on the benchmark datas\nets (VRD and VG) show that, by using pure vision information, our VSA-Net achieves state-of-the-art \nperformance for predicate detection, phrase detection, and relationship detection.", "cite_num": 2}, "73": {"title": "rgcnn: regularized graph cnn for point cloud segmentation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240621", "abstract": "", "cite_num": -1}, "56": {"title": "video-based person re-identification via self-paced learning and deep reinforcement learning framework.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240622", "abstract": "", "cite_num": -1}, "278": {"title": "emotion recognition in speech using cross-modal transfer in the wild.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240578", "abstract": "", "cite_num": -1}, "254": {"title": "dash for 3d networked virtual environment.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240701", "abstract": "", "cite_num": -1}, "248": {"title": "gesturegan for hand gesture-to-gesture translation in the wild.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240704", "abstract": "", "cite_num": -1}, "107": {"title": "attention-based multi-patch aggregation for image aesthetic assessment.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240554", "abstract": "Aggregation structures with explicit information, such as image attributes and scene semantics, are \neffective and popular for intelligent systems for assessing aesthetics of visual data. However, usef\nul information may not be available due to the high cost of manual annotation and expert design. In \nthis paper, we present a novel multi-patch (MP) aggregation method for image aesthetic assessment. D\nifferent from state-of-the-art methods, which augment an MP aggregation network with various visual \nattributes, we train the model in an end-to-end manner with aesthetic labels only (i.e., aesthetical\nly positive or negative). We achieve the goal by resorting to an attention-based mechanism that adap\ntively adjusts the weight of each patch during the training process to improve learning efficiency. \nIn addition, we propose a set of objectives with three typical attention mechanisms (i.e., average, \nminimum, and adaptive) and evaluate their effectiveness on the Aesthetic Visual Analysis (AVA) bench\nmark. Numerical results show that our approach outperforms existing methods by a large margin. We fu\nrther verify the effectiveness of the proposed attention-based objectives via ablation studies and s\nhed light on the design of aesthetic assessment systems.", "cite_num": 2}, "109": {"title": "high-quality exposure correction of underexposed photos.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240595", "abstract": "", "cite_num": -1}, "71": {"title": "support neighbor loss for person re-identification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240674", "abstract": "", "cite_num": -1}, "281": {"title": "session details: interactive art.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286950", "abstract": "", "cite_num": -1}, "282": {"title": "fast discrete cross-modal hashing with regressing from semantic labels.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240683", "abstract": "", "cite_num": -1}, "48": {"title": "learning to transfer: generalizable attribute learning with multitask neural model search.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240518", "abstract": "", "cite_num": -1}, "257": {"title": "aesthetic-driven image enhancement by adversarial learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240531", "abstract": "", "cite_num": -1}, "181": {"title": "avsu: workshop on audio-visual scene understanding for immersive multimedia.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243725", "abstract": "", "cite_num": -1}, "224": {"title": "summary for avec 2018: bipolar disorder and cross-cultural affect recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243719", "abstract": "", "cite_num": -1}, "160": {"title": "hierarchical memory modelling for video captioning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240538", "abstract": "", "cite_num": -1}, "127": {"title": "semantic image inpainting with progressive generative networks.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240625", "abstract": "", "cite_num": -1}, "225": {"title": "session details: vision-3 (applications in multimedia).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286935", "abstract": "", "cite_num": -1}, "215": {"title": "session details: multimedia -3 (multimedia search).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286940", "abstract": "", "cite_num": -1}, "299": {"title": "watch, think and attend: end-to-end video classification via dynamic knowledge evolution modeling.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240566", "abstract": "", "cite_num": -1}, "277": {"title": "monocular camera based real-time dense mapping using generative adversarial network.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240564", "abstract": "", "cite_num": -1}, "219": {"title": "transforming retailing experiences with artificial intelligence.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3267341", "abstract": "", "cite_num": -1}, "194": {"title": "cumulative nets for edge detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240688", "abstract": "", "cite_num": -1}, "152": {"title": "session details: open source software competition.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286934", "abstract": "", "cite_num": -1}, "121": {"title": "sonicontrol - a mobile ultrasonic firewall.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241393", "abstract": "", "cite_num": -1}, "183": {"title": "look deeper see richer: depth-aware image paragraph captioning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240583", "abstract": "", "cite_num": -1}, "230": {"title": "person re-identification with hierarchical deep learning feature and efficient xqda metric.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240717", "abstract": "", "cite_num": -1}, "283": {"title": "crossing-domain generative adversarial networks for unsupervised multi-domain image-to-image translation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240716", "abstract": "", "cite_num": -1}, "83": {"title": "collaborative annotation of semantic objects in images with multi-granularity supervisions.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240540", "abstract": "", "cite_num": -1}, "2": {"title": "group re-identification: leveraging and integrating multi-grain information.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240539", "abstract": "", "cite_num": -1}, "262": {"title": "heterstyle: a heterogeneous video style transfer application.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241402", "abstract": "", "cite_num": -1}, "203": {"title": "unprecedented usage of pre-trained cnns on beauty product.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3266433", "abstract": "", "cite_num": -1}, "150": {"title": "i read, i saw, i tell: texts assisted fine-grained visual classification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240579", "abstract": "", "cite_num": -1}, "294": {"title": "robustness and discrimination oriented hashing combining texture and invariant vector distance.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240690", "abstract": "", "cite_num": -1}, "158": {"title": "exploring temporal communities in mass media archives.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241392", "abstract": "", "cite_num": -1}, "47": {"title": "csan: contextual self-attention network for user sequential recommendation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240609", "abstract": "The sequential recommendation is an important task for online user-oriented services, such as purcha\nsing products, watching videos, and social media consumption. Recent work usually used RNN-based met\nhods to derive an overall embedding of the whole behavior sequence, which fails to discriminate the \nsignificance of individual user behaviors and thus decreases the recommendation performance. Besides\n, RNN-based encoding has fixed size and makes further recommendation application inefficient and inf\nlexible. The online sequential behaviors of a user are generally heterogeneous, polysemous, and dyna\nmically context-dependent. In this paper, we propose a unified Contextual Self-Attention Network (CS\nAN) to address the three properties. Heterogeneous user behaviors are considered in our model that a\nre projected into a common latent semantic space. Then the output is fed into the feature-wise self-\nattention network to capture the polysemy of user behaviors. In addition, the forward and backward p\nosition encoding matrices are proposed to model dynamic contextual dependency. Through extensive exp\neriments on two real-world datasets, we demonstrate the superior performance of the proposed model c\nompared with other state-of-the-art algorithms.", "cite_num": -1}, "79": {"title": "a margin-based mle for crowdsourced partial ranking.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240597", "abstract": "", "cite_num": -1}, "84": {"title": "multi-modal preference modeling for product search.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240541", "abstract": "The visual preference of users for products has been largely ignored by the existing product search \nmethods. In this work, we propose a multi-modal personalized product search method, which aims to se\narch products which not only are relevant to the submitted textual query, but also match the user pr\neferences from both textual and visual modalities. To achieve the goal, we first leverage the also_v\niew and buy_after_viewing products to construct the visual and textual latent spaces, which are expe\ncted to preserve the visual similarity and semantic similarity of products, respectively. We then pr\nopose a translation-based search model (TranSearch ) to 1) learn a multi-modal latent space based on\n the pre-trained visual and textual latent spaces; and 2) map the users, queries and products into t\nhis space for direct matching. The TranSearch model is trained based on a comparative learning strat\negy, such that the multi-modal latent space is oriented to personalized ranking in the training stag\ne. Experiments have been conducted on real-world datasets to validate the effectiveness of our metho\nd. The results demonstrate that our method outperforms the state-of-the-art method by a large margin\n.", "cite_num": 7}, "168": {"title": "asmmc-mmac 2018: the joint workshop of 4th the workshop on affective social multimedia computing and first multi-modal affective computing of large-scale multimedia data workshop.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243724", "abstract": "Affective social multimedia computing is an emergent research topic for both affective computing and\n multimedia research communities. Social multimedia is fundamentally changing how we communicate, in\nteract, and collaborate with other people in our daily lives. Social multimedia contains much affect\nive information. Effective extraction of affective information from social multimedia can greatly he\nlp social multimedia computing (e.g., processing, index, retrieval, and understanding). Besides, wit\nh the rapid development of digital photography and social networks, people get used to sharing their\n lives and expressing their opinions online. As a result, user-generated social media data, includin\ng text, images, audios, and videos, grow rapidly, which urgently demands advanced techniques on the \nmanagement, retrieval, and understanding of these data.", "cite_num": 0}, "145": {"title": "content-based video relevance prediction with second-order relevance and attention modeling.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3266434", "abstract": "This paper describes our proposed method for the Content-Based Video Relevance Prediction (CBVRP) ch\nallenge. Our method is based on deep learning, i.e. we train a deep network to predict the relevance\n between two video sequences from their features. We explore the usage of second-order relevance, bo\nth in preparing training data, and in extending the deep network. Second-order relevance refers to e\n.g. the relevance between x and z if x is relevant to y and y is relevant to z. In our proposed meth\nod, we use second-order relevance to increase positive samples and decrease negative samples, when p\nreparing training data. We further extend the deep network with an attention module, where the atten\ntion mechanism is designed for second-order relevant video sequences. We verify the effectiveness of\n our method on the validation set of the CBVRP challenge.", "cite_num": -1}, "245": {"title": "magical rice bowl: a real-time food category changer.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241391", "abstract": "", "cite_num": -1}, "304": {"title": "la-net: layout-aware dense network for monocular depth estimation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240628", "abstract": "", "cite_num": -1}, "165": {"title": "self-boosted gesture interactive system with st-net.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240530", "abstract": "", "cite_num": -1}, "255": {"title": "personalized serious games for cognitive intervention with lifelog visual analytics.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240598", "abstract": "", "cite_num": -1}, "78": {"title": "learning discriminative features with multiple granularities for person re-identification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240552", "abstract": "", "cite_num": -1}, "206": {"title": "session details: deep-3 (image processing-inpainting, super-resolution, deblurring).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286945", "abstract": "", "cite_num": -1}, "88": {"title": "flexstream: towards flexible adaptive video streaming on end devices using extreme sdn.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240676", "abstract": "", "cite_num": -1}, "210": {"title": "tagapp visualization: an application based visual art installation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3264580", "abstract": "", "cite_num": -1}, "250": {"title": "interpretable multimodal retrieval for fashion products.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240646", "abstract": "Deep learning methods have been successfully applied to fashion retrieval. However, the latent meani\nng of learned feature vectors hinders the explanation of retrieval results and integration of user f\needback. Fortunately, there are many online shopping websites organizing fashion items into hierarch\nical structures based on product taxonomy and domain knowledge. Such structures help to reveal how h\numan perceive the relatedness among fashion products. Nevertheless, incorporating structural knowled\nge for deep learning remains a challenging problem. This paper presents techniques for organizing an\nd utilizing the fashion hierarchies in deep learning to facilitate the reasoning of search results a\nnd user intent. The novelty of our work originates from the development of an EI (Exclusive & Indepe\nndent) tree that can cooperate with deep models for end-to-end multimodal learning. EI tree organize\ns the fashion concepts into multiple semantic levels and augments the tree structure with exclusive \nas well as independent constraints. It describes the different relationships among sibling concepts \nand guides the end-to-end learning of multi-level fashion semantics. From EI tree, we learn an expli\ncit hierarchical similarity function to characterize the semantic similarities among fashion product\ns. It facilitates the interpretable retrieval scheme that can integrate the concept-level feedback. \nExperiment results on two large fashion datasets show that the proposed approach can characterize th\ne semantic similarities among fashion items accurately and capture user's search intent precisely, l\neading to more accurate search results as compared to the state-of-the-art methods.", "cite_num": 6}, "267": {"title": "paragraph generation network with visual relationship detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240695", "abstract": "", "cite_num": -1}, "93": {"title": "dynamic sound field synthesis for speech and music optimization.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240644", "abstract": "", "cite_num": -1}, "308": {"title": "video forecasting with forward-backward-net: delving deeper into spatiotemporal consistency.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240551", "abstract": "", "cite_num": -1}, "300": {"title": "session details: system-1 (video analysis & streaming).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286924", "abstract": "", "cite_num": -1}, "9": {"title": "session details: ff-2.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286917", "abstract": "", "cite_num": -1}, "85": {"title": "optimizing personalized interaction experience in crowd-interactive livecast: a cloud-edge approach.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240642", "abstract": "", "cite_num": -1}, "232": {"title": "robust correlation filter tracking with shepherded instance-aware proposals.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240709", "abstract": "", "cite_num": -1}, "31": {"title": "learning multimodal taxonomy via variational deep graph embedding and clustering.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240586", "abstract": "Taxonomy learning is an important problem and facilitates various applications such as semantic unde\nrstanding and information retrieval. Previous work for building semantic taxonomies has primarily re\nlied on labor-intensive human contributions or focused on text-based extraction. In this paper, we i\nnvestigate the problem of automatically learning multimodal taxonomies from the multimedia data on t\nhe Web. A systematic framework called Variational Deep Graph Embedding and Clustering (VDGEC) is pro\nposed consisting of two stages as concept graph construction and taxonomy induction via variational \ndeep graph embedding and clustering. VDGEC discovers hierarchical concept relationships by exploitin\ng the semantic textual-visual correspondences and contextual co-occurrences in an unsupervised manne\nr. The unstructured semantics and noisy issues of multimedia documents are carefully addressed by VD\nGEC for high quality taxonomy induction. We conduct extensive experiments on the real-world datasets\n. Experimental results demonstrate the effectiveness of the proposed framework, where VDGEC outperfo\nrms previous unsupervised approaches by a large gap.", "cite_num": 0}, "64": {"title": "context-aware visual policy network for sequence-level image captioning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240632", "abstract": "", "cite_num": -1}, "191": {"title": "jaguar: low latency mobile augmented reality with flexible tracking.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240561", "abstract": "", "cite_num": -1}, "241": {"title": "when to learn what: deep cognitive subspace clustering.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240582", "abstract": "", "cite_num": -1}, "122": {"title": "musicality-novelty generative adversarial nets for algorithmic composition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240604", "abstract": "", "cite_num": -1}, "20": {"title": "session details: keynote 2.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286918", "abstract": "", "cite_num": -1}, "159": {"title": "session details: ff-5.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286938", "abstract": "", "cite_num": -1}, "146": {"title": "cellular music: an interactive game of life sequencer.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3264577", "abstract": "", "cite_num": -1}, "94": {"title": "causally regularized learning with agnostic data selection bias.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240577", "abstract": "", "cite_num": -1}, "58": {"title": "robust billboard-based, free-viewpoint video synthesis algorithm to overcome occlusions under challenging outdoor sport scenes.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240514", "abstract": "", "cite_num": -1}, "91": {"title": "session details: grand challenge-2.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286949", "abstract": "", "cite_num": -1}, "175": {"title": "facial expression recognition in the wild: a cycle-consistent adversarial attention transfer approach.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240574", "abstract": "Facial expression recognition (FER) is a very challenging problem due to different expressions under\n arbitrary poses. Most conventional approaches mainly perform FER under laboratory controlled enviro\nnment. Different from existing methods, in this paper, we formulate the FER in the wild as a domain \nadaptation problem, and propose a novel auxiliary domain guided Cycle-consistent adversarial Attenti\non Transfer model (CycleAT) for simultaneous facial image synthesis and facial expression recognitio\nn in the wild. The proposed model utilizes large-scale unlabeled web facial images as an auxiliary d\nomain to reduce the gap between source domain and target domain based on generative adversarial netw\norks (GAN) embedded with an effective attention transfer module, which enjoys several merits. First,\n the GAN-based method can automatically generate labeled facial images in the wild through harnessin\ng information from labeled facial images in source domain and unlabeled web facial images in auxilia\nry domain. Second, the class-discriminative spatial attention maps from the classifier in source dom\nain are leveraged to boost the performance of the classifier in target domain. Third, it can effecti\nvely preserve the structural consistency of local pixels and global attributes in the synthesized fa\ncial images through pixel cycle-consistency and discriminative loss. Quantitative and qualitative ev\naluations on two challenging in-the-wild datasets demonstrate that the proposed model performs favor\nably against state-of-the-art methods.", "cite_num": -1}, "251": {"title": "mahci 2018: the 1st workshop on multimedia for accessible human computer interface.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243723", "abstract": "", "cite_num": -1}, "148": {"title": "end2end semantic segmentation for 3d indoor scenes.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243933", "abstract": "", "cite_num": -1}, "104": {"title": "graphnet: learning image pseudo annotations for weakly-supervised semantic segmentation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240542", "abstract": "", "cite_num": -1}, "286": {"title": "somin.ai: social multimedia influencer discovery marketplace.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241387", "abstract": "", "cite_num": -1}, "87": {"title": "structural inpainting.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240678", "abstract": "", "cite_num": -1}, "179": {"title": "deep adaptive temporal pooling for activity recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240713", "abstract": "", "cite_num": -1}, "114": {"title": "object-difference attention: a simple relational attention for visual question answering.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240513", "abstract": "Attention mechanism has greatly promoted the development of Visual Question Answering (VQA). Attenti\non distribution, which weights differently on objects (such as image regions or bounding boxes) in a\nn image according to their importance for answering a question, plays a crucial role in attention me\nchanism. Most of the existing work focuses on fusing image features and text features to calculate t\nhe attention distribution without comparisons between different image objects. As a major property o\nf attention, selectivity depends on comparisons between different objects. Comparisons provide more \ninformation for assigning attentions better. For achieving this, we propose an object-difference att\nention (ODA) which calculates the probability of attention by implementing difference operator betwe\nen different image objects in an image under the guidance of questions in hand. Experimental results\n on three publicly available datasets show our ODA based VQA model achieves the state-of-the-art res\nults. Furthermore, a general form of relational attention is proposed. Besides ODA, several other re\nlational attentions are given. Experimental results show those relational attentions have strengths \non different types of questions.", "cite_num": -1}, "39": {"title": "face-voice matching using cross-modal embeddings.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240601", "abstract": "", "cite_num": -1}, "50": {"title": "vivid: virtual environment for visual deep learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243653", "abstract": "", "cite_num": -1}, "7": {"title": "decode human life from social media.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243935", "abstract": "", "cite_num": -1}, "237": {"title": "ihuman3d: intelligent human body 3d reconstruction using a single flying camera.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240600", "abstract": "", "cite_num": -1}, "27": {"title": "interactive story maker: tagged video retrieval system for video re-creation service.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241401", "abstract": "", "cite_num": -1}, "261": {"title": "a unified generative adversarial framework for image generation and person re-identification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240573", "abstract": "", "cite_num": -1}, "108": {"title": "decoupled novel object captioner.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240640", "abstract": "", "cite_num": -1}, "63": {"title": "altmm 2018 - 3rd international workshop on multimedia alternate realities.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243718", "abstract": "", "cite_num": -1}, "176": {"title": "jpeg decompression in the homomorphic encryption domain.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240672", "abstract": "", "cite_num": -1}, "143": {"title": "cross-modal moment localization in videos.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240549", "abstract": "", "cite_num": -1}, "238": {"title": "multi-scale correlation for sequential cross-modal hashing learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240560", "abstract": "Cross-modal hashing aims to learn hash functions, which map heterogeneous multimedia data into commo\nn Hamming space for fast and flexible cross-modal retrieval. Recently, several cross-modal hashing m\nethods learn the hash functions by mining the correlation among multimedia data. However, they ignor\ne two properties of cross-modal data: 1) The features of different scale in single modality consist \ndifferent information, such as texture, object and scene feature in the image, which can provide mul\nti-scale information on retrieval task. 2) The correlation among the features of different modalitie\ns and scales can provide multi-scale relationship for better cross-modal hashing learning. In this p\naper, we propose Multi-scale Correlation Sequential Cross-modal Hashing Learning (MCSCH) approach. T\nhe main contributions of the MCSCH can be summarized as follows: 1) We propose a multi-scale feature\n guided sequential hashing learning method which sequentially generates the hash code guided by diff\nerent scale features through a RNN based network. The multi-scale feature guided sequential hashing \nlearning method utilizes the scale information, which enhances the diversity of the hash codes and r\neduces the error caused by extreme situation in specifc features. 2) We propose a multi-scale correl\nation mining strategy during the multi-scale feature guided sequential hashing learning, which can s\nimultaneously mine the correlation among the features of different modalities and scales. Through th\nis strategy, we can mine any pair of scale features in different modalities and obtain abundant scal\ne correlation for better cross-modal retrieval. Experiments on two widely-used datasets demonstrate \nthe effectiveness of our proposed MCSCH approach.", "cite_num": 1}, "246": {"title": "session details: deep-2 (recognition).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286931", "abstract": "", "cite_num": -1}, "32": {"title": "explore multi-step reasoning in video question answering.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240563", "abstract": "", "cite_num": -1}, "198": {"title": "conditional expression synthesis with face parsing transformation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240647", "abstract": "", "cite_num": -1}, "208": {"title": "temporal cross-media retrieval with soft-smoothing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240665", "abstract": "", "cite_num": -1}, "200": {"title": "generating defensive plays in basketball games.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240670", "abstract": "", "cite_num": -1}, "234": {"title": "regional maximum activations of convolutions with attention for cross-domain beauty and personal care product retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3266436", "abstract": "Cross-domain beauty and personal care product image retrieval is a challenging problem due to data v\nariations (e.g., brightness, viewpoint, and scale), and the rich types of items. In this paper, we p\nresent a regional maximum activations of convolutions with attention (RA-MAC) descriptor to extract \nimage features for retrieval. RA-MAC improves the regional maximum activations of convolutions (R-MA\nC) descriptor considering the influence of background in cross-domain images (i.e., shopper domain a\nnd seller domain). More specifically, RA-MAC utilizes the characteristics of the convolutional layer\n to find the attention of an image, and reduces the influence of the unimportant regions in an unsup\nervised manner. Furthermore, a few strategies have been exploited to improve the performance, such a\ns multiple features fusion, query expansion, and database augmentation. Extensive experiments conduc\nted on a dataset consisting of half a million images of beauty care products (Perfect-500K) manifest\n the effectiveness of RA-MAC. Our approach achieves the 2nd place in the leader board of the Grand C\nhallenge of AI Meets Beauty in ACM Multimedia 2018. Our code is available at: https://github.com/Ret\nrainIt/Perfect-Half-Million-Beauty-Product-Image-Recognition-Challenge.", "cite_num": -1}, "184": {"title": "video-to-video translation with global temporal consistency.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240708", "abstract": "", "cite_num": -1}, "195": {"title": "post tuned hashing: a new approach to indexing high-dimensional data.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240529", "abstract": "", "cite_num": -1}, "21": {"title": "cross-modal health state estimation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241913", "abstract": "", "cite_num": -1}, "76": {"title": "visual domain adaptation with manifold embedded distribution alignment.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240512", "abstract": "", "cite_num": -1}, "28": {"title": "learning semantic structure-preserved embeddings for cross-modal retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240521", "abstract": "", "cite_num": -1}, "51": {"title": "alert: adding a secure layer in decision support for advanced driver assistance system (adas).", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241912", "abstract": "", "cite_num": -1}, "264": {"title": "ai + multimedia make better life?", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243932", "abstract": "", "cite_num": -1}, "42": {"title": "previewer for multi-scale object detector.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240544", "abstract": "", "cite_num": -1}, "231": {"title": "thoughtviz: visualizing human thoughts using generative adversarial network.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240641", "abstract": "", "cite_num": -1}, "105": {"title": "temporal hierarchical attention at category- and item-level for micro-video click-through prediction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240617", "abstract": "... but we have temporarily restricted your access to the Digital Library.\nYour activity appears to \nbe coming from some type of automated process.\nTo ensure the availability of the Digital Library we \ncan not allow these types of requests to continue.\nThe restriction will be removed automatically onc\ne this activity stops.\n", "cite_num": -1}, "311": {"title": "cross-domain adversarial feature learning for sketch re-identification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240606", "abstract": "", "cite_num": -1}, "259": {"title": "slions: a karaoke application to enhance foreign language learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240691", "abstract": "", "cite_num": -1}, "134": {"title": "perceptual temporal incoherence aware stereo video retargeting.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240682", "abstract": "", "cite_num": -1}, "34": {"title": "an effective text-based characterization combined with numerical features for social media headline prediction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3266438", "abstract": "", "cite_num": -1}, "13": {"title": "a large-scale rgb-d database for arbitrary-view human action recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240675", "abstract": "", "cite_num": -1}, "217": {"title": "session details: doctoral symposium.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286928", "abstract": "", "cite_num": -1}, "12": {"title": "user-guided deep anime line art colorization with conditional adversarial networks.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240661", "abstract": "", "cite_num": -1}, "5": {"title": "emotiongan: unsupervised domain adaptation for learning discrete probability distributions of image emotions.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240591", "abstract": "", "cite_num": -1}, "171": {"title": "examine before you answer: multi-task learning with adaptive-attentions for multiple-choice vqa.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240687", "abstract": "Multiple-choice (MC) Visual Question Answering (VQA) is a similar but essentially different task to \nopen-ended VQA because the answer options are provided. Most of existing works tackle them in a unif\nied pipeline by solving a multi-class problem to infer the best answer from a predefined answer set.\n The option that matches the best answer is selected for MC VQA. Nevertheless, this violates human t\nhinking logics. Normally, people examine the questions, answer options and the reference image befor\ne inferring a MC VQA. For MC VQA, human either rely on the question and answer options to directly d\neduce a correct answer if the question is not image-related, or read the question and answer options\n and then purposefully search for answers in a reference image. Therefore, we propose a novel approa\nch, namely Multi-task Learning with Adaptive-attention (MTA), to simulate human logics for MC VQA. S\npecifically, we first fuse the answer options and question features, and then adaptively attend to t\nhe visual features for inferring a MC VQA. Furthermore, we design our model as a multi-task learning\n architecture by integrating the open-ended VQA task to further boost the performance of MC VQA. We \nevaluate our approach on two standard benchmark datasets: VQA and Visual7W and our approach sets new\n records on both datasets for MC VQA task, reaching 73.5% and 65.9% average accuracy respectively.", "cite_num": -1}, "140": {"title": "miniview layout for bandwidth-efficient 360-degree video.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240705", "abstract": "", "cite_num": -1}, "25": {"title": "fashion sensitive clothing recommendation using hierarchical collocation model.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240596", "abstract": "", "cite_num": -1}, "185": {"title": "dense auto-encoder hashing for robust cross-modality retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240684", "abstract": "", "cite_num": -1}, "182": {"title": "participation-contributed temporal dynamic model for group activity recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240572", "abstract": "", "cite_num": -1}, "144": {"title": "cls: a cross-user learning based system for improving qoe in 360-degree video adaptive streaming.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240556", "abstract": "", "cite_num": -1}, "172": {"title": "adaptive temporal encoding network for video instance-level human parsing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240660", "abstract": "", "cite_num": -1}, "228": {"title": "icoobook: when the picture book for children encounters aesthetics of interaction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241397", "abstract": "", "cite_num": -1}, "10": {"title": "detecting abnormality without knowing normality: a two-stage approach for unsupervised video abnormal event detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240615", "abstract": "... but we have temporarily restricted your access to the Digital Library.\nYour activity appears to \nbe coming from some type of automated process.\nTo ensure the availability of the Digital Library we \ncan not allow these types of requests to continue.\nThe restriction will be removed automatically onc\ne this activity stops.\n", "cite_num": 0}, "111": {"title": "boosting scene parsing performance via reliable scale prediction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240657", "abstract": "", "cite_num": -1}, "227": {"title": "scratch: a scalable discrete matrix factorization hashing for cross-modal retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240547", "abstract": "", "cite_num": -1}, "125": {"title": "deep learning for multimedia: science or technology?", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243931", "abstract": "", "cite_num": -1}, "302": {"title": "attentive interactive convolutional matching for community question answering in social multimedia.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240626", "abstract": "", "cite_num": -1}, "265": {"title": "deep learning interpretation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241472", "abstract": "", "cite_num": -1}, "6": {"title": "structure guided photorealistic style transfer.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240637", "abstract": "", "cite_num": -1}, "17": {"title": "joint global and co-attentive representation learning for image-sentence retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240535", "abstract": "", "cite_num": -1}, "61": {"title": "sparsely grouped multi-task generative adversarial networks for facial attribute manipulation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240594", "abstract": "", "cite_num": -1}, "22": {"title": "sibnet: sibling convolutional encoder for video captioning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240667", "abstract": "", "cite_num": -1}, "149": {"title": "from volcano to toyshop: adaptive discriminative region discovery for scene recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240698", "abstract": "", "cite_num": -1}, "174": {"title": "a unified framework for multimodal domain adaptation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240633", "abstract": "Domain adaptation aims to train a model on labeled data from a source domain while minimizing test e\nrror on a target domain. Most of existing domain adaptation methods only focus on reducing domain sh\nift of single-modal data. In this paper, we consider a new problem of multimodal domain adaptation a\nnd propose a unified framework to solve it. The proposed multimodal domain adaptation neural network\ns(MDANN) consist of three important modules. (1) A covariant multimodal attention is designed to lea\nrn a common feature representation for multiple modalities. (2) A fusion module adaptively fuses att\nended features of different modalities. (3) Hybrid domain constraints are proposed to comprehensivel\ny learn domain-invariant features by constraining single modal features, fused features, and attenti\non scores. Through jointly attending and fusing under an adversarial objective, the most discriminat\nive and domain-adaptive parts of the features are adaptively fused together. Extensive experimental \nresults on two real-world cross-domain applications (emotion recognition and cross-media retrieval) \ndemonstrate the effectiveness of the proposed method.", "cite_num": 1}, "295": {"title": "investigation of small group social interactions using deep visual activity-based nonverbal features.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240685", "abstract": "", "cite_num": -1}, "82": {"title": "session details: ff-4.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286929", "abstract": "", "cite_num": -1}, "92": {"title": "drawing in a virtual 3d space - introducing vr drawing in elementary school art education.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240692", "abstract": "", "cite_num": -1}, "222": {"title": "multi-scale context attention network for image retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240602", "abstract": "Recent attempts on the Convolutional Neural Network (CNN) based image retrieval usually adopt the ou\ntput of a specific convolutional or fully connected layer as feature representation. Though superior\n representation capability has yielded better retrieval performance, the scale variation and clutter\n distracting remain to be two challenging problems in CNN based image retrieval. In this work, we pr\nopose a Multi-Scale Context Attention Network (MSCAN) to generate global descriptors, which is able \nto selectively focus on the informative regions with the assistance of multi-scale context informati\non. We model the multi-scale context information by an improved Long Short-Term Memory (LSTM) networ\nk across different layers. As such, the proposed global descriptor is equipped with the scale aware \nattention capability. Experimental results show that our proposed method can effectively capture the\n informative regions in images and retain reliable attention responses when encountering scale varia\ntion and clutter distracting. Moreover, we compare the performance of the proposed scheme with the s\ntate-of-the-art global descriptors, and extensive results verify that the proposed MSCAN can achieve\n superior performance on several image retrieval benchmarks.", "cite_num": 0}, "77": {"title": "session details: best paper session.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286927", "abstract": "", "cite_num": -1}, "142": {"title": "learning local descriptors with adversarial enhancer from volumetric geometry patches.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240666", "abstract": "", "cite_num": -1}, "164": {"title": "unregularized auto-encoder with generative adversarial networks for image generation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240569", "abstract": "", "cite_num": -1}, "153": {"title": "twitter sentiment analysis via bi-sense emoji embedding and attention-based lstm.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240533", "abstract": "Sentiment analysis on large-scale social media data is important to bridge the gaps between social m\nedia contents and real world activities including political election prediction, individual and publ\nic emotional status monitoring and analysis, and so on. Although textual sentiment analysis has been\n well studied based on platforms such as Twitter and Instagram, analysis of the role of extensive em\noji uses in sentiment analysis remains light. In this paper, we propose a novel scheme for Twitter s\nentiment analysis with extra attention on emojis. We first learn bi-sense emoji embeddings under pos\nitive and negative sentimental tweets individually, and then train a sentiment classifier by attendi\nng on these bi-sense emoji embeddings with an attention-based long short-term memory network (LSTM).\n Our experiments show that the bi-sense embedding is effective for extracting sentiment-aware embedd\nings of emojis and outperforms the state-of-the-art models. We also visualize the attentions to show\n that the bi-sense emoji embedding provides better guidance on the attention mechanism to obtain a m\nore robust understanding of the semantics and sentiments.", "cite_num": -1}, "167": {"title": "gnas: a greedy neural architecture search method for multi-attribute learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240588", "abstract": "", "cite_num": -1}, "204": {"title": "give me one portrait image, i will tell you your emotion and personality.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241384", "abstract": "", "cite_num": -1}, "270": {"title": "from data to knowledge: deep learning model compression, transmission and communication.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240654", "abstract": "", "cite_num": -1}, "44": {"title": "real-time 3d face-eye performance capture of a person wearing vr headset.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240570", "abstract": "", "cite_num": -1}, "173": {"title": "session details: demo + video + makers' program.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286930", "abstract": "", "cite_num": -1}, "15": {"title": "session details: multimedia-1 (multimedia recommendation & discovery).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286921", "abstract": "", "cite_num": -1}, "72": {"title": "session details: multimodal-1 (multimodal reasoning).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286923", "abstract": "... but we have temporarily restricted your access to the Digital Library.\nYour activity appears to \nbe coming from some type of automated process.\nTo ensure the availability of the Digital Library we \ncan not allow these types of requests to continue.\nThe restriction will be removed automatically onc\ne this activity stops.\n", "cite_num": 0}, "163": {"title": "when deep fool meets deep prior: adversarial attack on super-resolution network.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240603", "abstract": "This paper investigates the vulnerability of the deep prior used in deep learning based image restor\nation. In particular, the image super-resolution, which relies on the strong prior information to re\ngularize the solution space and plays important roles in the image pre-processing for future viewing\n and analysis, is shown to be vulnerable to the well-designed adversarial examples. We formulate the\n adversarial example generation process as an optimization problem, and given super-resolution model\n three different types of attack are designed based on the subsequent tasks: (i) style transfer atta\nck; (ii) classification attack; (iii) caption attack. Another interesting property of our design is \nthat the attack is hidden behind the super-resolution process, such that the utilization of low reso\nlution images is not significantly influenced. We show that the vulnerability to adversarial example\ns could bring risks to the pre-processing modules such as super-resolution deep neural network, whic\nh is also of paramount significance for the security of the whole system. Our results also shed ligh\nt on the potential security issues of the pre-processing modules, and raise concerns regarding the c\norresponding countermeasures for adversarial examples.", "cite_num": 1}, "249": {"title": "multi-label image classification via knowledge distillation from weakly-supervised detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240567", "abstract": "", "cite_num": -1}, "18": {"title": "fine-grained grocery product recognition by one-shot learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240522", "abstract": "", "cite_num": -1}, "279": {"title": "fast parameter adaptation for few-shot image captioning and visual question answering.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240527", "abstract": "", "cite_num": -1}, "101": {"title": "session details: brand new ideas.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286946", "abstract": "", "cite_num": -1}, "229": {"title": "random forest exploiting post-related and user-related features for social media popularity prediction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3266439", "abstract": "", "cite_num": -1}, "119": {"title": "modanet: a large-scale street fashion dataset with polygon annotations.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240652", "abstract": "", "cite_num": -1}, "118": {"title": "a distributed approach for bitrate selection in http adaptive streaming.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240589", "abstract": "", "cite_num": -1}, "178": {"title": "an iterative refinement approach for social media headline prediction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3266443", "abstract": "", "cite_num": -1}, "188": {"title": "deep multimodal image-repurposing detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240707", "abstract": "Nefarious actors on social media and other platforms often spread rumors and falsehoods through imag\nes whose metadata (e.g., captions) have been modified to provide visual substantiation of the rumor/\nfalsehood. This type of modification is referred to as image repurposing, in which often an unmanipu\nlated image is published along with incorrect or manipulated metadata to serve the actor's ulterior \nmotives. We present the Multimodal Entity Image Repurposing (MEIR) dataset, a substantially challeng\ning dataset over that which has been previously available to support research into image repurposing\n detection. The new dataset includes location, person, and organization manipulations on real-world \ndata sourced from Flickr. We also present a novel, end-to-end, deep multimodal learning model for as\nsessing the integrity of an image by combining information extracted from the image with related inf\normation from a knowledge base. The proposed method is compared against state-of-the-art techniques \non existing datasets as well as MEIR, where it outperforms existing methods across the board, with A\nUC improvement up to 0.23.", "cite_num": 3}, "98": {"title": "dance with melody: an lstm-autoencoder approach to music-oriented dance synthesis.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240526", "abstract": "", "cite_num": -1}, "30": {"title": "attention and language ensemble for scene text recognition with convolutional sequence modeling.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240571", "abstract": "Recent dominant approaches for scene text recognition are mainly based on convolutional neural netwo\nrk (CNN) and recurrent neural network (RNN), where the CNN processes images and the RNN generates ch\naracter sequences. Different from these methods, we propose an attention-based architecture1 which i\ns completely based on CNNs. The distinctive characteristics of our method include: (1) the method fo\nllows encoder-decoder architecture, in which the encoder is a two-dimensional residual CNN and the d\necoder is a deep one-dimensional CNN. (2) An attention module that captures visual cues, and a langu\nage module that models linguistic rules are designed equally in the decoder. Therefore the attention\n and language can be viewed as an ensemble to boost predictions jointly. (3) Instead of using a sing\nle loss from language aspect, multiple losses from attention and language are accumulated for traini\nng the networks in an end-to-end way. We conduct experiments on standard datasets for scene text rec\nognition, including Street View Text, IIIT5K and ICDAR datasets. The experimental results show our C\nNN-based method has achieved state-of-the-art performance on several benchmark datasets, even withou\nt the use of RNN.", "cite_num": 3}, "90": {"title": "healthmedia 2018: third international workshop on multimedia for personal health and health care.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243722", "abstract": "", "cite_num": -1}, "293": {"title": "anidance: real-time dance motion synthesize to the song.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241388", "abstract": "", "cite_num": -1}, "137": {"title": "don't just look - smell, taste, and feel the interaction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3267343", "abstract": "", "cite_num": -1}, "74": {"title": "svias: scene-segmented video information annotation system.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241400", "abstract": "", "cite_num": -1}, "24": {"title": "residual-guide network for single image deraining.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240694", "abstract": "", "cite_num": -1}, "151": {"title": "fast and light manifold cnn based 3d facial expression recognition across pose variations.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240568", "abstract": "", "cite_num": -1}, "81": {"title": "cropnet: real-time thumbnailing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240517", "abstract": "", "cite_num": -1}, "53": {"title": "predicting visual context for unsupervised event segmentation in continuous photo-streams.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240624", "abstract": "", "cite_num": -1}, "89": {"title": "structured deep learning for pixel-level understanding.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241469", "abstract": "", "cite_num": -1}, "216": {"title": "multi-human parsing machines.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240515", "abstract": "", "cite_num": -1}, "16": {"title": "knowledge-aware multimodal fashion chatbot.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241399", "abstract": "Multimodal fashion chatbot provides a natural and informative way to fulfill customers' fashion need\ns. However, making it 'smart' in generating substantive responses remains a challenging problem. In \nthis paper, we present a multimodal domain knowledge enriched fashion chatbot. It forms a taxonomy-b\nased learning module to capture the fine-grained semantics in images and leverages an end-to-end neu\nral conversational model to generate responses based on the conversation history, visual semantics, \nand domain knowledge. To avoid inconsistent dialogues, deep reinforcement learning method is used to\n further optimize the model.", "cite_num": 1}, "115": {"title": "style separation and synthesis via generative adversarial networks.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240524", "abstract": "", "cite_num": -1}, "126": {"title": "session details: vision-2 (object & scene understanding).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286922", "abstract": "", "cite_num": -1}, "139": {"title": "cascaded feature augmentation with diffusion for image retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240532", "abstract": "", "cite_num": -1}, "187": {"title": "fov-aware edge caching for adaptive 360\u00b0 video streaming.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240680", "abstract": "", "cite_num": -1}, "253": {"title": "slackliner - an interactive slackline training assistant.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240537", "abstract": "", "cite_num": -1}, "301": {"title": "unsupervised learning of 3d model reconstruction from hand-drawn sketches.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240699", "abstract": "", "cite_num": -1}, "80": {"title": "supervised online hashing via hadamard codebook learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240519", "abstract": "", "cite_num": -1}, "235": {"title": "chipgan: a generative adversarial network for chinese ink wash painting style transfer.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240655", "abstract": "", "cite_num": -1}, "272": {"title": "end-to-end blind quality assessment of compressed videos using deep neural networks.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240643", "abstract": "", "cite_num": -1}, "43": {"title": "multi-view image generation from a single-view.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240536", "abstract": "", "cite_num": -1}, "309": {"title": "beyond narrative description: generating poetry from images by multi-adversarial training.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240587", "abstract": "", "cite_num": -1}, "268": {"title": "similarity-based processing of motion capture data.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241468", "abstract": "", "cite_num": -1}, "66": {"title": "cross-species learning: a low-cost approach to learning human fight from animal fight.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240710", "abstract": "", "cite_num": -1}, "69": {"title": "the importance of medical multimedia.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241475", "abstract": "", "cite_num": -1}, "298": {"title": "to recognize families in the wild: a machine vision tutorial.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241471", "abstract": "", "cite_num": -1}, "46": {"title": "session details: grand challenge-1.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286947", "abstract": "", "cite_num": -1}, "75": {"title": "understanding humans in crowded scenes: deep nested adversarial learning and a new benchmark for multi-human parsing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240509", "abstract": "", "cite_num": -1}, "189": {"title": "attention-based pyramid aggregation network for visual place recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240525", "abstract": "Visual place recognition is challenging in the urban environment and is usually viewed as a large sc\nale image retrieval task. The intrinsic challenges in place recognition exist that the confusing obj\nects such as cars and trees frequently occur in the complex urban scene, and buildings with repetiti\nve structures may cause over-counting and the burstiness problem degrading the image representations\n. To address these problems, we present an Attention-based Pyramid Aggregation Network (APANet), whi\nch is trained in an end-to-end manner for place recognition. One main component of APANet, the spati\nal pyramid pooling, can effectively encode the multi-size buildings containing geo-information. The \nother one, the attention block, is adopted as a region evaluator for suppressing the confusing regio\nnal features while highlighting the discriminative ones. When testing, we further propose a simple y\net effective PCA power whitening strategy, which significantly improves the widely used PCA whitenin\ng by reasonably limiting the impact of over-counting. Experimental evaluations demonstrate that the \nproposed APANet outperforms the state-of-the-art methods on two place recognition benchmarks, and ge\nneralizes well on standard image retrieval datasets.", "cite_num": 7}, "273": {"title": "session details: keynote 1.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286916", "abstract": "", "cite_num": -1}, "223": {"title": "dest-resnet: a deep spatiotemporal residual network for hotspot traffic speed prediction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240656", "abstract": "", "cite_num": -1}, "157": {"title": "webly supervised joint embedding for cross-modal image-text retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240712", "abstract": "", "cite_num": -1}, "0": {"title": "coview'18: the 1st workshop and challenge on comprehensive video understanding in the wild.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243720", "abstract": "", "cite_num": -1}, "135": {"title": "few-shot adaptation for multimedia semantic indexing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240592", "abstract": "", "cite_num": -1}, "209": {"title": "connectionist temporal fusion for sign language translation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240671", "abstract": "", "cite_num": -1}, "260": {"title": "meet ar-bot: meeting anywhere, anytime with movable spatial ar robot.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241390", "abstract": "", "cite_num": -1}, "95": {"title": "session details: panel-1.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286933", "abstract": "", "cite_num": -1}, "287": {"title": "attribute-aware attention model for fine-grained representation learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240550", "abstract": "How to learn a discriminative fine-grained representation is a key point in many computer vision app\nlications, such as person re-identification, fine-grained classification, fine-grained image retriev\nal, etc. Most of the previous methods focus on learning metrics or ensemble to derive better global \nrepresentation, which are usually lack of local information. Based on the considerations above, we p\nropose a novel Attribute-Aware Attention Model ($A^3M$), which can learn local attribute representat\nion and global category representation simultaneously in an end-to-end manner. The proposed model co\nntains two attention models: attribute-guided attention module uses attribute information to help se\nlect category features in different regions, at the same time, category-guided attention module sele\ncts local features of different attributes with the help of category cues. Through this attribute-ca\ntegory reciprocal process, local and global features benefit from each other. Finally, the resulting\n feature contains more intrinsic information for image recognition instead of the noisy and irreleva\nnt features. Extensive experiments conducted on Market-1501, CompCars, CUB-200-2011 and CARS196 demo\nnstrate the effectiveness of our $A^3M$.", "cite_num": 4}, "289": {"title": "context-dependent diffusion network for visual relationship detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240668", "abstract": "", "cite_num": -1}, "62": {"title": "what dress fits me best?: fashion recommendation on the clothing style for personal body shape.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240546", "abstract": "", "cite_num": -1}, "128": {"title": "qarc: video quality aware rate control for real-time video streaming based on deep reinforcement learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240545", "abstract": "", "cite_num": -1}, "303": {"title": "cloudvr: cloud accelerated interactive mobile virtual reality.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240620", "abstract": "", "cite_num": -1}, "120": {"title": "only learn one sample: fine-grained visual categorization with one sample training.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240557", "abstract": "", "cite_num": -1}, "280": {"title": "deep priority hashing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240543", "abstract": "", "cite_num": -1}, "213": {"title": "a demonstration of an intelligent storytelling system.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241396", "abstract": "", "cite_num": -1}, "214": {"title": "reconfigurable inverted index.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240630", "abstract": "", "cite_num": -1}, "207": {"title": "session details: deep-1 (image translation).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286919", "abstract": "", "cite_num": -1}, "162": {"title": "ispa-net: iterative semantic pose alignment network.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240650", "abstract": "", "cite_num": -1}, "33": {"title": "session details: keynote 4.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286939", "abstract": "", "cite_num": -1}, "292": {"title": "comprehensive distance-preserving autoencoders for cross-modal retrieval.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240607", "abstract": "", "cite_num": -1}, "247": {"title": "circe: real-time caching for instance recognition on cloud environments and multi-core architectures.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240697", "abstract": "", "cite_num": -1}, "180": {"title": "session details: panel-2.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286937", "abstract": "", "cite_num": -1}, "70": {"title": "an end-to-end quadrilateral regression network for comic panel extraction.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240555", "abstract": "", "cite_num": -1}, "312": {"title": "pseudo transfer with marginalized corrupted attribute for zero-shot learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240715", "abstract": "", "cite_num": -1}, "196": {"title": "human conversation analysis using attentive multimodal networks with hierarchical encoder-decoder.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240714", "abstract": "Human conversation analysis is challenging because the meaning can be expressed through words, inton\nation, or even body language and facial expression. We introduce a hierarchical encoder-decoder stru\ncture with attention mechanism for conversation analysis. The hierarchical encoder learns word-level\n features from video, audio, and text data that are then formulated into conversation-level features\n. The corresponding hierarchical decoder is able to predict different attributes at given time insta\nnces. To integrate multiple sensory inputs, we introduce a novel fusion strategy with modality atten\ntion. We evaluated our system on published emotion recognition, sentiment analysis, and speaker trai\nt analysis datasets. Our system outperformed previous state-of-the-art approaches in both classifica\ntion and regressions tasks on three datasets. We also outperformed previous approaches in generaliza\ntion tests on two commonly used datasets. We achieved comparable performance in predicting co-existi\nng labels using the proposed model instead of multiple individual models. In addition, the easily-vi\nsualized modality and temporal attention demonstrated that the proposed attention mechanism helps fe\nature selection and improves model interpretability.", "cite_num": 0}, "243": {"title": "deep triplet quantization.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240516", "abstract": "", "cite_num": -1}, "86": {"title": "an admm-based universal framework for adversarial attacks on deep neural networks.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240639", "abstract": "Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That is, adversarial exampl\nes, obtained by adding delicately crafted distortions onto original legal inputs, can mislead a DNN \nto classify them as any target labels. In a successful adversarial attack, the targeted mis-classifi\ncation should be achieved with the minimal distortion added. In the literature, the added distortion\ns are usually measured by $L_0$, $L_1$, $L_2$, and $L_\\infty $ norms, namely, L_0, L_1, L_2, and L_\u221e\n attacks, respectively. However, there lacks a versatile framework for all types of adversarial atta\ncks. This work for the first time unifies the methods of generating adversarial examples by leveragi\nng ADMM (Alternating Direction Method of Multipliers), an operator splitting optimization approach, \nsuch that $L_0$, $L_1$, $L_2$, and $L_\\infty $ attacks can be effectively implemented by this genera\nl framework with little modifications. Comparing with the state-of-the-art attacks in each category,\n our ADMM-based attacks are so far the strongest, achieving both the 100% attack success rate and th\ne minimal distortion.", "cite_num": 6}, "11": {"title": "semi-supervised dff: decoupling detection and feature flow for video object detectors.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240693", "abstract": "", "cite_num": -1}, "211": {"title": "step-by-step erasion, one-by-one collection: a weakly supervised temporal action detector.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240511", "abstract": "", "cite_num": -1}, "156": {"title": "web-based configurable image annotations.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243656", "abstract": "", "cite_num": -1}, "271": {"title": "life-long cross-media correlation learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240558", "abstract": "", "cite_num": -1}, "117": {"title": "generative adversarial product quantisation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240590", "abstract": "", "cite_num": -1}, "186": {"title": "what has art got to do with it?", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3267345", "abstract": "", "cite_num": -1}, "212": {"title": "a general-purpose distributed programming system using data-parallel streams.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243654", "abstract": "", "cite_num": -1}, "275": {"title": "pob: toward reasoning patterns of beauty in image data.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240711", "abstract": "", "cite_num": -1}, "26": {"title": "the effect of foveation on high dynamic range video perception.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240703", "abstract": "", "cite_num": -1}, "37": {"title": "attentive recurrent neural network for weak-supervised multi-label image classification.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240649", "abstract": "", "cite_num": -1}, "55": {"title": "usar: an interactive user-specific aesthetic ranking framework for images.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240635", "abstract": "", "cite_num": -1}, "310": {"title": "photo squarization by deep multi-operator retargeting.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240623", "abstract": "", "cite_num": -1}, "40": {"title": "dissimilarity representation learning for generalized zero-shot recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240686", "abstract": "", "cite_num": -1}, "131": {"title": "personalized multiple facial action unit recognition through generative adversarial recognition network.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240613", "abstract": "", "cite_num": -1}, "202": {"title": "demonstration of an open source framework for qualitative evaluation of cbir systems.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241395", "abstract": "", "cite_num": -1}, "154": {"title": "feature re-learning with data augmentation for content-based video recommendation.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3266441", "abstract": "", "cite_num": -1}, "290": {"title": "mining semantics-preserving attention for group activity recognition.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240576", "abstract": "In this paper, we propose a Semantics-Preserving Teacher-Student (SPTS) model for group activity rec\nognition in videos, which aims to mine the semantics-preserving attention to automatically seek the \nkey people and discard the misleading people. Conventional methods usually aggregate the features ex\ntracted from individual persons by pooling operations, which cannot fully explore the contextual inf\normation for group activity recognition. To address this, our SPTS networks first learn a Teacher Ne\ntwork in semantic domain, which classifies the word of group activity based on the words of individu\nal actions. Then we carefully design a Student Network in vision domain, which recognizes the group \nactivity according to the input videos, and enforce the Student Network to mimic the Teacher Network\n during the learning process. In this way, we allocate semantics-preserving attention to different p\neople, which adequately explores the contextual information of different people and requires no extr\na labelled data. Experimental results on two widely used benchmarks for group activity recognition c\nlearly show the superior performance of our method in comparisons with the state-of-the-arts.", "cite_num": 1}, "1": {"title": "images2poem: generating chinese poetry from image streams.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241910", "abstract": "", "cite_num": -1}, "3": {"title": "bridge the gap between vqa and human behavior on omnidirectional video: a large-scale dataset and a deep learning model.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240581", "abstract": "", "cite_num": -1}, "132": {"title": "attentive crowd flow machines.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240681", "abstract": "", "cite_num": -1}, "129": {"title": "cilantro: a lean, versatile, and efficient library for point cloud data processing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243655", "abstract": "", "cite_num": -1}, "41": {"title": "ee-usad: acm mm 2018workshop on understandingsubjective attributes of data focus on evoked emotions.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3243721", "abstract": "", "cite_num": -1}, "96": {"title": "non-locally enhanced encoder-decoder network for single image de-raining.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240636", "abstract": "", "cite_num": -1}, "177": {"title": "an efficient deep quantized compressed sensing coding framework of natural images.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240706", "abstract": "", "cite_num": -1}, "133": {"title": "tracking-assisted weakly supervised online visual object segmentation in unconstrained videos.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240638", "abstract": "", "cite_num": -1}, "170": {"title": "incremental deep hidden attribute learning.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240510", "abstract": "", "cite_num": -1}, "19": {"title": "trusted guidance pyramid network for human parsing.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240634", "abstract": "", "cite_num": -1}, "161": {"title": "human behavior understanding: from action recognition to complex event detection.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3241474", "abstract": "", "cite_num": -1}, "141": {"title": "knowledge-aware multimodal dialogue systems.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240605", "abstract": "By offering a natural way for information seeking, multimodal dialogue systems are attracting increa\nsing attention in several domains such as retail, travel etc. However, most existing dialogue system\ns are limited to textual modality, which cannot be easily extended to capture the rich semantics in \nvisual modality such as product images. For example, in fashion domain, the visual appearance of clo\nthes and matching styles play a crucial role in understanding the user's intention. Without consider\ning these, the dialogue agent may fail to generate desirable responses for users. In this paper, we \npresent a Knowledge-aware Multimodal Dialogue (KMD) model to address the limitation of text-based di\nalogue systems. It gives special consideration to the semantics and domain knowledge revealed in vis\nual content, and is featured with three key components. First, we build a taxonomy-based learning mo\ndule to capture the fine-grained semantics in images the category and attributes of a product). Seco\nnd, we propose an end-to-end neural conversational model to generate responses based on the conversa\ntion history, visual semantics, and domain knowledge. Lastly, to avoid inconsistent dialogues, we ad\nopt a deep reinforcement learning method which accounts for future rewards to optimize the neural co\nnversational model. We perform extensive evaluation on a multi-turn task-oriented dialogue dataset i\nn fashion domain. Experiment results show that our method significantly outperforms state-of-the-art\n methods, demonstrating the efficacy of modeling visual modality and domain knowledge for dialogue s\nystems.", "cite_num": 9}, "49": {"title": "learning joint multimodal representation with adversarial attention networks.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240614", "abstract": "Recently, learning a joint representation for the multimodal data (e.g., containing both visual cont\nent and text description) has attracted extensive research interests. Usually, the features of diffe\nrent modalities are correlational and compositive, and thus a joint representation capturing the cor\nrelation is more effective than a subset of the features. Most of existing multimodal representation\n learning methods suffer from lack of additional constraints to enhance the robustness of the learne\nd representations. In this paper, a novel Adversarial Attention Networks (AAN) is proposed to incorp\norate both the attention mechanism and the adversarial networks for effective and robust multimodal \nrepresentation learning. Specifically, a visual-semantic attention model with siamese learning strat\negy is proposed to encode the fine-grained correlation between visual and textual modalities. Meanwh\nile, the adversarial learning model is employed to regularize the generated representation by matchi\nng the posterior distribution of the representation to the given priors. Then, the two modules are i\nncorporated into a integrated learning framework to learn the joint multimodal representation. Exper\nimental results in two tasks, i.e., multi-label classification and tag recommendation, show that the\n proposed model outperforms state-of-the-art representation learning methods.", "cite_num": 3}, "97": {"title": "text-to-image synthesis via symmetrical distillation networks.", "conf": "mm", "time": "2018", "url": "https://doi.org/10.1145/3240508.3240559", "abstract": "", "cite_num": -1}, "226": {"title": "session details: keynote 3.", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286926", "abstract": "", "cite_num": -1}, "65": {"title": "session details: vision-4 (representation learning).", "conf": "mm", "time": "2018", "url": "https://dl.acm.org/citation.cfm?id=3286948", "abstract": "", "cite_num": -1}}
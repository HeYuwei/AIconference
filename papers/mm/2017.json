{"52": {"title": "more than an answer: neural pivot network for visual qestion answering.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123335", "abstract": "", "cite_num": -1}, "68": {"title": "chainercv: a library for deep learning in computer vision.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129395", "abstract": "", "cite_num": -1}, "266": {"title": "sports vr content generation from regular camera feeds.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123315", "abstract": "", "cite_num": -1}, "124": {"title": "deepq: advancing healthcare through artificial intelligence and virtual reality.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130875", "abstract": "", "cite_num": -1}, "38": {"title": "two-stream attentive cnns for image retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123396", "abstract": "", "cite_num": -1}, "8": {"title": "split consideration for foreground and background painting using artificial neural networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129327", "abstract": "", "cite_num": -1}, "197": {"title": "knowing yourself: improving video caption via in-depth recap.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127901", "abstract": "", "cite_num": -1}, "29": {"title": "webdnn: fastest dnn execution framework on web browser.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129394", "abstract": "", "cite_num": -1}, "252": {"title": "region-based image retrieval revisited.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123312", "abstract": "", "cite_num": -1}, "269": {"title": "a dual-network progressive approach to weakly supervised object detection.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123455", "abstract": "", "cite_num": -1}, "256": {"title": "deep active learning through cognitive information parcels.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123337", "abstract": "", "cite_num": -1}, "110": {"title": "experimental analysis of bandwidth allocation in automated video surveillance systems.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123376", "abstract": "", "cite_num": -1}, "218": {"title": "enhancing music events using physiological sensor data.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127919", "abstract": "", "cite_num": -1}, "239": {"title": "do individuals smile more in diverse social company?: studying smiles and diversity via social media photos.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127908", "abstract": "", "cite_num": -1}, "35": {"title": "using dash assisting network elements for optimizing video streaming quality.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123965", "abstract": "", "cite_num": -1}, "59": {"title": "acm sigmm award for outstanding technical contributions to multimedia computing, communications and applications.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3139462", "abstract": "", "cite_num": -1}, "221": {"title": "efficient binary coding for subspace-based query-by-image video retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123392", "abstract": "", "cite_num": -1}, "23": {"title": "lsvc2017: large-scale video classification challenge.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3138874", "abstract": "", "cite_num": -1}, "116": {"title": "modeling the intransitive pairwise image preference from multiple angles.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123285", "abstract": "", "cite_num": -1}, "242": {"title": "multiview and multimodal pervasive indoor localization.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123436", "abstract": "Pervasive indoor localization (PIL) aims to locate an indoor mobile-phone user without any infrastru\ncture assistance. Conventional PIL approaches employ a single probe (i.e., target) measurement to lo\ncalize by identifying its best match out of a fingerprint gallery. However, a single measurement usu\nally captures limited and inadequate location features. More importantly, the reliance on a single m\neasurement bears the inherent risk of being inaccurate and unreliable, due to the fact that the meas\nurement could be noisy and even corrupted.   In this paper, we address the deficiency of using a sin\ngle measurement by proposing the original idea of localization based on multi-view and multi-modal m\neasurements. Specifically, a location is represented as a multi-view graph (MVG), which captures bot\nh local features and global contexts. We then formulate the location retrieval problem into an MVG m\natching problem. In MVG matching, a collaborative-reconstruction based measure is proposed to evalua\nte the node/edge similarity between two MVGs, which can explicitly address noisy measurements or out\nliers. Extensive experiments have been conducted on three different types of buildings with a total \narea of 18,719 m^2. We show that even with 30% noisy measurements or outliers, our method is able to\n achieve a promising accuracy of 1 meter. As another contribution, we construct a benchmark dataset \nfor the PIL task and make it publicly available, which to our knowledge, is the first public dataset\n that is tailored for multi-view multi-modal indoor localization and contains both magnetic and visu\nal signals.", "cite_num": 4}, "123": {"title": "image quality assessment for dibr synthesized views using elastic metric.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123329", "abstract": "", "cite_num": -1}, "136": {"title": "natural experiences in museums through virtual reality and voice commands.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127916", "abstract": "", "cite_num": -1}, "205": {"title": "how personality affects our likes: towards a better understanding of actionable images.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127909", "abstract": "", "cite_num": -1}, "240": {"title": "facecloud: heterogeneous cloud visualization of multiplex networks for multimedia archive exploration.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127917", "abstract": "", "cite_num": -1}, "138": {"title": "towards global optimization in display advertising by integrating multimedia metrics with real-time bidding.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123966", "abstract": "", "cite_num": -1}, "112": {"title": "cross-media retrieval by learning rich semantic embeddings of multimedia.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123369", "abstract": "", "cite_num": -1}, "147": {"title": "a unified personalized video recommendation via dynamic recurrent neural networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123433", "abstract": "", "cite_num": -1}, "45": {"title": "mutually guided image filtering.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123378", "abstract": "", "cite_num": -1}, "60": {"title": "sketch-based image retrieval using generative adversarial networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127939", "abstract": "", "cite_num": -1}, "258": {"title": "deep low-rank sparse collective factorization for cross-domain recommendation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123361", "abstract": "", "cite_num": -1}, "233": {"title": "skeleton-aided articulated motion generation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123277", "abstract": "", "cite_num": -1}, "169": {"title": "sharerender: bypassing gpu virtualization to enable fine-grained resource sharing for cloud gaming.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123306", "abstract": "", "cite_num": -1}, "67": {"title": "a novel system for visual navigation of educational videos using multimodal cues.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123406", "abstract": "With recent developments and advances in distance learning and MOOCs, the amount of open educational\n videos on the Internet has grown dramatically in the past decade. However, most of these videos are\n lengthy and lack of high-quality indexing and annotations, which triggers an urgent demand for effi\ncient and effective tools that facilitate video content navigation and exploration. In this paper, w\ne propose a novel visual navigation system for exploring open educational videos. The system tightly\n integrates multimodal cues obtained from the visual, audio and textual channels of the video and pr\nesents them with a series of interactive visualization components. With the help of this system, use\nrs can explore the video content using multiple levels of details to identify content of interest wi\nth ease. Extensive experiments and comparisons against previous studies demonstrate the effectivenes\ns of the proposed system.", "cite_num": 3}, "100": {"title": "unrealcv: virtual worlds for computer vision.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129396", "abstract": "", "cite_num": -1}, "193": {"title": "h-time: haptic-enabled tele-immersive musculoskeletal examination.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123395", "abstract": "", "cite_num": -1}, "236": {"title": "structcap: structured semantic embedding for image captioning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123275", "abstract": "", "cite_num": -1}, "166": {"title": "it's all around you: exploring 360\u00b0 video viewing experiences on mobile devices.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123347", "abstract": "", "cite_num": -1}, "113": {"title": "query-adaptive video summarization via quality-aware relevance estimation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123297", "abstract": "", "cite_num": -1}, "220": {"title": "social multimedia sentiment analysis.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130143", "abstract": "", "cite_num": -1}, "4": {"title": "midot-key: a smart key instantly generated on your item.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127921", "abstract": "", "cite_num": -1}, "263": {"title": "from part to whole: who is behind the painting?", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123325", "abstract": "", "cite_num": -1}, "244": {"title": "unconstrained fashion landmark detection via hierarchical recurrent transformer networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123276", "abstract": "", "cite_num": -1}, "190": {"title": "exploring consistent preferences: discrete hashing with pair-exemplar for scalable landmark search.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123301", "abstract": "", "cite_num": -1}, "199": {"title": "shadow puppetry with robotic arms.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127925", "abstract": "", "cite_num": -1}, "36": {"title": "salient object detection with chained multi-scale fully convolutional network.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123318", "abstract": "", "cite_num": -1}, "99": {"title": "exploring domain knowledge for affective video content analyses.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123352", "abstract": "", "cite_num": -1}, "14": {"title": "exploiting high-level semantics for no-reference image quality assessment of realistic blur images.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123322", "abstract": "", "cite_num": -1}, "201": {"title": "mr.mapp: mixed reality for managing phantom pain.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123419", "abstract": "", "cite_num": -1}, "102": {"title": "enhancing and augmenting human perception with artificial intelligence technologies.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130870", "abstract": "", "cite_num": -1}, "106": {"title": "region-based activity recognition using conditional gan.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123365", "abstract": "", "cite_num": -1}, "130": {"title": "fractal: fec-based rate control for rtp.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123373", "abstract": "", "cite_num": -1}, "192": {"title": "fastshrinkage: perceptually-aware retargeting toward mobile platforms.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123377", "abstract": "", "cite_num": -1}, "103": {"title": "magic-wall: visualizing room decoration.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123398", "abstract": "", "cite_num": -1}, "155": {"title": "privacy protection in online multimedia.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3133335", "abstract": "", "cite_num": -1}, "54": {"title": "deep matching and validation network: an end-to-end solution to constrained image splicing localization and detection.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123411", "abstract": "", "cite_num": -1}, "57": {"title": "hierarchical recurrent neural network for video summarization.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123328", "abstract": "", "cite_num": -1}, "73": {"title": "sawacmmm'17: the 1st workshop on multi media applications within the south african context.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132052", "abstract": "", "cite_num": -1}, "56": {"title": "towards forward-looking online bitrate adaptation for dash.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123284", "abstract": "", "cite_num": -1}, "254": {"title": "deepcadx: automated prostate cancer detection and diagnosis in mp-mri based on multimodal convolutional neural networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127914", "abstract": "In this paper, we present DeepCADx, a computer-aided prostate detection and diagnosis (CADx) system \npowered by a novel deep convolutional neural networks (CNNs). Specifically, the developed DeepCADx s\nystem processes multi-parametric magnetic resonance imaging (mp-MRI) sequences in three major steps:\n 1) pre-processing which registers images from different modalities and detect prostates, 2) multimo\ndal CNNs which jointly identifies images containing prostate cancers (PCa) and generate cancer respo\nnse maps (CRM) with each pixel indicating the probability to be cancerous, and 3) post-processing wh\nich localize lesion in CRMs and assess the aggressiveness (i.e. Gleason score) of each localized les\nion using multimodal CNN features and a 5-class SVM classifier.", "cite_num": 0}, "248": {"title": "detecting temporal proposal for action localization with tree-structured search policy.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123362", "abstract": "", "cite_num": -1}, "107": {"title": "#visualhashtags: visual summarization of social media events using mid-level visual elements.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123407", "abstract": "", "cite_num": -1}, "109": {"title": "optile: toward optimal tiling in 360-degree video streaming.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123339", "abstract": "", "cite_num": -1}, "71": {"title": "quality-of-experience of adaptive video streaming: exploring the space of adaptations.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123418", "abstract": "", "cite_num": -1}, "48": {"title": "pqk-means: billion-scale clustering for product-quantized codes.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123430", "abstract": "", "cite_num": -1}, "257": {"title": "capturing spatial and temporal patterns for distinguishing between posed and spontaneous expressions.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123350", "abstract": "", "cite_num": -1}, "181": {"title": "musa2: first acm workshop on multimodal understanding of social, affective and subjective attributes.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132057", "abstract": "Multimedia scientists have largely focused their research on the recognition of tangible properties \nof data, such as objects and scenes. Recently, the field has started evolving towards the modeling o\nf more complex properties. For example, the understanding of social, affective and subjective attrib\nutes of data has attracted the attention of many research teams at the crossroads of computer vision\n, multimedia, and social sciences. These intangible attributes include, for example, visual beauty, \nvideo popularity, or user behavior. Multiple, diverse challenges arise when modeling such properties\n from multimedia data. Issues concern technical aspects such as reliable groundtruth collection, the\n effective learning of subjective properties, or the impact of context in subjective perception. The\n first edition of the ACM MM'17 MUSA2 workshop has gathered together high-quality research works foc\nusing on the computational understanding of intangible properties from multimodal data, including vi\nsual emotions, user intent, human relationships, and personality.", "cite_num": 2}, "224": {"title": "pd-survey: supporting audience-centric research through surveys on pervasive display networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123293", "abstract": "", "cite_num": -1}, "160": {"title": "summary for avec 2017: real-life depression and affect challenge and workshop.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132049", "abstract": "", "cite_num": -1}, "127": {"title": "occlusion-aware video temporal consistency.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123363", "abstract": "", "cite_num": -1}, "225": {"title": "sketch recognition with deep visual-sequential fusion model.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123321", "abstract": "", "cite_num": -1}, "215": {"title": "multimodal fusion with recurrent neural networks for rumor detection on microblogs.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123454", "abstract": "Microblogs have become popular media for news propagation in recent years. Meanwhile, numerous rumor\ns and fake news also bloom and spread wildly on the open social media platforms. Without verificatio\nn, they could seriously jeopardize the credibility of microblogs. We observe that an increasing numb\ner of users are using images and videos to post news in addition to texts. Tweets or microblogs are \ncommonly composed of text, image and social context. In this paper, we propose a novel Recurrent Neu\nral Network with an attention mechanism (att-RNN) to fuse multimodal features for effective rumor de\ntection. In this end-to-end network, image features are incorporated into the joint features of text\n and social context, which are obtained with an LSTM (Long-Short Term Memory) network, to produce a \nreliable fused classification. The neural attention from the outputs of the LSTM is utilized when fu\nsing with the visual features. Extensive experiments are conducted on two multimedia rumor datasets \ncollected from Weibo and Twitter. The results demonstrate the effectiveness of the proposed end-to-e\nnd att-RNN in detecting rumors with multimodal contents.", "cite_num": 15}, "219": {"title": "on server provisioning for cloud gaming.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123310", "abstract": "", "cite_num": -1}, "194": {"title": "combining multiple features for image popularity prediction in social media.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127900", "abstract": "", "cite_num": -1}, "152": {"title": "multi-scale cascade network for salient object detection.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123290", "abstract": "", "cite_num": -1}, "121": {"title": "3d cnns on distance matrices for human action recognition.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123299", "abstract": "", "cite_num": -1}, "183": {"title": "statistical inference of gaussian-laplace distribution for person verification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123421", "abstract": "", "cite_num": -1}, "230": {"title": "ibm high-five: highlights from intelligent video engine.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127924", "abstract": "", "cite_num": -1}, "83": {"title": "photo2trip: exploiting visual contents in geo-tagged photos for personalized tour recommendation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123336", "abstract": "", "cite_num": -1}, "2": {"title": "learning a target sample re-generator for cross-database micro-expression recognition.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123367", "abstract": "", "cite_num": -1}, "262": {"title": "video question answering via hierarchical dual-level attention network learning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123364", "abstract": "Video question answering is a challenging task in visual information retrieval, which provides the a\nccurate answer from the referenced video contents according to the given question. However, the exis\nting visual question answering approaches mainly tackle the problem of static image question answeri\nng, which may be ineffectively applied for video question answering directly, due to the insufficien\ncy of modeling the video temporal dynamics. In this paper, we study the problem of video question an\nswering from the viewpoint of hierarchical dual-level attention network learning. We obtain the obje\nct appearance and movement information in the video based on both frame-level and segment-level feat\nure representation methods. We then develop the hierarchical duallevel attention networks to learn t\nhe question-aware video representations with word-level and question-level attention mechanisms. We \nnext devise the question-level fusion attention mechanism for our proposed networks to learn the que\nstionaware joint video representation for video question answering. We construct two large-scale vid\neo question answering datasets. The extensive experiments validate the effectiveness of our method.", "cite_num": 11}, "203": {"title": "learning to recognise unseen classes by a few similes.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123323", "abstract": "", "cite_num": -1}, "150": {"title": "online cross-modal scene retrieval by binary representation and semantic graph.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123311", "abstract": "", "cite_num": -1}, "158": {"title": "stylized adversarial autoencoder for image generation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123450", "abstract": "", "cite_num": -1}, "47": {"title": "cross-domain image retrieval with attention modeling.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123429", "abstract": "With the proliferation of e-commerce websites and the ubiquitousness of smart phones, cross-domain i\nmage retrieval using images taken by smart phones as queries to search products on e-commerce websit\nes is emerging as a popular application. One challenge of this task is to locate the attention of bo\nth the query and database images. In particular, database images, e.g. of fashion products, on e-com\nmerce websites are typically displayed with other accessories, and the images taken by users contain\n noisy background and large variations in orientation and lighting. Consequently, their attention is\n difficult to locate. In this paper, we exploit the rich tag information available on the e-commerce\n websites to locate the attention of database images. For query images, we use each candidate image \nin the database as the context to locate the query attention. Novel deep convolutional neural networ\nk architectures, namely TagYNet and CtxYNet, are proposed to learn the attention weights and then ex\ntract effective representations of the images. Experimental results on public datasets confirm that \nour approaches have significant improvement over the existing methods in terms of the retrieval accu\nracy and efficiency.", "cite_num": 7}, "79": {"title": "neurostylist: neural compatibility modeling for clothing matching.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123314", "abstract": "", "cite_num": -1}, "84": {"title": "cross-modal recipe retrieval with rich food attributes.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123428", "abstract": "", "cite_num": -1}, "168": {"title": "deep temporal models using identity skip-connections for speech emotion recognition.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123353", "abstract": "", "cite_num": -1}, "145": {"title": "searching personal photos on the phone with instant visual query suggestion and joint text-image hashing.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123446", "abstract": "", "cite_num": -1}, "245": {"title": "where are the sweet spots?: a systematic approach to reproducible dash player comparisons.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123426", "abstract": "", "cite_num": -1}, "165": {"title": "adaptive 360-degree video streaming using scalable video coding.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123414", "abstract": "", "cite_num": -1}, "255": {"title": "fine-grained discriminative localization via saliency-guided faster r-cnn.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123319", "abstract": "", "cite_num": -1}, "78": {"title": "place-centric visual urban perception with deep multi-instance regression.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123271", "abstract": "", "cite_num": -1}, "206": {"title": "pedestrian detection via bi-directional multi-scale analysis.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123356", "abstract": "", "cite_num": -1}, "88": {"title": "\u00e0 quatre mains.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129329", "abstract": "", "cite_num": -1}, "210": {"title": "probably/possibly?: an immersive interactive visual/sonic quantum composition and synthesizer.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129324", "abstract": "", "cite_num": -1}, "250": {"title": "metric-based generative adversarial network.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123334", "abstract": "", "cite_num": -1}, "267": {"title": "deep attribute-preserving metric learning for natural language object retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123439", "abstract": "", "cite_num": -1}, "93": {"title": "bringing gaming; vr; and ar to life with deep learning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130873", "abstract": "", "cite_num": -1}, "9": {"title": "deep asymmetric pairwise hashing.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123345", "abstract": "", "cite_num": -1}, "85": {"title": "protest activity detection and perceived violence estimation from social media images.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123282", "abstract": "", "cite_num": -1}, "232": {"title": "a delicious recipe analysis framework for exploring multi-modal recipes with various attributes.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123272", "abstract": "Human beings have developed a diverse food culture. Many factors like ingredients, visual appearance\n, courses (e.g., breakfast and lunch), flavor and geographical regions affect our food perception an\nd choice. In this work, we focus on multi-dimensional food analysis based on these food factors to b\nenefit various applications like summary and recommendation. For that solution, we propose a delicio\nus recipe analysis framework to incorporate various types of continuous and discrete attribute featu\nres and multi-modal information from recipes. First, we develop a Multi-Attribute Theme Modeling (MA\nTM) method, which can incorporate arbitrary types of attribute features to jointly model them and th\ne textual content. We then utilize a multi-modal embedding method to build the correlation between t\nhe learned textual theme features from MATM and visual features from the deep learning network. By l\nearning attribute-theme relations and multi-modal correlation, we are able to fulfill different appl\nications, including (1) flavor analysis and comparison for better understanding the flavor patterns \nfrom different dimensions, such as the region and course, (2) region-oriented multi-dimensional food\n summary with both multi-modal and multi-attribute information and (3) multi-attribute oriented reci\npe recommendation. Furthermore, our proposed framework is flexible and enables easy incorporation of\n arbitrary types of attributes and modalities. Qualitative and quantitative evaluation results have \nvalidated the effectiveness of the proposed method and framework on the collected Yummly dataset.", "cite_num": 6}, "31": {"title": "adaptive audio classification for smartphone in noisy car environment.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123397", "abstract": "", "cite_num": -1}, "64": {"title": "multimedia semantic integrity assessment using joint embedding of images and text.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123385", "abstract": "", "cite_num": -1}, "191": {"title": "single shot temporal action detection.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123343", "abstract": "", "cite_num": -1}, "241": {"title": "mmhealth 2017: workshop on multimedia for personal health and health care.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132051", "abstract": "", "cite_num": -1}, "122": {"title": "leaf: latent extended attribute features discovery for visual classification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123342", "abstract": "", "cite_num": -1}, "20": {"title": "nubomedia: the first open source webrtc paas.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129392", "abstract": "", "cite_num": -1}, "159": {"title": "video captioning with guidance of multimodal latent topics.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123420", "abstract": "The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions i\nn describing video contents, and therefore, makes the video captioning task even more challenging. I\nn this paper, we propose an unified caption framework, M&M TGM, which mines multimodal topics in uns\nupervised fashion from data and guides the caption decoder with these topics. Compared to pre-define\nd topics, the mined multimodal topics are more semantically and visually coherent and can reflect th\ne topic distribution of videos better. We formulate the topic-aware caption generation as a multi-ta\nsk learning problem, in which we add a parallel task, topic prediction, in addition to the caption t\nask. For the topic prediction task, we use the mined topics as the teacher to train a student topic \nprediction model, which learns to predict the latent topics from multimodal contents of videos. The \ntopic prediction provides intermediate supervision to the learning process. As for the caption task,\n we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions wi\nth the guidance from latent topics. The entire learning procedure is end-to-end and it optimizes bot\nh tasks simultaneously. The results from extensive experiments conducted on the MSR-VTT and Youtube2\nText datasets demonstrate the effectiveness of our proposed model. M&M TGM not only outperforms prio\nr state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also a\nchieves better generalization ability.", "cite_num": 14}, "146": {"title": "altmm 2017 - 2nd international workshop on multimedia alternate realities.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132055", "abstract": "", "cite_num": -1}, "94": {"title": "normface: l2 hypersphere embedding for face verification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123359", "abstract": "", "cite_num": -1}, "58": {"title": "semi-dense depth interpolation using deep convolutional neural networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123360", "abstract": "", "cite_num": -1}, "91": {"title": "t2u: a deep cross-platform video recommendation system with a novel interface.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127938", "abstract": "", "cite_num": -1}, "175": {"title": "venues in social media: examining ambiance perception through scene semantics.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123402", "abstract": "", "cite_num": -1}, "251": {"title": "visualization of stone trajectories in live curling broadcasts using online machine learning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123351", "abstract": "", "cite_num": -1}, "148": {"title": "deep unsupervised convolutional domain adaptation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123292", "abstract": "", "cite_num": -1}, "104": {"title": "selective deep convolutional features for image retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123417", "abstract": "", "cite_num": -1}, "87": {"title": "temporal binary coding for large-scale video search.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123273", "abstract": "", "cite_num": -1}, "179": {"title": "from multimedia logs to personal chronicles.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123375", "abstract": "", "cite_num": -1}, "114": {"title": "matplanner: plan your days in conferences by resolving conflicting events.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127915", "abstract": "", "cite_num": -1}, "39": {"title": "a system for spatiotemporal anomaly localization in surveillance videos.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127912", "abstract": "... but we have temporarily restricted your access to the Digital Library.\nYour activity appears to \nbe coming from some type of automated process.\nTo ensure the availability of the Digital Library we \ncan not allow these types of requests to continue.\nThe restriction will be removed automatically onc\ne this activity stops.\n", "cite_num": 83}, "50": {"title": "quetra: a queuing theory approach to dash rate adaptation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123390", "abstract": "", "cite_num": -1}, "7": {"title": "video description with spatial-temporal attention.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123354", "abstract": "Temporal attention has been widely used in video description to adaptively focus on important frames\n. However, most existing methods based on temporal attention suffer from the problems of recognition\n error and detail missing, because only coarse frame-level global features are employed. Inspired by\n recent successful work in image description using spatial attention, we propose a spatial-temporal \nattention (STAT) method to address such problems. In particular, first, we take advantage of object-\nlevel local features to address the problem of detail missing. Second, the STAT method further selec\nts relevant local features by spatial attention and then attend to important frames by temporal atte\nntion to recognize related semantics. The proposed two-stage attention mechanism can recognize the s\nalient objects more precisely with high recall and automatically focus on the most relevant spatial-\ntemporal segments given the sentence context. Extensive experiments on two well-known benchmarks sug\ngest that STAT method outperforms the state-of-the-art methods on MSVD with BLEU4 score 0.511, and a\nchieves superior BLEU4 score 0.374 on MSR-VTT-10K. Compared to the method without local features, th\ne relative improvements derived from our STAT method are 10.1% and 0.8% respectively on two benchmar\nks. Compared to the method using only temporal attention, the relative improvements derived from our\n STAT method are 18.3% and 9.0% respectively on two benchmarks.", "cite_num": 5}, "237": {"title": "smart mirror: intelligent makeup recommendation and synthesis.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127926", "abstract": "", "cite_num": -1}, "27": {"title": "laplacian-steered neural style transfer.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123425", "abstract": "", "cite_num": -1}, "261": {"title": "multi-modal localization and enhancement of multiple sound sources from a micro aerial vehicle.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123412", "abstract": "The ego-noise generated by the motors and propellers of a micro aerial vehicle (MAV) masks the envir\nonmental sounds and considerably degrades the quality of the on-board sound recording. Sound enhance\nment approaches generally require knowledge of the direction of arrival of the target sound sources,\n which are difficult to estimate due to the low signal-to-noise-ratio (SNR) caused by the ego-noise \nand the interferences between multiple sources. To address this problem, we propose a multi-modal an\nalysis approach that jointly exploits audio and video to enhance the sounds of multiple targets capt\nured from an MAV equipped with a microphone array and a video camera. We first address audio-visual \ncalibration via camera resectioning, audio-visual temporal alignment and geometrical alignment to jo\nintly use the features in the audio and video streams, which are independently generated. The spatia\nl information from the video is used to assist sound enhancement by tracking multiple potential soun\nd sources with a particle filter. Then we infer the directions of arrival of the target sources from\n the video tracking results and extract the sound from the desired direction with a time-frequency s\npatial filter, which suppresses the ego-noise by exploiting its time-frequency sparsity. Experimenta\nl demonstration results with real outdoor data verify the robustness of the proposed multi-modal app\nroach for multiple speakers in extremely low-SNR scenarios.", "cite_num": 1}, "108": {"title": "is foveated rendering perceivable in virtual reality?: exploring the efficiency and consistency of quality assessment methods.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123434", "abstract": "", "cite_num": -1}, "63": {"title": "deepart: learning joint representations of visual arts.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123405", "abstract": "", "cite_num": -1}, "176": {"title": "a multi-task framework for weather recognition.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123382", "abstract": "", "cite_num": -1}, "143": {"title": "real-time deep video spatial resolution upconversion system (struct++ demo).", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127927", "abstract": "", "cite_num": -1}, "238": {"title": "robust visual object tracking with top-down reasoning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123449", "abstract": "", "cite_num": -1}, "246": {"title": "a paralinguistic approach to speaker diarisation: using age, gender, voice likability and personality traits.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123338", "abstract": "", "cite_num": -1}, "32": {"title": "automatic adjustment of stereoscopic content for long-range projections in outdoor areas.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123269", "abstract": "", "cite_num": -1}, "198": {"title": "real-time false-contours removal for inverse tone mapped hdr content.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123400", "abstract": "", "cite_num": -1}, "208": {"title": "a tag recommendation system for popularity boosting.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127913", "abstract": "", "cite_num": -1}, "200": {"title": "deep supervised quantization by self-organizing map.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123415", "abstract": "", "cite_num": -1}, "234": {"title": "anti-camera led lighting.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123416", "abstract": "", "cite_num": -1}, "184": {"title": "a hybrid p2p/multi-server quality-adaptive live-streaming solution enhancing end-user's qoe.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127936", "abstract": "", "cite_num": -1}, "195": {"title": "vocktail: a virtual cocktail for pairing digital taste, smell, and color sensations.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123440", "abstract": "", "cite_num": -1}, "21": {"title": "deep location-specific tracking.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123381", "abstract": "", "cite_num": -1}, "76": {"title": "popularity meter: an influence- and aesthetics-aware social media popularity predictor.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127903", "abstract": "", "cite_num": -1}, "28": {"title": "multi-feature fusion for predicting social media popularity.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127897", "abstract": "", "cite_num": -1}, "51": {"title": "fine-grained recognition via attribute-guided attentive feature aggregation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123358", "abstract": "", "cite_num": -1}, "264": {"title": "muver'17: first international workshop on multimedia verification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132058", "abstract": "", "cite_num": -1}, "42": {"title": "deep learning for intelligent video analysis.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130141", "abstract": "", "cite_num": -1}, "231": {"title": "catching the temporal regions-of-interest for video captioning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123327", "abstract": "", "cite_num": -1}, "105": {"title": "tensorlayer: a versatile library for efficient deep learning development.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129391", "abstract": "", "cite_num": -1}, "259": {"title": "deep cross-modality alignment for multi-shot person re-identification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123324", "abstract": "Multi-shot person Re-IDentification (Re-ID) has recently received more research attention as its pro\nblem setting is more realistic compared to single-shot Re-ID in terms of application. While many lar\nge-scale single-shot Re-ID human image datasets have been released, most existing multishot Re-ID vi\ndeo sequence datasets containonly a few (i.e., several hundreds) human instances, which hinders furt\nher improvement of multi-shot Re-ID performance. To this end, we propose a deep cross-modality align\nment network, which jointly explores both human sequence pairs and image pairs to facilitate trainin\ng better multi-shot human Re-ID models, i.e., via transferring knowledge from image data to sequence\n data. To mitigate modality-to-modality mismatch issue, the proposed network is equipped with an ima\nge-to-sequence adaption module called cross-modality alignment sub-network, which successfully maps \neach human image into a pseudo human sequence to facilitate knowledge transferring and joint trainin\ng. Extensive experimental results on several multi-shot person Re-ID benchmarks demonstrate great pe\nrformance gain brought up by the proposed network.", "cite_num": 4}, "134": {"title": "integrated face analytics networks through cross-dataset hybrid training.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123438", "abstract": "", "cite_num": -1}, "34": {"title": "visual sentiment analysis for review images with item-oriented and user-oriented cnn.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123374", "abstract": "", "cite_num": -1}, "13": {"title": "semi-relaxation supervised hashing for cross-modal retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123320", "abstract": "", "cite_num": -1}, "217": {"title": "pl@ntnet - my business.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129312", "abstract": "", "cite_num": -1}, "12": {"title": "multimodal learning for web information extraction.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123296", "abstract": "We consider the problem of extracting text instances of predefined categories from the Web. Instance\ns of a category may be scattered across thousands of independent sources in many different formats w\nith potential noises, which makes open-domain information extraction a challenging problem. Learning\n syntactic rules like \"cities such as _\" or \"_ is a city\" in a semi-supervised manner using a few la\nbeled examples is usually unreliable because 1) high quality syntactic rules are rare and 2) the lea\nrning task is usually underconstrained. To address these problems, in this paper we propose to learn\n multimodal rules to combat the difficulty of syntactic rules. The multimodal rules are learned from\n information sources of different modalities, which is motivated by an intuition that information th\nat is difficult to disambiguate correctly in one modality may be easily recognized in another. To de\nmonstrate the effectiveness of this method, we have built a sophisticated end-to-end multimodal info\nrmation extraction system that takes unannotated raw web pages as input, and generates a set of extr\nacted instances as outputs. More specifically, our system learns reliable relationship between multi\nmodal information by multimodal relation analysis on big unstructured data. Based on the learned rel\nationship, we further train a set of multimodal rules for information extraction. Experimental evalu\nation shows that a greater accuracy for information extraction can be achieved by multimodal learnin\ng.", "cite_num": 4}, "5": {"title": "a hybrid model combining convolutional neural network with xgboost for predicting social media popularity.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127902", "abstract": "", "cite_num": -1}, "171": {"title": "deep siamese network with multi-level similarity perception for person re-identification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123452", "abstract": "", "cite_num": -1}, "140": {"title": "sync-draw: automatic video generation using deep recurrent attentive architectures.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123309", "abstract": "", "cite_num": -1}, "25": {"title": "deep binary reconstruction for cross-modal hashing.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123355", "abstract": "", "cite_num": -1}, "185": {"title": "social media prediction based on residual learning and random forest.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127894", "abstract": "", "cite_num": -1}, "182": {"title": "teleconsultant: communication and analysis of wearable videos in emergency medical environments.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127920", "abstract": "", "cite_num": -1}, "144": {"title": "indefinite kernel logistic regression.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123295", "abstract": "", "cite_num": -1}, "172": {"title": "brain2image: converting brain signals into images.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127907", "abstract": "", "cite_num": -1}, "228": {"title": "rsvp: a real-time surveillance video parsing system with single frame supervision.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127928", "abstract": "", "cite_num": -1}, "10": {"title": "vscc'2017: visual analysis for smart and connected communities.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132053", "abstract": "", "cite_num": -1}, "111": {"title": "rethinking http adaptive streaming with the mobile user perception.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123357", "abstract": "", "cite_num": -1}, "227": {"title": "enhancing micro-video understanding by harnessing external sounds.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123313", "abstract": "", "cite_num": -1}, "125": {"title": "when cloud meets uncertain crowd: an auction approach for crowdsourced livecast transcoding.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123384", "abstract": "", "cite_num": -1}, "265": {"title": "one-shot fine-grained instance retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123278", "abstract": "", "cite_num": -1}, "6": {"title": "fast and accurate pedestrian detection using dual-stage group cost-sensitive realboost with vector form filters.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123303", "abstract": "", "cite_num": -1}, "17": {"title": "pseudo label based unsupervised deep discriminative hashing for image retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123403", "abstract": "", "cite_num": -1}, "61": {"title": "manet: a modal attention network for describing videos.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127898", "abstract": "Exploiting multimodal features has become a standard approach towards many video applications, inclu\nding the video captioning task. One problem with the existing work is that it models the relevance o\nf each type of features evenly, which neutralizes the impact of each individual modality to the word\n to be generated. In this paper, we propose a novel Modal Attention Network (MANet) to account for t\nhis issue. Our MANet extends the standard encoder-decoder network by adapting the attention mechanis\nm to video modalities. As a result, MANet emphasizes the impact of each modality with respect to the\n word to be generated. Experimental results show that our MANet effectively utilizes multimodal feat\nures to generate better video descriptions. Especially, our MANet system was ranked among the top th\nree systems at the 2nd Video to Language Challenge in both automatic metrics and human evaluations.", "cite_num": 0}, "22": {"title": "presently untitled: data mapping of 2016 u.s. presidential election twitter activity, phase iii.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129332", "abstract": "", "cite_num": -1}, "149": {"title": "from hard to soft: towards more human-like emotion recognition by modelling the perception uncertainty.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123383", "abstract": "", "cite_num": -1}, "174": {"title": "real-time dense monocular slam for augmented reality.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127918", "abstract": "", "cite_num": -1}, "82": {"title": "multirate multimodal video captioning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127904", "abstract": "Automatically describing videos with natural language is a crucial challenge of video understanding.\n Compared to images, videos have specific spatial-temporal structure and various modality informatio\nn. In this paper, we propose a Multirate Multimodal Approach for video captioning. Considering that \nthe speed of motion in videos varies constantly, we utilize a Multirate GRU to capture temporal stru\ncture of videos. It encodes video frames with different intervals and has a strong ability to deal w\nith motion speed variance. As videos contain different modality cues, we design a particular multimo\ndal fusion method. By incorporating visual, motion, and topic information together, we construct a w\nell-designed video representation. Then the video representation is fed into a RNN-based language mo\ndel for generating natural language descriptions. We evaluate our approach for video captioning on \"\nMicrosoft Research - Video to Text\" (MSR-VTT), a large-scale video benchmark for video understanding\n. And our approach gets great performance on the 2nd MSR Video to Language Challenge.", "cite_num": 2}, "92": {"title": "attention transfer from web images for video recognition.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123432", "abstract": "Training deep learning based video classifiers for action recognition requires a large amount of lab\neled videos. The labeling process is labor-intensive and time-consuming. On the other hand, large am\nount of weakly-labeled images are uploaded to the Internet by users everyday. To harness the rich an\nd highly diverse set of Web images, a scalable approach is to crawl these images to train deep learn\ning based classifier, such as Convolutional Neural Networks (CNN). However, due to the domain shift \nproblem, the performance of Web images trained deep classifiers tend to degrade when directly deploy\ned to videos. One way to address this problem is to fine-tune the trained models on videos, but suff\nicient amount of annotated videos are still required. In this work, we propose a novel approach to t\nransfer knowledge from image domain to video domain. The proposed method can adapt to the target dom\nain (i.e. video data) with limited amount of training data. Our method maps the video frames into a \nlow-dimensional feature space using the class-discriminative spatial attention map for CNNs. We desi\ngn a novel Siamese EnergyNet structure to learn energy functions on the attention maps by jointly op\ntimizing two loss functions, such that the attention map corresponding to a ground truth concept wou\nld have higher energy. We conduct extensive experiments on two challenging video recognition dataset\ns (i.e. TVHI and UCF101), and demonstrate the efficacy of our proposed method.", "cite_num": 7}, "222": {"title": "time traveler: a real-time face aging system.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127922", "abstract": "", "cite_num": -1}, "77": {"title": "learning fashion compatibility with bidirectional lstms.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123394", "abstract": "", "cite_num": -1}, "142": {"title": "multiedtech 2017: 1st international workshop on multimedia-based educational and knowledge technologies for personalized and social online training.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132056", "abstract": "", "cite_num": -1}, "164": {"title": "automatic generation of lyrics parodies.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123410", "abstract": "", "cite_num": -1}, "153": {"title": "towards micro-video understanding by joint sequential-sparse modeling.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123341", "abstract": "", "cite_num": -1}, "167": {"title": "adversarial cross-modal retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123326", "abstract": "", "cite_num": -1}, "204": {"title": "elasticplay: interactive video summarization with dynamic time budgets.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123393", "abstract": "", "cite_num": -1}, "270": {"title": "sketchparse: towards rich descriptions for poorly drawn sketches using multi-task hierarchical deep networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123270", "abstract": "", "cite_num": -1}, "44": {"title": "affect recognition in ads with application to computational advertising.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123444", "abstract": "", "cite_num": -1}, "173": {"title": "moving as a leader: detecting emergent leadership in small groups using body pose.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123404", "abstract": "", "cite_num": -1}, "15": {"title": "real-time monocular dense mapping for augmented reality.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123348", "abstract": "", "cite_num": -1}, "72": {"title": "adaptively weighted multi-task deep network for person attribute classification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123424", "abstract": "", "cite_num": -1}, "163": {"title": "video question answering via gradually refined attention over appearance and motion.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123427", "abstract": "Recently image question answering (ImageQA) has gained lots of attention in the research community. \nHowever, as its natural extension, video question answering (VideoQA) is less explored. Although bot\nh tasks look similar, VideoQA is more challenging mainly because of the complexity and diversity of \nvideos. As such, simply extending the ImageQA methods to videos is insufficient and suboptimal. Part\nicularly, working with the video needs to model its inherent temporal structure and analyze the dive\nrse information it contains. In this paper, we consider exploiting the appearance and motion informa\ntion resided in the video with a novel attention mechanism. More specifically, we propose an end-to-\nend model which gradually refines its attention over the appearance and motion features of the video\n using the question as guidance. The question is processed word by word until the model generates th\ne final optimized attention. The weighted representation of the video, as well as other contextual i\nnformation, are used to generate the answer. Extensive experiments show the advantages of our model \ncompared to other baseline models. We also demonstrate the effectiveness of our model by analyzing t\nhe refined attention weights during the question answering procedure.", "cite_num": 23}, "249": {"title": "pedestrian path forecasting in crowd: a deep spatio-temporal perspective.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123287", "abstract": "", "cite_num": -1}, "18": {"title": "360probdash: improving qoe of 360 video streaming using tile-based http adaptive streaming.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123291", "abstract": "", "cite_num": -1}, "101": {"title": "learning object-centric transformation for video prediction.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123349", "abstract": "", "cite_num": -1}, "229": {"title": "learning visual emotion distributions via multi-modal features fusion.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130858", "abstract": "Current image emotion recognition works mainly classified the images into one dominant emotion categ\nory, or regressed the images with average dimension values by assuming that the emotions perceived a\nmong different viewers highly accord with each other. However, due to the influence of various perso\nnal and situational factors, such as culture background and social interactions, different viewers m\nay react totally different from the emotional perspective to the same image. In this paper, we propo\nse to formulate the image emotion recognition task as a probability distribution learning problem. M\notivated by the fact that image emotions can be conveyed through different visual features, such as \naesthetics and semantics, we present a novel framework by fusing multi-modal features to tackle this\n problem. In detail, weighted multi-modal conditional probability neural network (WMMCPNN) is design\ned as the learning model to associate the visual features with emotion probabilities. By jointly exp\nloring the complementarity and learning the optimal combination coefficients of different modality f\neatures, WMMCPNN could effectively utilize the representation ability of each uni-modal feature. We \nconduct extensive experiments on three publicly available benchmarks and the results demonstrate tha\nt the proposed method significantly outperforms the state-of-the-art approaches for emotion distribu\ntion prediction.", "cite_num": 10}, "119": {"title": "a simplified topological representation of text for local and global context.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123330", "abstract": "", "cite_num": -1}, "118": {"title": "bmxnet: an open-source binary neural network implementation based on mxnet.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129393", "abstract": "", "cite_num": -1}, "178": {"title": "what your facebook profile picture reveals about your personality.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123331", "abstract": "", "cite_num": -1}, "188": {"title": "who composes the music?: musicality evaluation for algorithmic composition via electroencephalography.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123967", "abstract": "", "cite_num": -1}, "98": {"title": "first international acm thematic workshops 2017.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132060", "abstract": "", "cite_num": -1}, "30": {"title": "diversified and summarized video search system.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127937", "abstract": "", "cite_num": -1}, "90": {"title": "incremental accelerated kernel discriminant analysis.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123401", "abstract": "", "cite_num": -1}, "137": {"title": "regle: spatially regularized graph learning for visual tracking.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123288", "abstract": "", "cite_num": -1}, "74": {"title": "wheel: accelerating cnns with distributed gpus via hybrid parallelism and alternate strategy.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123435", "abstract": "", "cite_num": -1}, "24": {"title": "face aging with contextual generative adversarial nets.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123431", "abstract": "", "cite_num": -1}, "151": {"title": "lta 2017: the second workshop on lifelogging tools and applications.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132050", "abstract": "", "cite_num": -1}, "81": {"title": "multicamera summarization of rehabilitation sessions in home environment.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123387", "abstract": "", "cite_num": -1}, "53": {"title": "exploring the use of time-dependent cross-network information for personalized recommendations.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123447", "abstract": "", "cite_num": -1}, "89": {"title": "optimal set of 360-degree videos for viewport-adaptive streaming.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123372", "abstract": "", "cite_num": -1}, "216": {"title": "rfiw: large-scale kinship recognition challenge.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3132059", "abstract": "", "cite_num": -1}, "16": {"title": "adaptively attending to visual attributes and linguistic knowledge for captioning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123391", "abstract": "", "cite_num": -1}, "115": {"title": "improved multimodal representation learning with skip connections.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123332", "abstract": "Multimodal Deep Boltzmann Machines (DBMs) have demonstrated huge successes in multimodal representat\nion learning tasks. During inference, DBMs function as Recurrent Neural Nets (RNNs) because of the i\nntractable distributions. To learn the parameters, optimizations can alternatively be operated on th\nese surrogate RNNs with \"truncated message passing\". As a consequence, the gradient will propagate t\nhrough a long chain without any local guidance which can potentially affects the optimization proced\nure. In this paper, we address this problem by adding skip connections during back-propagation while\n keeping the forward propagation (inference) untouched. With skip connections, we implicitly assign \nlocal \"targets\" for the states of intermediate inference loops to approach. Applied to different tra\nining criteria on different data sets, we demonstrate the proposed algorithms can consistently help \nto train better models while at a lower cost of training time. Experimental results show that our al\ngorithms can achieve state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flic\nkr data set.", "cite_num": 0}, "126": {"title": "building multi-modal interfaces for smartphones.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130874", "abstract": "Smartphones are the central component of our modern, connected life. We carry them from the moment w\ne wake up till the time we go to bed. Continuous technology innovation has created ever more sophist\nicated phones. Their small size belies a complexity that is largely unknown to the user -- making it\n almost impossible to discover and use these expanded capabilities. Designers are forced to make tra\ndeoffs between adding more capabilities and burying the functionality in menu hierarchies. Users are\n frustrated when forced to accept either of these choices. This is the fundamental limitation of mod\nern touch based interfaces to smartphones.   On the path to solving this problem, it is natural to a\nsk \"How can we make it easier for users to learn and use the new features\". We believe this is the w\nrong question to ask and took a fundamentally different approach.   Dr. Injong Rhee will discuss thi\ns problem, its ramifications to multimedia and future outlook.", "cite_num": 0}, "139": {"title": "the role of visual attention in sentiment prediction.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123445", "abstract": "Automated assessment of visual sentiment has many applications, such as monitoring social media and \nfacilitating online advertising. In current research on automated visual sentiment assessment, image\ns are mainly input and processed as a whole. However, human attention is biased, and a focal region \nwith high acuity can disproportionately influence visual sentiment. To investigate how attention inf\nluences visual sentiment, we conducted experiments that reveal critical insights into human percepti\non. We discover that negative sentiments are elicited by the focal region without a notable influenc\ne of contextual information, whereas positive sentiments are influenced by both focal and contextual\n information. Building on these insights, we create new deep convolutional neural networks for senti\nment prediction that have additional channels devoted to encoding focal information. On two benchmar\nk datasets, the proposed models demonstrate superior performance compared with the state-of-the-art \nmethods. Extensive visualizations and statistical analyses indicate that the focal channels are more\n effective on images with focal objects, especially for images that also elicit negative sentiments.\n", "cite_num": 3}, "187": {"title": "glad: global-local-alignment descriptor for pedestrian retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123279", "abstract": "", "cite_num": -1}, "253": {"title": "facecollage: a rapidly deployable system for real-time head reconstruction for on-the-go 3d telepresence.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123281", "abstract": "", "cite_num": -1}, "80": {"title": "an http/2-based adaptive streaming framework for 360\u00b0 virtual reality videos.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123453", "abstract": "", "cite_num": -1}, "235": {"title": "fashion world map: understanding cities through streetwear fashion.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123268", "abstract": "", "cite_num": -1}, "272": {"title": "multi-modal knowledge representation learning via webly-supervised relationships mining.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123443", "abstract": "Knowledge representation learning (KRL) encodes enormous structured information with entities and re\nlations into a continuous low-dimensional semantic space. Most conventional methods solely focus on \nlearning knowledge representation from single modality, yet neglect the complementary information fr\nom others. The more and more rich available multi-modal data on Internet also drive us to explore a \nnovel approach for KRL in multi-modal way, and overcome the limitations of previous single-modal bas\ned methods. This paper proposes a novel multi-modal knowledge representation learning (MM-KRL) frame\nwork which attempts to handle knowledge from both textual and visual modal web data. It consists of \ntwo stages, i.e., webly-supervised multi-modal relationship mining, and bi-enhanced cross-modal know\nledge representation learning. Compared with existing knowledge representation methods, our framewor\nk has several advantages: (1) It can effectively mine multi-modal knowledge with structured textual \nand visual relationships from web automatically. (2) It is able to learn a common knowledge space wh\nich is independent to both task and modality by the proposed Bi-enhanced Cross-modal Deep Neural Net\nwork (BC-DNN). (3) It has the ability to represent unseen multi-modal relationships by transferring \nthe learned knowledge with isolated seen entities and relations into unseen relationships. We build \na large-scale multi-modal relationship dataset (MMR-D) and the experimental results show that our fr\namework achieves excellent performance in zero-shot multi-modal retrieval and visual relationship re\ncognition.", "cite_num": 1}, "43": {"title": "fluency-guided cross-lingual image captioning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123366", "abstract": "", "cite_num": -1}, "268": {"title": "modeling the resource requirements of convolutional neural networks on mobile devices.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123389", "abstract": "", "cite_num": -1}, "66": {"title": "finding the secret of cnn parameter layout under strict size constraint.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123346", "abstract": "", "cite_num": -1}, "69": {"title": "future-supervised retrieval of unseen queries for live video.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123437", "abstract": "", "cite_num": -1}, "46": {"title": "two birds one stone: on both cold-start and long-tail recommendation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123316", "abstract": "", "cite_num": -1}, "75": {"title": "exploring outliers in crowdsourced ranking for qoe.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123267", "abstract": "", "cite_num": -1}, "189": {"title": "profilio: psychometric profiling to boost social media advertising.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129311", "abstract": "", "cite_num": -1}, "223": {"title": "livejack: integrating cdns and edge clouds for live content broadcasting.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123283", "abstract": "", "cite_num": -1}, "157": {"title": "request: seamless dynamic adaptive streaming over http for multi-homed smartphone under resource constraints.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123368", "abstract": "", "cite_num": -1}, "0": {"title": "pic2dish: a customized cooking assistant system.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3126490", "abstract": "", "cite_num": -1}, "135": {"title": "human-like visual learning and reasoning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130144", "abstract": "", "cite_num": -1}, "209": {"title": "multi-scale context based attention for dynamic music emotion prediction.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123408", "abstract": "Dynamic music emotion prediction is to recognize the continuous emotion information in music, which \nis necessary for music retrieval and recommendation. In this paper, we adopt the dimensional valence\n-arousal (V-A) emotion model to represent the dynamic emotion in music. In our opinion, music and V-\nA emotion label do not have the one-to-one correspondence in the time domain, while the expression o\nf music emotion at one moment is the accumulation of previous music content for a period of time, so\n we propose Long Short-Term Memory (LSTM) based sequence-to-one mapping for dynamic music emotion pr\nediction. Based on this sequence-to-one music emotion mapping, it is proved that different time scal\nes' preceding content has an influence on the LSTM model's performance, so we further propose the Mu\nlti-scale Context based Attention (MCA) for dynamic music emotion prediction. We evaluate our propos\ned method on the database of Emotion in Music task at MediaEval 2015, and the results show that our \nproposed method outperforms most of the models using the same features and achieves a competitive pe\nrformance with the state-of-the-art methods.", "cite_num": 1}, "260": {"title": "cross-media relevance computation for multimedia retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123963", "abstract": "", "cite_num": -1}, "95": {"title": "touch me here: a virtual touch cinema.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129325", "abstract": "", "cite_num": -1}, "62": {"title": "adaptive low-rank multi-label active learning for image classification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123388", "abstract": "", "cite_num": -1}, "128": {"title": "learning semantic feature map for visual content recognition.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123379", "abstract": "", "cite_num": -1}, "120": {"title": "panel: cross-media intelligence.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3133336", "abstract": "", "cite_num": -1}, "213": {"title": "data generation for improving person re-identification.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123302", "abstract": "", "cite_num": -1}, "214": {"title": "outdoor object recognition for smart audio guides.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127923", "abstract": "", "cite_num": -1}, "207": {"title": "aristo: an augmented reality platform for immersion and interactivity.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123308", "abstract": "", "cite_num": -1}, "162": {"title": "rgb-d scene recognition with object-to-object relation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123300", "abstract": "", "cite_num": -1}, "33": {"title": "las barricadas misteriosas.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129330", "abstract": "", "cite_num": -1}, "247": {"title": "beyond human-level license plate super-resolution with progressive vehicle search and domain priori gan.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123422", "abstract": "", "cite_num": -1}, "180": {"title": "spatial magnetic field visualization: interactive kinetic art installation driven by the invisible forces of magnetic fields.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129328", "abstract": "", "cite_num": -1}, "70": {"title": "richer semantic visual and language representation for video captioning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127895", "abstract": "", "cite_num": -1}, "196": {"title": "too many pixels to perceive: subpixel shutoff for display energy reduction on oled smartphones.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123344", "abstract": "", "cite_num": -1}, "243": {"title": "positive and unlabeled learning for anomaly detection with multi-features.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123304", "abstract": "... but we have temporarily restricted your access to the Digital Library.\nYour activity appears to \nbe coming from some type of automated process.\nTo ensure the availability of the Digital Library we \ncan not allow these types of requests to continue.\nThe restriction will be removed automatically onc\ne this activity stops.\n", "cite_num": 0}, "86": {"title": "understanding fashion trends from street photos via neighbor-constrained embedding learning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123441", "abstract": "", "cite_num": -1}, "11": {"title": "fast deep matting for portrait animation on mobile phone.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123286", "abstract": "", "cite_num": -1}, "211": {"title": "automatic music video generation based on simultaneous soundtrack recommendation and video editing.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123399", "abstract": "", "cite_num": -1}, "156": {"title": "discriminative training of complex-valued deep recurrent neural network for singing voice separation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123386", "abstract": "", "cite_num": -1}, "271": {"title": "learning to compose with professional photographs on the web.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123274", "abstract": "", "cite_num": -1}, "117": {"title": "multi-networks joint learning for large-scale cross-modal retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123317", "abstract": "This paper proposes a novel deep framework of multi-networks joint learning for large-scale cross-mo\ndal retrieval. For most existing cross-modal methods, the processes of training and testing don't ca\nre about the problem of memory requirement. Hence, they are generally implemented on small-scale dat\na. Moreover, they take feature learning and latent space embedding as two separate steps which canno\nt generate specific features to accord with the cross-modal task. To alleviate the problems, we firs\nt disintegrate the multiplication and inverse of some big matrices, usually involved in existing met\nhods, into that of many sub-matrices. Each sub-matrix is targeted to dispose one pair of image-sente\nnce, for which we further design a novel sampling strategy to select the most representative samples\n to construct the cross-modal ranking loss and within-modal discriminant loss functions. By this way\n, the proposed model consumes less memory each time such that it can scale to large-scale data. Furt\nhermore, we apply the proposed discriminative ranking loss to effectively unify two heterogenous net\nworks, deep residual network for images and long short-term memory for sentences, into an end-to-end\n deep learning architecture. Finally, we can simultaneously achieve specific features adapting to cr\noss-modal task and learn a shared latent space for images and sentences. Extensive evaluations on tw\no large-scale cross-modal datasets show that the proposed method brings substantial improvements ove\nr other state-of-the-art ranking methods.", "cite_num": 5}, "186": {"title": "medical multimedia information systems (mmis).", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3130142", "abstract": "", "cite_num": -1}, "212": {"title": "predicting human intentions from motion cues only: a 2d+3d fusion approach.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123298", "abstract": "", "cite_num": -1}, "26": {"title": "deep progressive hashing for image retrieval.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123280", "abstract": "", "cite_num": -1}, "37": {"title": "video visual relation detection.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123380", "abstract": "", "cite_num": -1}, "55": {"title": "16k cinematic vr streaming.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123307", "abstract": "", "cite_num": -1}, "40": {"title": "filters.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129326", "abstract": "", "cite_num": -1}, "131": {"title": "learning non-local image diffusion for image denoising.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123370", "abstract": "", "cite_num": -1}, "202": {"title": "temporally selective attention model for social and affective state recognition in multimedia content.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123413", "abstract": "The sheer amount of human-centric multimedia content has led to increased research on human behavior\n understanding. Most existing methods model behavioral sequences without considering the temporal sa\nliency. This work is motivated by the psychological observation that temporally selective attention \nenables the human perceptual system to process the most relevant information. In this paper, we intr\noduce a new approach, named Temporally Selective Attention Model (TSAM), designed to selectively att\nend to salient parts of human-centric video sequences. Our TSAM models learn to recognize affective \nand social states using a new loss function called speaker-distribution loss. Extensive experiments \nshow that our model achieves the state-of-the-art performance on rapport detection and multimodal se\nntiment analysis. We also show that our speaker-distribution loss function can generalize to other c\nomputational models, improving the prediction performance of deep averaging network and Long Short T\nerm Memory (LSTM).", "cite_num": -1}, "154": {"title": "hashtag-centric immersive search on social media.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123442", "abstract": "", "cite_num": -1}, "1": {"title": "outlining objects for interactive segmentation on touch devices.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123409", "abstract": "", "cite_num": -1}, "3": {"title": "towards smp challenge: stacking of diverse models for social image popularity prediction.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127899", "abstract": "", "cite_num": -1}, "132": {"title": "to create what you tell: generating videos from captions.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127905", "abstract": "", "cite_num": -1}, "129": {"title": "sigmm award for outstanding ph.d. thesis in multimedia computing, communications and applications 2017.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3139464", "abstract": "", "cite_num": -1}, "41": {"title": "acm sigmm rising star award 2017.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3139463", "abstract": "", "cite_num": -1}, "96": {"title": "improving event extraction via multimodal integration.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123294", "abstract": "In this paper, we focus on improving Event Extraction (EE) by incorporating visual knowledge with wo\nrds and phrases from text documents. We first discover visual patterns from large-scale text-image p\nairs in a weakly-supervised manner and then propose a multimodal event extraction algorithm where th\ne event extractor is jointly trained with textual features and visual patterns. Extensive experiment\nal results on benchmark data sets demonstrate that the proposed multimodal EE method can achieve sig\nnificantly better performance on event extraction: absolute 7.1% F-score gain on event trigger label\ning and 8.5% F-score gain on event argument labeling.", "cite_num": 3}, "177": {"title": "drag a star 3.0: an audience participatory interactive art.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3129323", "abstract": "", "cite_num": -1}, "133": {"title": "an image-based deep spectrum feature representation for the recognition of emotional speech.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123371", "abstract": "", "cite_num": -1}, "170": {"title": "weighted sparse representation regularized graph learning for rgb-t object tracking.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123289", "abstract": "", "cite_num": -1}, "19": {"title": "learning to generate and edit hairstyles.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123423", "abstract": "", "cite_num": -1}, "161": {"title": "spatio-temporal autoencoder for video anomaly detection.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123451", "abstract": "Anomalous events detection in real-world video scenes is a challenging problem due to the complexity\n of \"anomaly\" as well as the cluttered backgrounds, objects and motions in the scenes. Most existing\n methods use hand-crafted features in local spatial regions to identify anomalies. In this paper, we\n propose a novel model called Spatio-Temporal AutoEncoder (ST AutoEncoder or STAE), which utilizes d\neep neural networks to learn video representation automatically and extracts features from both spat\nial and temporal dimensions by performing 3-dimensional convolutions. In addition to the reconstruct\nion loss used in existing typical autoencoders, we introduce a weight-decreasing prediction loss for\n generating future frames, which enhances the motion feature learning in videos. Since most anomaly \ndetection datasets are restricted to appearance anomalies or unnatural motion anomalies, we collecte\nd a new challenging dataset comprising a set of real-world traffic surveillance videos. Several expe\nriments are performed on both the public benchmarks and our traffic dataset, which show that our pro\nposed method remarkably outperforms the state-of-the-art approaches.", "cite_num": 11}, "141": {"title": "modeling image virality with pairwise spatial transformer networks.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123333", "abstract": "", "cite_num": -1}, "49": {"title": "nexgentv: providing real-time insight during political debates in a second screen application.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127929", "abstract": "", "cite_num": -1}, "97": {"title": "learning multimodal attention lstm networks for video captioning.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123448", "abstract": "Automatic generation of video caption is a challenging task as video is an information-intensive med\nia with complex variations. Most existing methods, either based on language templates or sequence le\narning, have treated video as a flat data sequence while ignoring intrinsic multimodality nature. Ob\nserving that different modalities (e.g., frame, motion, and audio streams), as well as the elements \nwithin each modality, contribute differently to the sentence generation, we present a novel deep fra\nmework to boost video captioning by learning Multimodal Attention Long-Short Term Memory networks (M\nA-LSTM). Our proposed MA-LSTM fully exploits both multimodal streams and temporal attention to selec\ntively focus on specific elements during the sentence generation. Moreover, we design a novel child-\nsum fusion unit in the MA-LSTM to effectively combine different encoded modalities to the initial de\ncoding states. Different from existing approaches that employ the same LSTM structure for different \nmodalities, we train modality-specific LSTM to capture the intrinsic representations of individual m\nodalities. The experiments on two benchmark datasets (MSVD and MSR-VTT) show that our MA-LSTM signif\nicantly outperforms the state-of-the-art methods with 52.3 BLEU@4 and 70.4 CIDER-D metrics on MSVD d\nataset, respectively.", "cite_num": 17}, "226": {"title": "harnessing a.i. for augmenting creativity: application to movie trailer creation.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3127906", "abstract": "", "cite_num": -1}, "65": {"title": "3densinet: a robust neural network architecture towards 3d volumetric object prediction from 2d image.", "conf": "mm", "time": "2017", "url": "https://doi.org/10.1145/3123266.3123340", "abstract": "", "cite_num": -1}}
{"52": {"title": "early embedding and late reranking for video captioning.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984064", "abstract": "", "cite_num": -1}, "68": {"title": "multimedia and medicine: teammates for better disease detection and survival.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2976760", "abstract": "", "cite_num": -1}, "266": {"title": "n-dimensional display interface.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971476", "abstract": "", "cite_num": -1}, "124": {"title": "on estimating air pollution from photos using convolutional neural network.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967230", "abstract": "", "cite_num": -1}, "38": {"title": "tamp: a library for compact deep neural networks with structured matrices.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973802", "abstract": "", "cite_num": -1}, "8": {"title": "exploiting objects with lstms for video categorization.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967199", "abstract": "", "cite_num": -1}, "197": {"title": "a multi-video browser for endoscopic videos on tablets.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973821", "abstract": "", "cite_num": -1}, "29": {"title": "deep learning for image memorability prediction: the emotional bias.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967269", "abstract": "", "cite_num": -1}, "252": {"title": "bbridge: a big data platform for social multimedia analytics.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973836", "abstract": "", "cite_num": -1}, "269": {"title": "share-and-chat: achieving human-level video commenting by search and multi-view embedding.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964320", "abstract": "", "cite_num": -1}, "256": {"title": "synthesizing emerging images from photographs.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967304", "abstract": "", "cite_num": -1}, "110": {"title": "adaptive bitrate selection for video encoding with reduced block artifacts.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967227", "abstract": "", "cite_num": -1}, "218": {"title": "cross-batch reference learning for deep classification and retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964324", "abstract": "", "cite_num": -1}, "239": {"title": "face recognition via active annotation and learning.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984059", "abstract": "", "cite_num": -1}, "35": {"title": "smart beholder: an extensible smart lens platform.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973793", "abstract": "", "cite_num": -1}, "59": {"title": "multi-modal learning: study on a large-scale micro-video data collection.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971477", "abstract": "Micro-video sharing social services, as a new phenomenon in social media, enable users to share micr\no-videos and thus gain increasing enthusiasm among people. One distinct characteristic of micro-vide\nos is the multi-modality, as these videos always have visual signals, audio tracks, textual descript\nions as well as social clues. Such multi-modality data makes it possible to obtain a comprehensive u\nnderstanding of videos and hence provides new opportunities for researchers. However, limited effort\ns thus far have been dedicated to this new emerging user-generated contents (UGCs) due to the lack o\nf large-scale benchmark dataset. Towards this end, in this paper, we construct a large-scale micro-v\nideo dataset, which can support many research domains, such as popularity prediction and venue estim\nation. Based upon this dataset, we conduct an initial study in popularity prediction of micro-videos\n. Finally, we identify our future work.", "cite_num": 5}, "221": {"title": "dictionary learning based hashing for cross-modal retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967206", "abstract": "", "cite_num": -1}, "23": {"title": "deep bi-directional cross-triplet embedding for cross-domain clothing retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967182", "abstract": "", "cite_num": -1}, "116": {"title": "lta 2016: the first workshop on lifelogging tools and applications.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2980534", "abstract": "", "cite_num": -1}, "242": {"title": "global consistent shape correspondence for efficient and effective active shape models.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967283", "abstract": "", "cite_num": -1}, "123": {"title": "who is where?: matching people in video to wearable acceleration during crowded mingling events.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967224", "abstract": "", "cite_num": -1}, "136": {"title": "lsod: local sparse orthogonal descriptor for image matching.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967217", "abstract": "", "cite_num": -1}, "205": {"title": "deep correlation features for image style classification.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967251", "abstract": "", "cite_num": -1}, "240": {"title": "situation recognition from multimodal data.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2986913", "abstract": "Situation recognition is the problem of deriving actionable insights from heterogeneous, real-time, \nbig multimedia data to benefit human lives and resources in different applications. This tutorial wi\nll discuss the recent developments towards converting multitudes of data streams including weather p\natterns, stock prices, social media, traffic information, and disease incidents into actionable insi\nghts.", "cite_num": 1}, "138": {"title": "superstreamer: enabling progressive content streaming in a game engine.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973827", "abstract": "", "cite_num": -1}, "112": {"title": "madmom: a new python audio and music signal processing library.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973795", "abstract": "", "cite_num": -1}, "147": {"title": "a discriminative and compact audio representation for event detection.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2970377", "abstract": "", "cite_num": -1}, "45": {"title": "multimodal-based multimedia analysis, retrieval, and services in support of social media applications.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971471", "abstract": "The rapid growth in the amount of user-generated content (UGCs) online necessitates for social media\n companies to automatically extract knowledge structures (concepts) from user-generated images (UGIs\n) and user-generated videos (UGVs) to provide diverse multimedia-related services. For instance, rec\nommending preference-aware multimedia content, the understanding of semantics and sentics from UGCs,\n and automatically computing tag relevance for UGIs are benefited from knowledge structures extracte\nd from multiple modalities. Since contextual information captured by modern devices in conjunction w\nith a media item greatly helps in its understanding, we leverage both multimedia content and context\nual information (eg., spatial and temporal metadata) to address above-mentioned social media problem\ns in our doctoral research. We present our approaches, results, and works in progress on these probl\nems.", "cite_num": 10}, "60": {"title": "personal multi-view viewpoint recommendation based on trajectory distribution of the viewing target.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967265", "abstract": "", "cite_num": -1}, "258": {"title": "exploration of large image corpuses in virtual reality.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967291", "abstract": "", "cite_num": -1}, "233": {"title": "crowdsourcing biodiversity monitoring: how sharing your photo stream can sustain our planet.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2976762", "abstract": "", "cite_num": -1}, "169": {"title": "multi-modal conditional attention fusion for dimensional emotion prediction.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967286", "abstract": "Continuous dimensional emotion prediction is a challenging task where the fusion of various modaliti\nes usually achieves state-of-the-art performance such as early fusion or late fusion. In this paper,\n we propose a novel multi-modal fusion strategy named conditional attention fusion, which can dynami\ncally pay attention to different modalities at each time step. Long-short term memory recurrent neur\nal networks (LSTM-RNN) is applied as the basic uni-modality model to capture long time dependencies.\n The weights assigned to different modalities are automatically decided by the current input feature\ns and recent history information rather than being fixed at any kinds of situation. Our experimental\n results on a benchmark dataset AVEC2015 show the effectiveness of our method which outperforms seve\nral common fusion strategies for valence prediction.", "cite_num": 10}, "67": {"title": "attention-based lstm with semantic consistency for videos captioning.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967242", "abstract": "Recent progress in using Long Short-Term Memory (LSTM) for image description has motivated the explo\nration of their applications for automatically describing video content with natural language senten\nces. By taking a video as a sequence of features, LSTM model is trained on video-sentence pairs to l\nearn association of a video to a sentence. However, most existing methods compress an entire video s\nhot or frame into a static representation, without considering attention which allows for salient fe\natures. Furthermore, most existing approaches model the translating error, but ignore the correlatio\nns between sentence semantics and visual content.   To tackle these issues, we propose a novel end-t\no-end framework named aLSTMs, an attention-based LSTM model with semantic consistency, to transfer v\nideos to natural sentences. This framework integrates attention mechanism with LSTM to capture salie\nnt structures of video, and explores the correlation between multi-modal representations for generat\ning sentences with rich semantic content. More specifically, we first propose an attention mechanism\n which uses the dynamic weighted sum of local 2D Convolutional Neural Network (CNN) and 3D CNN repre\nsentations. Then, a LSTM decoder takes these visual features at time $t$ and the word-embedding feat\nure at time $t$-$1$ to generate important words. Finally, we uses multi-modal embedding to map the v\nisual and sentence features into a joint space to guarantee the semantic consistence of the sentence\n description and the video visual content. Experiments on the benchmark datasets demonstrate the sup\neriority of our method than the state-of-the-art baselines for video captioning in both BLEU and MET\nEOR.", "cite_num": 21}, "100": {"title": "inrs audiovisual quality dataset.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967204", "abstract": "", "cite_num": -1}, "193": {"title": "vitrivr: a flexible retrieval stack supporting multiple query modes for searching in multimedia collections.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973797", "abstract": "", "cite_num": -1}, "236": {"title": "marim: mobile augmented reality for interactive manuals.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973807", "abstract": "", "cite_num": -1}, "166": {"title": "analyzing and predicting gif interestingness.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967195", "abstract": "", "cite_num": -1}, "113": {"title": "barrista: caffe well-served.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973803", "abstract": "", "cite_num": -1}, "220": {"title": "multilayer and multimodal fusion of deep neural networks for video classification.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964297", "abstract": "This paper presents a novel framework to combine multiple layers and modalities of deep neural netwo\nrks for video classification. We first propose a multilayer strategy to simultaneously capture a var\niety of levels of abstraction and invariance in a network, where the convolutional and fully connect\ned layers are effectively represented by our proposed feature aggregation methods. We further introd\nuce a multimodal scheme that includes four highly complementary modalities to extract diverse static\n and dynamic cues at multiple temporal scales. In particular, for modeling the long-term temporal in\nformation, we propose a new structure, FC-RNN, to effectively transform pre-trained fully connected \nlayers into recurrent layers. A robust boosting model is then introduced to optimize the fusion of m\nultiple layers and modalities in a unified way. In the extensive experiments, we achieve state-of-th\ne-art results on two public benchmark datasets: UCF101 and HMDB51.", "cite_num": 40}, "4": {"title": "binary optimized hashing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964331", "abstract": "", "cite_num": -1}, "263": {"title": "image emotion computing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971473", "abstract": "", "cite_num": -1}, "244": {"title": "robust visual-textual sentiment analysis: when attention meets tree-structured recursive neural networks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964288", "abstract": "Sentiment analysis is crucial for extracting social signals from social media content. Due to huge v\nariation in social media, the performance of sentiment classifiers using single modality (visual or \ntextual) still lags behind satisfaction. In this paper, we propose a new framework that integrates t\nextual and visual information for robust sentiment analysis. Different from previous work, we believ\ne visual and textual information should be treated jointly in a structural fashion. Our system first\n builds a semantic tree structure based on sentence parsing, aimed at aligning textual words and ima\nge regions for accurate analysis. Next, our system learns a robust joint visual-textual semantic rep\nresentation by incorporating 1) an attention mechanism with LSTM (long short term memory) and 2) an \nauxiliary semantic learning task. Extensive experimental results on several known data sets show tha\nt our method outperforms existing the state-of-the-art joint models in sentiment analysis. We also i\nnvestigate different tree-structured LSTM (T-LSTM) variants and analyze the effect of the attention \nmechanism in order to provide deeper insight on how the attention mechanism helps the learning of th\ne joint visual-textual sentiment classifier.", "cite_num": -1}, "190": {"title": "data aesthetics: the ethics and aesthetics of big data gathering seen from the artists eye.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2993205", "abstract": "", "cite_num": -1}, "199": {"title": "efficient digital holographic image reconstruction on mobile devices.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967192", "abstract": "", "cite_num": -1}, "36": {"title": "placing broadcast news videos in their social media context using hashtags.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2970929", "abstract": "", "cite_num": -1}, "99": {"title": "local diffusion map signature for symmetry-aware non-rigid shape correspondence.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967277", "abstract": "", "cite_num": -1}, "14": {"title": "one sensor is not enough: adapting and fusing sensors for the quality assessment of user generated video.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967297", "abstract": "", "cite_num": -1}, "201": {"title": "innerview: learning place ambiance from social media images.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967261", "abstract": "", "cite_num": -1}, "102": {"title": "a compact binary aggregated descriptor via dual selection for visual search.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967256", "abstract": "", "cite_num": -1}, "106": {"title": "bidirectional long-short term memory for video description.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967258", "abstract": "", "cite_num": -1}, "130": {"title": "deep ctr prediction in display advertising.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964325", "abstract": "", "cite_num": -1}, "192": {"title": "server allocation for multiplayer cloud gaming.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964301", "abstract": "", "cite_num": -1}, "103": {"title": "frustratingly easy cross-modal hashing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967218", "abstract": "", "cite_num": -1}, "155": {"title": "academic coupled dictionary learning for sketch-based image retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964329", "abstract": "", "cite_num": -1}, "54": {"title": "improving speaker diarization of tv series using talking-face detection and clustering.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967202", "abstract": "", "cite_num": -1}, "57": {"title": "lime: a method for low-light image enhancement.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967188", "abstract": "", "cite_num": -1}, "73": {"title": "multi-pose facial expression recognition using transformed dirichlet process.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967240", "abstract": "", "cite_num": -1}, "56": {"title": "ad recommendation for sponsored search engine via composite long-short term memory.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967254", "abstract": "", "cite_num": -1}, "254": {"title": "joint image and text representation for aesthetics analysis.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967223", "abstract": "", "cite_num": -1}, "248": {"title": "describing videos using multi-modal fusion.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984065", "abstract": "Describing videos with natural language is one of the ultimate goals of video understanding. Video r\necords multi-modal information including image, motion, aural, speech and so on. MSR Video to Langua\nge Challenge provides a good chance to study multi-modality fusion in caption task. In this paper, w\ne propose the multi-modal fusion encoder and integrate it with text sequence decoder into an end-to-\nend video caption framework. Features from visual, aural, speech and meta modalities are fused toget\nher to represent the video contents. Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) ar\ne then used as the decoder to generate natural language sentences. Experimental results show the eff\nectiveness of multi-modal fusion encoder trained in the end-to-end framework, which achieved top per\nformance in both common metrics evaluation and human evaluation.", "cite_num": 40}, "107": {"title": "emerging topics in learning from noisy and missing data.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2986910", "abstract": "", "cite_num": -1}, "109": {"title": "transform-invariant convolutional neural networks for image classification and search.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964316", "abstract": "", "cite_num": -1}, "71": {"title": "seventh international workshop on human behavior understanding (hbu 2016).", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2980538", "abstract": "", "cite_num": -1}, "48": {"title": "technology & art in stimulating creative placemaking in public-use spaces.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2986912", "abstract": "", "cite_num": -1}, "257": {"title": "interactive image search for clothing recommendation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973834", "abstract": "", "cite_num": -1}, "181": {"title": "facial expression recognition with deep two-view support vector machine.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967295", "abstract": "", "cite_num": -1}, "224": {"title": "weakly-supervised recognition, localization, and explanation of visual entities.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971479", "abstract": "", "cite_num": -1}, "160": {"title": "multimedia on the mountaintop: using public snow images to improve water systems operation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2976759", "abstract": "", "cite_num": -1}, "127": {"title": "geospatial multimedia data for situation recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971472", "abstract": "", "cite_num": -1}, "225": {"title": "multimodal video description.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984066", "abstract": "Real-world web videos often contain cues to supplement visual information for generating natural lan\nguage descriptions. In this paper we propose a sequence-to-sequence model which explores such auxili\nary information. In particular, audio and the topic of the video are used in addition to the visual \ninformation in a multimodal framework to generate coherent descriptions of videos \"in the wild\". In \ncontrast to current encoder-decoder based models which exploit visual information only during the en\ncoding stage, our model fuses multiple sources of information judiciously, showing improvement over \nusing the different modalities separately. We based our multimodal video description network on the \nstate-of-the-art sequence to sequence video to text (S2VT) model and extended it to take advantage o\nf multiple modalities. Extensive experiments on the challenging MSR-VTT dataset are carried out to s\nhow the superior performance of the proposed approach on natural videos found in the web.", "cite_num": 54}, "215": {"title": "zero-example multimedia event detection and recounting with unsupervised evidence localization.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971480", "abstract": "", "cite_num": -1}, "219": {"title": "summary for avec 2016: depression, mood, and emotion recognition workshop and challenge.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2980532", "abstract": "", "cite_num": -1}, "194": {"title": "efficient mobile implementation of a cnn-based object recognition system.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967243", "abstract": "", "cite_num": -1}, "152": {"title": "performance measurements of virtual reality systems: quantifying the timing and positioning accuracy.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967303", "abstract": "", "cite_num": -1}, "121": {"title": "do textual descriptions help action recognition?", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967301", "abstract": "", "cite_num": -1}, "183": {"title": "a new tool for collaborative video search via content-based retrieval and visual inspection.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973824", "abstract": "", "cite_num": -1}, "230": {"title": "a robust distance with correlated metric learning for multi-instance multi-label data.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967259", "abstract": "", "cite_num": -1}, "83": {"title": "magic mirror: a virtual fashion consultant.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2970928", "abstract": "", "cite_num": -1}, "2": {"title": "multimedia commons workshop 2016 (mmcommons 2016): datasets, evaluation, and reproducibility.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2980533", "abstract": "", "cite_num": -1}, "262": {"title": "play and rewind: optimizing binary representations of videos by self-supervised temporal hashing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964308", "abstract": "", "cite_num": -1}, "203": {"title": "zero-shot hashing via transferring supervised knowledge.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964319", "abstract": "", "cite_num": -1}, "150": {"title": "context-aware geometric object reconstruction for mobile education.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967244", "abstract": "", "cite_num": -1}, "158": {"title": "spatio-temporal analysis of bandwidth maps for geo-predictive video streaming in mobile environments.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964333", "abstract": "", "cite_num": -1}, "47": {"title": "frame untangling for unobtrusive display-camera visible light communication.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967302", "abstract": "", "cite_num": -1}, "79": {"title": "swiden: convolutional neural networks for depiction invariant object recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967208", "abstract": "", "cite_num": -1}, "84": {"title": "vibrotactile experiences for augmented reality.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973830", "abstract": "", "cite_num": -1}, "168": {"title": "multimodal popularity prediction of brand-related social media posts.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967210", "abstract": "Brand-related user posts on social networks are growing at a staggering rate, where users express th\neir opinions about brands by sharing multimodal posts. However, while some posts become popular, oth\ners are ignored. In this paper, we present an approach for identifying what aspects of posts determi\nne their popularity. We hypothesize that brand-related posts may be popular due to several cues rela\nted to factual information, sentiment, vividness and entertainment parameters about the brand. We ca\nll the ensemble of cues engagement parameters. In our approach, we propose to use these parameters f\nor predicting brand-related user post popularity. Experiments on a collection of fast food brand-rel\nated user posts crawled from Instagram show that: visual and textual features are complementary in p\nredicting the popularity of a post; predicting popularity using our proposed engagement parameters i\ns more accurate than predicting popularity directly from visual and textual features; and our propos\ned approach makes it possible to understand what drives post popularity in general as well as isolat\ne the brand specific drivers.", "cite_num": 21}, "145": {"title": "a pragmatically designed adaptive and web-compliant object-based video streaming methodology: implementation and subjective evaluation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964300", "abstract": "", "cite_num": -1}, "245": {"title": "cross-modal retrieval with label completion.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967231", "abstract": "", "cite_num": -1}, "165": {"title": "cross-modal retrieval by real label partial least squares.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967216", "abstract": "", "cite_num": -1}, "255": {"title": "automatic reflection removal using gradient intensity and motion cues.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967264", "abstract": "", "cite_num": -1}, "78": {"title": "contextual enrichment of remote-sensed events with social media streams.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984063", "abstract": "", "cite_num": -1}, "206": {"title": "automatic music video generation based on emotion-oriented pseudo song prediction and matching.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967245", "abstract": "", "cite_num": -1}, "88": {"title": "user redirection and direct haptics in virtual environments.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964293", "abstract": "", "cite_num": -1}, "210": {"title": "partial multi-modal sparse coding via adaptive similarity structure regularization.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967201", "abstract": "Multi-modal sparse coding has played an important role in many multimedia applications, where data a\nre usually with multiple modalities. Recently, various multi-modal sparse coding approaches have bee\nn proposed to learn sparse codes of multi-modal data, which assume that data appear in all modalitie\ns, or at least there is one modality containing all data. However, in real applications, it is often\n the case that some modalities of the data may suffer from missing information and thus result in pa\nrtial multi-modality data. In this paper, we propose to solve the partial multi-modal sparse coding \nproblem via multi-modal similarity structure regularization. Specifically, we propose a partial mult\ni-modal sparse coding framework termed Adaptive Partial Multi-Modal Similarity Structure Regularizat\nion for Sparse Coding (AdaPM2SC), which preserves the similarity structure within the same modality \nand between different modalities. Experimental results conducted on two real-world datasets demonstr\nate that AdaPM2SC significantly outperforms the state-of-the-art methods under partial multi-modalit\ny scenario.", "cite_num": 5}, "250": {"title": "history rhyme: searching historic events by multimedia knowledge.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973832", "abstract": "", "cite_num": -1}, "267": {"title": "parsimonious mixed-effects hodgerank for crowdsourced preference aggregation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964298", "abstract": "", "cite_num": -1}, "93": {"title": "antiloiter: a loitering discovery system for longtime videos across multiple surveillance cameras.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2970927", "abstract": "", "cite_num": -1}, "9": {"title": "key color generation for affective multimedia production: an initial method and its application.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964323", "abstract": "", "cite_num": -1}, "85": {"title": "semantic image profiling for historic events: linking images to phrases.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964306", "abstract": "", "cite_num": -1}, "232": {"title": "deep cross residual learning for multitask visual recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964309", "abstract": "", "cite_num": -1}, "31": {"title": "video chatbot: triggering live social interactions by automatic video commenting.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973835", "abstract": "", "cite_num": -1}, "64": {"title": "multimodal learning via exploring deep semantic similarity.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967239", "abstract": "Deep learning is skilled at learning representation from raw data, which are embedded in the semanti\nc space. Traditional multimodal networks take advantage of this, and maximize the joint distribution\n over the representations of different modalities. However, the similarity among the representations\n are not emphasized, which is an important property for multimodal data. In this paper, we will intr\noduce a novel learning method for multimodal networks, named as Semantic Similarity Learning (SSL), \nwhich aims at training the model via enhancing the similarity between the high-level features of dif\nferent modalities. Sets of experiments are conducted for evaluating the method on different multimod\nal networks and multiple tasks. The experimental results demonstrate the effectiveness of SSL in kee\nping the shared information and improving the discrimination. Particularly, SSL shows its ability in\n encouraging each modality to learn transferred knowledge from the other one when faced with missing\n data.", "cite_num": 7}, "191": {"title": "motion segmentation using visual and bio-mechanical features.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967266", "abstract": "", "cite_num": -1}, "241": {"title": "kurento: the webrtc modular media server.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973798", "abstract": "", "cite_num": -1}, "122": {"title": "multimedia for personal health and health care.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2980536", "abstract": "", "cite_num": -1}, "20": {"title": "synchronization among groups of spectators for highlight detection in movies.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967229", "abstract": "", "cite_num": -1}, "159": {"title": "processing-aware privacy-preserving photo sharing over online social networks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967288", "abstract": "", "cite_num": -1}, "146": {"title": "pyo, the python dsp toolbox.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973804", "abstract": "", "cite_num": -1}, "94": {"title": "micro-expression recognition with expression-state constrained spatio-temporal feature representations.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967247", "abstract": "", "cite_num": -1}, "58": {"title": "artist-based classification via deep learning with multi-scale weighted pooling.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967299", "abstract": "", "cite_num": -1}, "91": {"title": "learning to make better mistakes: semantics-aware visual food recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967205", "abstract": "", "cite_num": -1}, "175": {"title": "novel word embedding and translation-based language modeling for extractive speech summarization.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967246", "abstract": "", "cite_num": -1}, "251": {"title": "unitbox: an advanced object detection network.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967274", "abstract": "", "cite_num": -1}, "148": {"title": "robust face recognition with deep multi-view representation learning.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984061", "abstract": "", "cite_num": -1}, "104": {"title": "v3i-stal: visual vehicle-to-vehicle interaction via simultaneous tracking and localization.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964285", "abstract": "", "cite_num": -1}, "87": {"title": "families in the wild (fiw): large-scale kinship image database and benchmarks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967219", "abstract": "", "cite_num": -1}, "179": {"title": "audio event detection using weakly labeled data.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964310", "abstract": "", "cite_num": -1}, "114": {"title": "mental visual indexing: towards fast video browsing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967296", "abstract": "", "cite_num": -1}, "39": {"title": "facial age estimation using robust label distribution.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967186", "abstract": "", "cite_num": -1}, "50": {"title": "workcache: salvaging siloed knowledge.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973809", "abstract": "", "cite_num": -1}, "7": {"title": "visual analytics for multimedia: challenges and opportunities.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984750", "abstract": "", "cite_num": -1}, "237": {"title": "learning music emotion primitives via supervised dynamic clustering.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967215", "abstract": "", "cite_num": -1}, "27": {"title": "sentiment and emotion analysis for social multimedia: methodologies and applications.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971475", "abstract": "", "cite_num": -1}, "261": {"title": "demand-adaptive clothing image retrieval using hybrid topic model.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967270", "abstract": "", "cite_num": -1}, "108": {"title": "scalable multimedia streaming in wireless networks with device-to-device cooperation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973837", "abstract": "", "cite_num": -1}, "63": {"title": "multi-stream multi-class fusion of deep networks for video classification.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964328", "abstract": "", "cite_num": -1}, "176": {"title": "learning a multi-class discriminative dictionary with nonredundancy constraints for visual classification.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967255", "abstract": "", "cite_num": -1}, "143": {"title": "wimby: what's in my backyard?", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973818", "abstract": "", "cite_num": -1}, "238": {"title": "openvq: a video quality assessment toolkit.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973800", "abstract": "", "cite_num": -1}, "246": {"title": "stressclick: sensing stress from gaze-click patterns.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964318", "abstract": "", "cite_num": -1}, "32": {"title": "what makes a good movie trailer?: interpretation from simultaneous eeg and eyetracker recording.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967187", "abstract": "", "cite_num": -1}, "198": {"title": "from seed discovery to deep reconstruction: predicting saliency in crowd via deep networks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967185", "abstract": "", "cite_num": -1}, "208": {"title": "predicting and optimizing image compression.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967305", "abstract": "", "cite_num": -1}, "200": {"title": "overview of the acm multimedia 2016 international workshop on multimedia assisted dietary management.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2980535", "abstract": "", "cite_num": -1}, "234": {"title": "detecting sarcasm in multimodal social platforms.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964321", "abstract": "Sarcasm is a peculiar form of sentiment expression, where the surface sentiment differs from the imp\nlied sentiment. The detection of sarcasm in social media platforms has been applied in the past main\nly to textual utterances where lexical indicators (such as interjections and intensifiers), linguist\nic markers, and contextual information (such as user profiles, or past conversations) were used to d\netect the sarcastic tone. However, modern social media platforms allow to create multimodal messages\n where audiovisual content is integrated with the text, making the analysis of a mode in isolation p\nartial. In our work, we first study the relationship between the textual and visual aspects in multi\nmodal posts from three major social media platforms, i.e., Instagram, Tumblr and Twitter, and we run\n a crowdsourcing task to quantify the extent to which images are perceived as necessary by human ann\notators. Moreover, we propose two different computational frameworks to detect sarcasm that integrat\ne the textual and visual modalities. The first approach exploits visual semantics trained on an exte\nrnal dataset, and concatenates the semantics features with state-of-the-art textual features. The se\ncond method adapts a visual neural network initialized with parameters trained on ImageNet to multim\nodal sarcastic posts. Results show the positive effect of combining modalities for the detection of \nsarcasm across platforms and methods.", "cite_num": 18}, "184": {"title": "abnormal event discovery in user generated photos.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2975215", "abstract": "... but we have temporarily restricted your access to the Digital Library.\nYour activity appears to \nbe coming from some type of automated process.\nTo ensure the availability of the Digital Library we \ncan not allow these types of requests to continue.\nThe restriction will be removed automatically onc\ne this activity stops.\n", "cite_num": 0}, "195": {"title": "intelli-wrench: smart navigation tool for mechanical assembly and maintenance.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973833", "abstract": "", "cite_num": -1}, "21": {"title": "emotion in context: deep semantic feature fusion for video emotion recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967196", "abstract": "", "cite_num": -1}, "76": {"title": "linear distance preserving pseudo-supervised and unsupervised hashing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964334", "abstract": "", "cite_num": -1}, "28": {"title": "joint image-text representation by gaussian visual-semantic embedding.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967212", "abstract": "", "cite_num": -1}, "51": {"title": "sensecap: synchronized data collection with microsoft kinect2 and leapmotion.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973805", "abstract": "", "cite_num": -1}, "264": {"title": "hevc-compliant tile-based streaming of panoramic video for virtual reality applications.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967292", "abstract": "", "cite_num": -1}, "42": {"title": "deepsketch2image: deep convolutional neural networks for partial sketch recognition and image retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973828", "abstract": "", "cite_num": -1}, "231": {"title": "research challenges in developing multimedia systems for managing emergency situations.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2976761", "abstract": "", "cite_num": -1}, "105": {"title": "spectral and cepstral audio noise reduction techniques in speech emotion recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967306", "abstract": "", "cite_num": -1}, "259": {"title": "exploiting hierarchical activations of neural network for image retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967197", "abstract": "", "cite_num": -1}, "134": {"title": "readme: a real-time recommendation system for mobile augmented reality ecosystems.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967233", "abstract": "", "cite_num": -1}, "34": {"title": "social and affective robotics tutorial.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2986914", "abstract": "", "cite_num": -1}, "13": {"title": "pl-ranking: a novel ranking method for cross-modal retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964336", "abstract": "", "cite_num": -1}, "217": {"title": "enabling my robot to play pictionary: recurrent neural networks for sketch recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967220", "abstract": "", "cite_num": -1}, "12": {"title": "news program detection in tv broadcast videos.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967281", "abstract": "", "cite_num": -1}, "5": {"title": "a domain robust approach for image dataset construction.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967213", "abstract": "", "cite_num": -1}, "171": {"title": "accelerating convolutional neural networks for mobile applications.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967280", "abstract": "", "cite_num": -1}, "140": {"title": "playlistcreator: an assisted approach for playlist creation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973816", "abstract": "", "cite_num": -1}, "25": {"title": "qoe prediction for enriched assessment of individual video viewing experience.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964330", "abstract": "", "cite_num": -1}, "185": {"title": "learning multimodal temporal representation for dubbing detection in broadcast media.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967211", "abstract": "Person discovery in the absence of prior identity knowledge requires accurate association of visual \nand auditory cues. In broadcast data, multimodal analysis faces additional challenges due to narrate\nd voices over muted scenes or dubbing in different languages. To address these challenges, we define\n and analyze the problem of dubbing detection in broadcast data, which has not been explored before.\n We propose a method to represent the temporal relationship between the auditory and visual streams.\n This method consists of canonical correlation analysis to learn a joint multimodal space, and long \nshort term memory (LSTM) networks to model cross-modality temporal dependencies. Our contributions a\nlso include the introduction of a newly acquired dataset of face-speech segments from TV data, which\n we have made publicly available. The proposed method achieves promising performance on this real wo\nrld dataset as compared to several baselines.", "cite_num": 6}, "182": {"title": "real-time wearable computer vision system for improved museum experience.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973813", "abstract": "", "cite_num": -1}, "144": {"title": "generating diverse image datasets with limited labeling.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967285", "abstract": "", "cite_num": -1}, "172": {"title": "driving: distributed scheduling for video streaming in vehicular wi-fi systems.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964290", "abstract": "", "cite_num": -1}, "228": {"title": "a browsing and retrieval system for broadcast videos using scene detection and automatic annotation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973825", "abstract": "", "cite_num": -1}, "10": {"title": "query adaptive instance search using object sketches.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964317", "abstract": "", "cite_num": -1}, "111": {"title": "theplantgame: actively training human annotators for domain-specific crowdsourcing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973820", "abstract": "", "cite_num": -1}, "227": {"title": "online weighted clustering for real-time abnormal event detection in video surveillance.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967279", "abstract": "Detecting abnormal events in video surveillance is a challenging problem due to the large scale, str\neam fashion video data as well as the real-time constraint. In this paper, we present an online, ada\nptive, and real-time framework to address this problem. The spatial locations in a frame is partitio\nned into grids, in each grid the proposed Adaptive Multi-scale Histogram Optical Flow (AMHOF) featur\nes are extracted and modelled by an Online Weighted Clustering (OWC) algorithm. The AMHOFs which can\nnot be fit to a cluster with large weight are regarded as abnormal events. The OWC algorithm is simp\nle to implement and computational efficient. In addition, we improve the detection performance by a \nMultiple Target Tracking (MTT) algorithm. Experimental results demonstrate our approach outperforms \nthe state-of-the-art approaches in pixel-level rate of detection at a processing speed of 30 FPS.", "cite_num": 5}, "125": {"title": "leveraging contextual cues for generating basketball highlights.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964286", "abstract": "", "cite_num": -1}, "265": {"title": "mp3dg-pcc, open source software framework for implementation and evaluation of point cloud compression.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973806", "abstract": "", "cite_num": -1}, "6": {"title": "vision and language integration meets multimedia fusion: proceedings of acm multimedia 2016 workshop.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2980537", "abstract": "", "cite_num": -1}, "17": {"title": "assessing 3d scan quality through paired-comparisons psychophysics.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967200", "abstract": "", "cite_num": -1}, "61": {"title": "a tablet annotation tool for endoscopic videos.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973822", "abstract": "", "cite_num": -1}, "22": {"title": "image captioning with deep bidirectional lstms.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964299", "abstract": "", "cite_num": -1}, "149": {"title": "boosting video description generation by explicitly translating from frame-level captions.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967298", "abstract": "", "cite_num": -1}, "174": {"title": "multimedia privacy.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2986915", "abstract": "", "cite_num": -1}, "82": {"title": "experience individualization on online tv platforms through persona-based account decomposition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967221", "abstract": "", "cite_num": -1}, "92": {"title": "multiview video super-resolution via information extraction and merging.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967260", "abstract": "", "cite_num": -1}, "222": {"title": "application-layer rate-adaptive multicast video streaming over 802.11 for mobile devices.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967272", "abstract": "", "cite_num": -1}, "77": {"title": "contagnet: exploiting user context for image tag recommendation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984068", "abstract": "", "cite_num": -1}, "142": {"title": "adaptation of word vectors using tree structure for visual semantics.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967226", "abstract": "", "cite_num": -1}, "164": {"title": "image2text: a multimodal image captioner.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973831", "abstract": "In this work, we showcase the Image2Text system, which is a real-time captioning system that can gen\nerate human-level natural language description for any input image. We formulate the problem of imag\ne captioning as a multimodal translation task. Analogous to machine translation, we present a sequen\nce-to-sequence recurrent neural networks (RNN) model for image caption generation. Different from mo\nst existing work where the whole image is represented by a convolutional neural networks (CNN) featu\nre, we propose to represent the input image as a sequence of detected objects to serve as the source\n sequence of the RNN model. Based on the captioning framework, we develop a user-friendly system to \nautomatically generated human-level captions for users. The system also enables users to detect sali\nent objects in an image, and retrieve similar images and corresponding descriptions from a database.\n", "cite_num": 5}, "153": {"title": "action recognition based on joint trajectory maps using convolutional neural networks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967191", "abstract": "", "cite_num": -1}, "167": {"title": "cnn vs. sift for image retrieval: alternative or complementary?", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967252", "abstract": "", "cite_num": -1}, "204": {"title": "a novel shadow-free feature extractor for real-time road detection.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967294", "abstract": "", "cite_num": -1}, "270": {"title": "shorter-is-better: venue category estimation from micro-video.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964307", "abstract": "", "cite_num": -1}, "44": {"title": "super resolution of the partial pixelated images with deep convolutional neural network.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967235", "abstract": "", "cite_num": -1}, "173": {"title": "morph: a fast and scalable cloud transcoding system.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973792", "abstract": "", "cite_num": -1}, "15": {"title": "capped lp-norm graph embedding for photo clustering.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967257", "abstract": "", "cite_num": -1}, "72": {"title": "event specific multimodal pattern mining for knowledge base construction.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964287", "abstract": "Knowledge bases, which consist of a collection of entities, attributes, and the relations between th\nem are widely used and important for many information retrieval tasks. Knowledge base schemas are of\nten constructed manually using experts with specific domain knowledge for the field of interest. Onc\ne the knowledge base is generated then many tasks such as automatic content extraction and knowledge\n base population can be performed, which have so far been robustly studied by the Natural Language P\nrocessing community. However, the current approaches ignore visual information that could be used to\n build or populate these structured ontologies. Preliminary work on visual knowledge base constructi\non only explores limited basic objects and scene relations. In this paper, we propose a novel multim\nodal pattern mining approach towards constructing a high-level \"event\" schema semi-automatically, wh\nich has the capability to extend text only methods for schema construction. We utilize a large uncon\nstrained corpus of weakly-supervised image-caption pairs related to high-level events such as \"attac\nk\" and \"demonstration\" to both discover visual aspects of an event, and name these visual components\n automatically. We compare our method with several state-of-the-art visual pattern mining approaches\n and demonstrate that our proposed method can achieve dramatic improvements in terms of the number o\nf concepts discovered (33% gain), semantic consistence of visual patterns (52% gain), and correctnes\ns of pattern naming (150% gain).", "cite_num": 9}, "163": {"title": "kvazaar: open-source hevc/h.265 encoder.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973796", "abstract": "", "cite_num": -1}, "249": {"title": "neighborhood-preserving hashing for large-scale cross-modal search.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967241", "abstract": "", "cite_num": -1}, "18": {"title": "socialfx: studying a crowdsourced folksonomy of audio effects terms.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967207", "abstract": "", "cite_num": -1}, "101": {"title": "deep convolutional neural network with independent softmax for large scale face recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984060", "abstract": "", "cite_num": -1}, "229": {"title": "context-aware image tweet modelling and recommendation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964291", "abstract": "", "cite_num": -1}, "119": {"title": "multimodal interest level estimation via variational bayesian mixture of robust cca.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967248", "abstract": "This paper presents a method which estimates interest level while watching videos, based on collabor\native use of facial expression and biological signals such as electroencephalogram (EEG) and electro\ncardiogram (ECG). To the best of our knowledge, no studies have been carried out on the collaborativ\ne use of facial expression and biological signals for estimating interest level. Since training data\n, which is used for estimating interest level, is generally small and imbalanced, Variational Bayesi\nan Mixture of Robust Canonical Correlation Analysis (VBMRCCA) is newly applied to facial expression \nand biological signals, which are obtained from users while they are watching the videos. Unlike som\ne related works, VBMRCCA is used to obtain the posterior distributions which represent the latent co\nrrelation between facial expression and biological signals in our method. Then, the users' interest \nlevel can be estimated by comparing the posterior distributions of the positive class data with thos\ne of the negative. Consequently, successful interest level estimation, via collaborative use of faci\nal expression and biological signals, becomes feasible.", "cite_num": 3}, "118": {"title": "supervised recurrent hashing for large scale video retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967225", "abstract": "", "cite_num": -1}, "178": {"title": "cnndroid: gpu-accelerated execution of trained deep convolutional neural networks on android.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973801", "abstract": "", "cite_num": -1}, "188": {"title": "crowdnet: a deep convolutional network for dense crowd counting.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967300", "abstract": "", "cite_num": -1}, "98": {"title": "how cosmopolitan are emojis?: exploring emojis usage and meaning over different languages with distributional semantics.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967278", "abstract": "", "cite_num": -1}, "30": {"title": "video generation using 3d convolutional neural network.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967287", "abstract": "", "cite_num": -1}, "90": {"title": "image captioning with both object and scene information.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984069", "abstract": "", "cite_num": -1}, "137": {"title": "ensemble of sparse cross-modal metrics for heterogeneous face recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964311", "abstract": "", "cite_num": -1}, "74": {"title": "micro tells macro: predicting the popularity of micro-videos via a transductive model.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964314", "abstract": "", "cite_num": -1}, "24": {"title": "sdndash: improving qoe of http adaptive streaming using software defined networking.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964332", "abstract": "", "cite_num": -1}, "151": {"title": "improved dense trajectory with cross streams.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967222", "abstract": "", "cite_num": -1}, "81": {"title": "leveraging icn for secure content distribution in ip networks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973838", "abstract": "", "cite_num": -1}, "53": {"title": "a perceptual quality metric for videos distorted by spatially correlated noise.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964302", "abstract": "", "cite_num": -1}, "89": {"title": "transportation mode detection on mobile devices using recurrent nets.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967249", "abstract": "", "cite_num": -1}, "216": {"title": "news archive exploration combining face detection and tracking with network visual analytics.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973823", "abstract": "", "cite_num": -1}, "16": {"title": "detecting arbitrary oriented text in the wild with a visual attention model.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967282", "abstract": "Text embedded in images provides important semantic information about a scene and its content. Detec\nting text in an unconstrained environment is a challenging task because of the many fonts, sizes, ba\nckgrounds, and alignments of the characters. We present a novel attention model for detecting arbitr\nary oriented and curved scene text. Inspired by the attention mechanisms in the human visual system,\n our model utilizes a spatial glimpse network to processes the attended area and deploys a recurrent\n neural network that aggregates the information over time to determine the attention movement. Combi\nning this with an off-the-shelf region proposal method, the model achieves the state-of-the-art perf\normance on the highly cited ICDAR2013 dataset, and the MSRA-TD500 dataset which contains arbitrary o\nriented text.", "cite_num": 8}, "115": {"title": "multi-protocol video delivery with late trans-muxing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967189", "abstract": "", "cite_num": -1}, "126": {"title": "modular parallelization framework for multi-stream video processing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973799", "abstract": "", "cite_num": -1}, "139": {"title": "superselect: an interactive superpixel-based segmentation method for touch displays.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973819", "abstract": "", "cite_num": -1}, "187": {"title": "a fast cattle recognition system using smart devices.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973829", "abstract": "", "cite_num": -1}, "253": {"title": "event localization in music auto-tagging.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964292", "abstract": "", "cite_num": -1}, "80": {"title": "deep-based ingredient recognition for cooking recipe retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964315", "abstract": "", "cite_num": -1}, "235": {"title": "action recognition using local consistent group sparse coding with spatio-temporal structure.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967234", "abstract": "", "cite_num": -1}, "43": {"title": "discriminative paired dictionary learning for visual recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967184", "abstract": "", "cite_num": -1}, "268": {"title": "towards ultra-low-bitrate video conferencing using facial landmarks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967284", "abstract": "", "cite_num": -1}, "66": {"title": "joint graph learning and video segmentation via multiple cues and topology calibration.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964295", "abstract": "", "cite_num": -1}, "69": {"title": "what makes photo cultures different?", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967228", "abstract": "", "cite_num": -1}, "46": {"title": "adaptive visual feedback generation for facial expression improvement with multi-task deep neural networks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967236", "abstract": "", "cite_num": -1}, "75": {"title": "aksda-msvm: a gpu-accelerated multiclass learning framework for multimedia.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967263", "abstract": "", "cite_num": -1}, "189": {"title": "looking good with flickr faves: gaussian processes for finding difference makers in personality impressions.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967253", "abstract": "", "cite_num": -1}, "223": {"title": "weighted linear fusion of multimodal data: a reasonable baseline?", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964304", "abstract": "The ever-increasing demand for reliable inference capable of handling unpredictable challenges of pr\nactical application in the real world, has made research on information fusion of major importance. \nThere are few fields of application and research where this is more evident than in the sphere of mu\nltimedia which by its very nature inherently involves the use of multiple modalities, be it for lear\nning, prediction, or human-computer interaction, say. In the development of the most common type, sc\nore-level fusion algorithms, it is virtually without an exception desirable to have as a reference s\ntarting point a simple and universally sound baseline benchmark which newly developed approaches can\n be compared to. One of the most pervasively used methods is that of weighted linear fusion. It has \ncemented itself as the default off-the-shelf baseline owing to its simplicity of implementation, int\nerpretability, and surprisingly competitive performance across a wide range of application domains a\nnd information source types. In this paper I argue that despite this track record, weighted linear f\nusion is not a good baseline on the grounds that there is an equally simple and interpretable altern\native - namely quadratic mean-based fusion - which is theoretically more principled and which is mor\ne successful in practice. I argue the former from first principles and demonstrate the latter using \na series of experiments on a diverse set of fusion problems: computer vision-based object recognitio\nn, arrhythmia detection, and fatality prediction in motor vehicle accidents.", "cite_num": 6}, "157": {"title": "fast supervised lda for discovering micro-events in large-scale video datasets.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967237", "abstract": "", "cite_num": -1}, "0": {"title": "analyzing structural characteristics of object category representations from their semantic-part distributions.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967190", "abstract": "", "cite_num": -1}, "135": {"title": "jockey time: making video playback to enhance emotional effect.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967183", "abstract": "", "cite_num": -1}, "209": {"title": "deeply-supervised recurrent convolutional neural network for saliency detection.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967250", "abstract": "", "cite_num": -1}, "260": {"title": "locality-preserving k-svd based joint dictionary and classifier learning for object recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967267", "abstract": "", "cite_num": -1}, "95": {"title": "matchdr: image correspondence by leveraging distance ratio constraint.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967293", "abstract": "", "cite_num": -1}, "62": {"title": "time matters: multi-scale temporalization of social media popularity.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964335", "abstract": "", "cite_num": -1}, "128": {"title": "beauty emakeup: a deep makeup transfer system.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973812", "abstract": "", "cite_num": -1}, "120": {"title": "semantic description of timbral transformations in music production.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967238", "abstract": "", "cite_num": -1}, "213": {"title": "a digital world to thrive in: how the internet of things can make the \"invisible hand\" work.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984749", "abstract": "", "cite_num": -1}, "214": {"title": "objectness-aware semantic segmentation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967232", "abstract": "", "cite_num": -1}, "207": {"title": "multi-scale triplet cnn for person re-identification.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967209", "abstract": "", "cite_num": -1}, "162": {"title": "scenetextreg: a real-time video ocr system.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973811", "abstract": "", "cite_num": -1}, "33": {"title": "predicting personalized emotion perceptions of social images.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964289", "abstract": "", "cite_num": -1}, "247": {"title": "first person view video summarization subject to the user needs.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2971474", "abstract": "", "cite_num": -1}, "180": {"title": "a supervised approach for text illustration.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967214", "abstract": "", "cite_num": -1}, "70": {"title": "affective contextual mobile recommender system.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964327", "abstract": "", "cite_num": -1}, "196": {"title": "dynamic resource provisioning with qos guarantee for video transcoding in online video sharing service.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964296", "abstract": "", "cite_num": -1}, "243": {"title": "scene image synthesis from natural sentences using hierarchical syntactic analysis.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967193", "abstract": "", "cite_num": -1}, "86": {"title": "a deeply-supervised deconvolutional network for horizon line detection.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967198", "abstract": "", "cite_num": -1}, "11": {"title": "first-person shooter game for virtual reality headset with advanced multi-agent intelligent system.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973826", "abstract": "", "cite_num": -1}, "211": {"title": "generating affective captions using concept and syntax transition networks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984070", "abstract": "", "cite_num": -1}, "156": {"title": "lightnet: a versatile, standalone matlab-based environment for deep learning.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973791", "abstract": "", "cite_num": -1}, "117": {"title": "a multimodal gamified platform for real-time user feedback in sports performance.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973815", "abstract": "In this paper we introduce a novel platform that utilises multi-modal low-cost motion capture techno\nlogy for the delivery of real-time visual feedback for sports performance. This platform supports th\ne expansion to multi-modal interfaces that utilise haptic and audio feedback, which scales effective\nly with motor task complexity. We demonstrate an implementation of our platform within the field of \nsports performance. The platform includes low-cost motion capture through a fusion technique, combin\ning a Microsoft Kinect V2 with two wrist inertial sensors, which make use of the accelerometer and g\nyroscope sensors, alongside a game-based Graphical User Interface (GUI) for instruction, visual feed\nback and gamified score tracking.", "cite_num": 1}, "186": {"title": "multi-modal multi-view topic-opinion mining for social event analysis.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964294", "abstract": "In this paper, we propose a novel multi-modal multi-view topic-opinion mining (MMTOM) model for soci\nal event analysis in multiple collection sources. Compared with existing topic-opinion mining method\ns, our proposed model has several advantages: (1) The proposed MMTOM can effectively take into accou\nnt multi-modal and multi-view properties jointly in a unified and principled way for social event mo\ndeling. (2) Our model is general and can be applied to many other applications in multimedia, such a\ns opinion mining and sentiment analysis, multi-view association visualization, and topic-opinion min\ning for movie review. (3) The proposed MMTOM is able to not only discover multi-modal common topics \nfrom all collections as well as summarize the similarities and differences of these collections alon\ng each specific topic, but also automatically mine multi-view opinions on the learned topics across \ndifferent collections. (4) Our topic-opinion mining results can be effectively applied to many appli\ncations including multi-modal multi-view topic-opinion retrieval and visualization, which achieve mu\nch better performance than existing methods. To evaluate the proposed model, we collect a real-world\n dataset for research on multi-modal multi-view social event analysis, and will release it for acade\nmic use. We have conducted extensive experiments, and both qualitative and quantitative evaluation r\nesults have demonstrated the effectiveness of the proposed MMTOM.", "cite_num": 10}, "212": {"title": "geotracks: adaptive music for everyday journeys.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967181", "abstract": "", "cite_num": -1}, "26": {"title": "a live face swapper.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973808", "abstract": "", "cite_num": -1}, "37": {"title": "a fast 3d retrieval algorithm via class-statistic and pair-constraint model.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967194", "abstract": "", "cite_num": -1}, "55": {"title": "human pose estimation from depth images via inference embedded multi-task learning.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964322", "abstract": "", "cite_num": -1}, "40": {"title": "label tree embeddings for acoustic scene classification.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967268", "abstract": "", "cite_num": -1}, "131": {"title": "alone versus in-a-group: a comparative analysis of facial affect recognition.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967276", "abstract": "", "cite_num": -1}, "202": {"title": "altmm 2016: 1st international workshop on multimedia alternate realities.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2980531", "abstract": "", "cite_num": -1}, "154": {"title": "location-independent wifi action recognition via vision-based methods.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967203", "abstract": "", "cite_num": -1}, "1": {"title": "dash2m: exploring http/2 for internet streaming to mobile devices.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964313", "abstract": "", "cite_num": -1}, "3": {"title": "deep multi-task learning with label correlation constraint for video concept detection.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967271", "abstract": "", "cite_num": -1}, "132": {"title": "are safer looking neighborhoods more lively?: a multimodal investigation into urban life.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964312", "abstract": "Policy makers, urban planners, architects, sociologists, and economists are interested in creating u\nrban areas that are both lively and safe. But are the safety and liveliness of neighborhoods indepen\ndent characteristics? Or are they just two sides of the same coin? In a world where people avoid uns\nafe looking places, neighborhoods that look unsafe will be less lively, and will fail to harness the\n natural surveillance of human activity. But in a world where the preference for safe looking neighb\norhoods is small, the connection between the perception of safety and liveliness will be either weak\n or nonexistent. In this paper we explore the connection between the levels of activity and the perc\neption of safety of neighborhoods in two major Italian cities by combining mobile phone data (as a p\nroxy for activity or liveliness) with scores of perceived safety estimated using a Convolutional Neu\nral Network trained on a dataset of Google Street View images scored using a crowdsourced visual per\nception survey. We find that: (i) safer looking neighborhoods are more active than what is expected \nfrom their population density, employee density, and distance to the city centre; and (ii) that the \ncorrelation between appearance of safety and activity is positive, strong, and significant, for fema\nles and people over 50, but negative for people under 30, suggesting that the behavioral impact of p\nerception depends on the demographic of the population. Finally, we use occlusion techniques to iden\ntify the urban features that contribute to the appearance of safety, finding that greenery and stree\nt facing windows contribute to a positive appearance of safety (in agreement with Oscar Newman's def\nensible space theory). These results suggest that urban appearance modulates levels of human activit\ny and, consequently, a neighborhood's rate of natural surveillance.", "cite_num": 11}, "129": {"title": "frame- and segment-level features and candidate pool evaluation for video caption generation.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984062", "abstract": "", "cite_num": -1}, "41": {"title": "detecting violence in video using subclasses.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967289", "abstract": "", "cite_num": -1}, "96": {"title": "video ecommerce: towards online video advertising.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964326", "abstract": "", "cite_num": -1}, "177": {"title": "hypervideo production using crowdsourced youtube videos.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973810", "abstract": "", "cite_num": -1}, "133": {"title": "a platform for building new human-computer interface systems that support online automatic recognition of audio-gestural commands.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973794", "abstract": "", "cite_num": -1}, "170": {"title": "patterns of free-form curation: visual thinking with web content.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964303", "abstract": "", "cite_num": -1}, "19": {"title": "deep representation for abnormal event detection in crowded scenes.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967290", "abstract": "Abnormal event detection is extremely important, especially for video surveillance. Nowadays, many d\netectors have been proposed based on hand-crafted features. However, it remains challenging to effec\ntively distinguish abnormal events from normal ones. This paper proposes a deep representation based\n algorithm which extracts features in an unsupervised fashion. Specially, appearance, texture, and s\nhort-term motion features are automatically learned and fused with stacked denoising autoencoders. S\nubsequently, long-term temporal clues are modeled with a long short-term memory (LSTM) recurrent net\nwork, in order to discover meaningful regularities of video events. The abnormal events are identifi\ned as samples which disobey these regularities. Moreover, this paper proposes a spatial anomaly dete\nction strategy via manifold ranking, aiming at excluding false alarms. Experiments and comparisons o\nn real world datasets show that the proposed algorithm outperforms state of the arts for the abnorma\nl event detection problem in crowded scenes.", "cite_num": 11}, "161": {"title": "scalable compression of deep neural networks.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967273", "abstract": "", "cite_num": -1}, "141": {"title": "the lifecycle of geotagged multimedia data.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2986911", "abstract": "", "cite_num": -1}, "49": {"title": "quartet-net learning for visual instance retrieval.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2967262", "abstract": "", "cite_num": -1}, "97": {"title": "high-speed depth stream generation from a hybrid camera.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2964305", "abstract": "", "cite_num": -1}, "226": {"title": "an intention-aware interactive system for mobile video browsing.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2973814", "abstract": "", "cite_num": -1}, "65": {"title": "tracking natural events through social media and computer vision.", "conf": "mm", "time": "2016", "url": "https://doi.org/10.1145/2964284.2984067", "abstract": "", "cite_num": -1}}
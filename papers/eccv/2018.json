{"191": {"title": "learning to dodge a bullet: concyclic view morphing via deep learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_14", "abstract": "", "cite_num": -1}, "235": {"title": "pose-normalized image generation for person re-identification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_40", "abstract": "", "cite_num": -1}, "475": {"title": "mvsnet: depth inference for unstructured multi-view stereo.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_47", "abstract": "", "cite_num": -1}, "616": {"title": "fast light field reconstruction with deep coarse-to-fine modeling of spatial-angular clues.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_9", "abstract": "", "cite_num": -1}, "284": {"title": "deepwrinkles: accurate and realistic clothing modeling.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_41", "abstract": "", "cite_num": -1}, "8": {"title": "graph distillation for action detection with privileged modalities.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_11", "abstract": "", "cite_num": -1}, "18": {"title": "3d ego-pose estimation via imitation learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_45", "abstract": "", "cite_num": -1}, "393": {"title": "supervising the new with the old: learning sfm from sfm.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_43", "abstract": "", "cite_num": -1}, "419": {"title": "ganimation: anatomically-aware facial animation from a single image.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_50", "abstract": "", "cite_num": -1}, "110": {"title": "revisiting autofocus for smartphone cameras.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_32", "abstract": "", "cite_num": -1}, "35": {"title": "on the solvability of viewing graphs.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_20", "abstract": "", "cite_num": -1}, "450": {"title": "pivot correlational neural network for multimodal video categorization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_24", "abstract": "This paper considers an architecture for multimodal video categorization referred to as Pivot Correl\national Neural Network (Pivot CorrNN). The architecture consists of modal-specific streams dedicated\n exclusively to one specific modal input as well as modal-agnostic pivot stream that considers all m\nodal inputs without distinction, and the architecture tries to refine the pivot prediction based on \nmodal-specific predictions. The Pivot CorrNN consists of three modules: (1) maximizing pivot-correla\ntion module that maximizes the correlation between the hidden states as well as the predictions of t\nhe modal-agnostic pivot stream and modal-specific streams in the network, (2) contextual Gated Recur\nrent Unit (cGRU) module which extends the capability of a generic GRU to take multimodal inputs in u\npdating the pivot hidden-state, and (3) adaptive aggregation module that aggregates all modal-specif\nic predictions as well as the modal-agnostic pivot predictions into one final prediction. We evaluat\ne the Pivot CorrNN on two publicly available large-scale multimodal video categorization datasets, F\nCVID and YouTube-8M. From the experimental results, Pivot CorrNN achieves the best performance on th\ne FCVID database and performance comparable to the state-of-the-art on YouTube-8M database.", "cite_num": 1}, "241": {"title": "structure-from-motion-aware patchmatch for adaptive optical flow estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_35", "abstract": "", "cite_num": -1}, "668": {"title": "learnable pins: cross-modal embeddings for person identity.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_5", "abstract": "", "cite_num": -1}, "441": {"title": "learning to predict crisp boundaries.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_35", "abstract": "", "cite_num": -1}, "624": {"title": "mplp++: fast, parallel dual block-coordinate ascent for dense graphical models.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_16", "abstract": "", "cite_num": -1}, "474": {"title": "semi-supervised adversarial learning to generate photorealistic face images of new identities from 3d morphable model.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_14", "abstract": "", "cite_num": -1}, "361": {"title": "hierarchical bilinear pooling for fine-grained visual recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_35", "abstract": "", "cite_num": -1}, "716": {"title": "selective zero-shot classification with augmented attributes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_29", "abstract": "", "cite_num": -1}, "147": {"title": "progressive neural architecture search.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_2", "abstract": "", "cite_num": -1}, "189": {"title": "unsupervised class-specific deblurring.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_22", "abstract": "", "cite_num": -1}, "346": {"title": "learning to navigate for fine-grained classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_26", "abstract": "", "cite_num": -1}, "249": {"title": "value-aware quantization for training and inference of neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_36", "abstract": "", "cite_num": -1}, "169": {"title": "open set learning with counterfactual images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_38", "abstract": "", "cite_num": -1}, "67": {"title": "object level visual reasoning in videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_7", "abstract": "", "cite_num": -1}, "313": {"title": "associating inter-image salient instances for weakly supervised semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_23", "abstract": "", "cite_num": -1}, "113": {"title": "mancs: a multi-task attentional network with curriculum sampling for person re-identification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_23", "abstract": "We propose a novel deep network called Mancs that solves the person re-identification problem from t\nhe following aspects: fully utilizing the attention mechanism for the person misalignment problem an\nd properly sampling for the ranking loss to obtain more stable person representation. Technically, w\ne contribute a novel fully attentional block which is deeply supervised and can be plugged into any \nCNN, and a novel curriculum sampling method which is effective for training ranking losses. The lear\nning tasks are integrated into a unified framework and jointly optimized. Experiments have been carr\nied out on Market1501, CUHK03 and DukeMTMC. All the results show that Mancs can significantly outper\nform the previous state-of-the-arts. In addition, the effectiveness of the newly proposed ideas has \nbeen confirmed by extensive ablation studies.", "cite_num": -1}, "4": {"title": "characterizing adversarial examples based on spatial consistency information for semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_14", "abstract": "", "cite_num": -1}, "194": {"title": "adaptively transforming graph matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_38", "abstract": "", "cite_num": -1}, "576": {"title": "to learn image super-resolution, use a gan to learn how to do image degradation first.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_12", "abstract": "", "cite_num": -1}, "201": {"title": "scenes-objects-actions: a multi-task, multi-label video dataset.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_39", "abstract": "", "cite_num": -1}, "504": {"title": "realtime time synchronized event-based stereo.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_27", "abstract": "", "cite_num": -1}, "124": {"title": "deep continuous fusion for multi-sensor 3d object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_39", "abstract": "", "cite_num": -1}, "586": {"title": "graininess-aware deep feature learning for pedestrian detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_45", "abstract": "", "cite_num": -1}, "629": {"title": "constraint-aware deep neural network compression.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_25", "abstract": "", "cite_num": -1}, "498": {"title": "task-driven webpage saliency.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_18", "abstract": "", "cite_num": -1}, "509": {"title": "simultaneous 3d reconstruction for water surface and underwater scene.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_46", "abstract": "", "cite_num": -1}, "56": {"title": "hairnet: single-view hair reconstruction using convolutional neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_15", "abstract": "", "cite_num": -1}, "369": {"title": "relaxation-free deep hashing via policy gradient.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_9", "abstract": "", "cite_num": -1}, "378": {"title": "3dfeat-net: weakly supervised local 3d features for point cloud registration.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_37", "abstract": "", "cite_num": -1}, "638": {"title": "light structure from pin motion: simple and accurate point light calibration for physics-based modeling.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_1", "abstract": "", "cite_num": -1}, "409": {"title": "relocnet: continuous metric learning relocalisation using neural nets.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_46", "abstract": "", "cite_num": -1}, "332": {"title": "end-to-end incremental learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_15", "abstract": "", "cite_num": -1}, "270": {"title": "look before you leap: bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_3", "abstract": "", "cite_num": -1}, "351": {"title": "deterministic consensus maximization with biconvex programming.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_42", "abstract": "", "cite_num": -1}, "304": {"title": "composition loss for counting, density map estimation and localization in dense crowds.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_33", "abstract": "", "cite_num": -1}, "230": {"title": "learning with biased complementary labels.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_5", "abstract": "", "cite_num": -1}, "663": {"title": "sampling algebraic varieties for robust camera autocalibration.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_17", "abstract": "", "cite_num": -1}, "769": {"title": "car-net: clairvoyant attentive recurrent network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_10", "abstract": "", "cite_num": -1}, "603": {"title": "deforming autoencoders: unsupervised disentangling of shape and appearance.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_40", "abstract": "", "cite_num": -1}, "281": {"title": "deep texture and structure aware filtering network for image smoothing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_14", "abstract": "", "cite_num": -1}, "336": {"title": "learning to look around objects for top-view representations of outdoor scenes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_48", "abstract": "", "cite_num": -1}, "693": {"title": "grounding visual explanations.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_17", "abstract": "", "cite_num": -1}, "631": {"title": "deep factorised inverse-sketching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_3", "abstract": "", "cite_num": -1}, "316": {"title": "the mutex watershed: efficient, parameter-free image partitioning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_34", "abstract": "", "cite_num": -1}, "538": {"title": "revisiting the inverted indices for billion-scale approximate nearest neighbors.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_13", "abstract": "", "cite_num": -1}, "258": {"title": "depth-aware cnn for rgb-d segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_9", "abstract": "", "cite_num": -1}, "420": {"title": "semi-supervised fusedgan for conditional image generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_41", "abstract": "", "cite_num": -1}, "652": {"title": "pyramid dilated deeper convlstm for video salient object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_44", "abstract": "", "cite_num": -1}, "267": {"title": "improved structure from motion using fiducial marker matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_17", "abstract": "", "cite_num": -1}, "669": {"title": "stroke controllable fast style transfer with adaptive receptive fields.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_15", "abstract": "", "cite_num": -1}, "530": {"title": "quadtree convolutional neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_34", "abstract": "", "cite_num": -1}, "711": {"title": "toward scale-invariance and position-sensitive region proposal networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_11", "abstract": "", "cite_num": -1}, "88": {"title": "mvtec d2s: densely segmented supermarket dataset.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_35", "abstract": "", "cite_num": -1}, "481": {"title": "a new large scale dynamic texture dataset with application to convnet understanding.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_20", "abstract": "", "cite_num": -1}, "31": {"title": "recognition in terra incognita.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_28", "abstract": "", "cite_num": -1}, "122": {"title": "activestereonet: end-to-end self-supervised learning for active stereo systems.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_48", "abstract": "", "cite_num": -1}, "595": {"title": "triplet loss in siamese network for object tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_28", "abstract": "", "cite_num": -1}, "176": {"title": "foresthash: semantic hashing with shallow random forests and tiny convolutional networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_27", "abstract": "", "cite_num": -1}, "175": {"title": "self-calibrating isometric non-rigid structure-from-motion.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_16", "abstract": "", "cite_num": -1}, "104": {"title": "is robustness the cost of accuracy? - a comprehensive study on the robustness of 18 deep image classification models.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_39", "abstract": "", "cite_num": -1}, "282": {"title": "dcan: dual channel-wise alignment networks for unsupervised scene adaptation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_32", "abstract": "", "cite_num": -1}, "196": {"title": "joint representation and truncated inference learning for correlation filter based tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_34", "abstract": "", "cite_num": -1}, "114": {"title": "efficient dense point cloud object reconstruction using deformation vector fields.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_31", "abstract": "", "cite_num": -1}, "39": {"title": "tbn: convolutional neural network with ternary inputs and binary weights.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_20", "abstract": "", "cite_num": -1}, "442": {"title": "video object detection with an aligned spatial-temporal memory.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_30", "abstract": "", "cite_num": -1}, "403": {"title": "learning rigidity in dynamic scenes with a moving camera for 3d motion field estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_29", "abstract": "", "cite_num": -1}, "252": {"title": "toward characteristic-preserving image-based virtual try-on network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_36", "abstract": "", "cite_num": -1}, "388": {"title": "zero-shot object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_24", "abstract": "", "cite_num": -1}, "718": {"title": "deep pictorial gaze estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_44", "abstract": "", "cite_num": -1}, "470": {"title": "integrating egocentric videos in top-view surveillance videos: joint identification and temporal alignment.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_18", "abstract": "", "cite_num": -1}, "21": {"title": "face recognition with contrastive convolution.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_8", "abstract": "", "cite_num": -1}, "76": {"title": "fisheyerecnet: a multi-context collaborative deep network for fisheye image rectification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_29", "abstract": "", "cite_num": -1}, "712": {"title": "holistic 3d scene parsing and reconstruction from a single rgb image.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_12", "abstract": "", "cite_num": -1}, "529": {"title": "deepjdot: deep joint distribution optimal transport for unsupervised domain adaptation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_28", "abstract": "", "cite_num": -1}, "671": {"title": "distortion-aware convolutional filters for dense prediction in panoramic images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_43", "abstract": "", "cite_num": -1}, "264": {"title": "pm-gans: discriminative representation learning for action recognition using partial-modalities.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_24", "abstract": "", "cite_num": -1}, "706": {"title": "occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_38", "abstract": "", "cite_num": -1}, "105": {"title": "transferring gans: generating images from limited data.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_14", "abstract": "", "cite_num": -1}, "311": {"title": "understanding perceptual and conceptual fluency at a large scale.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_41", "abstract": "", "cite_num": -1}, "315": {"title": "omnidepth: dense depth estimation for indoors spherical panoramas.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_28", "abstract": "", "cite_num": -1}, "556": {"title": "human motion analysis with deep metric learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_41", "abstract": "", "cite_num": -1}, "726": {"title": "linear span network for object skeleton detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_9", "abstract": "", "cite_num": -1}, "144": {"title": "dividing and aggregating network for multi-view action recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_28", "abstract": "", "cite_num": -1}, "111": {"title": "ppf-foldnet: unsupervised learning of rotation invariant 3d local descriptors.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_37", "abstract": "", "cite_num": -1}, "344": {"title": "cirl: controllable imitative reinforcement learning for vision-based self-driving.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_36", "abstract": "", "cite_num": -1}, "681": {"title": "self-produced guidance for weakly-supervised object localization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_37", "abstract": "", "cite_num": -1}, "17": {"title": "fine-grained visual categorization using meta-learning optimization with sample selection of auxiliary data.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_15", "abstract": "", "cite_num": -1}, "350": {"title": "women also snowboard: overcoming bias in captioning models.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_47", "abstract": "", "cite_num": -1}, "477": {"title": "semi-convolutional operators for instance segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_6", "abstract": "", "cite_num": -1}, "547": {"title": "skeleton-based action recognition with spatial reasoning and temporal stack learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_7", "abstract": "", "cite_num": -1}, "82": {"title": "dist-gan: an improved gan using distance constraints.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_23", "abstract": "", "cite_num": -1}, "501": {"title": "unsupervised image-to-image translation with stacked cycle-consistent adversarial networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_12", "abstract": "", "cite_num": -1}, "309": {"title": "generative domain-migration hashing for sketch-to-image retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_19", "abstract": "", "cite_num": -1}, "209": {"title": "weakly- and semi-supervised panoptic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_7", "abstract": "", "cite_num": -1}, "729": {"title": "partial adversarial domain adaptation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_9", "abstract": "", "cite_num": -1}, "319": {"title": "switchable temporal propagation network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_6", "abstract": "", "cite_num": -1}, "728": {"title": "ps-fcn: a flexible learning framework for photometric stereo.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_1", "abstract": "", "cite_num": -1}, "605": {"title": "efficient uncertainty estimation for semantic segmentation in videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_32", "abstract": "", "cite_num": -1}, "202": {"title": "multi-scale structure-aware network for human pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_44", "abstract": "", "cite_num": -1}, "673": {"title": "action anticipation with rbf kernelized feature mapping rnn.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_19", "abstract": "", "cite_num": -1}, "398": {"title": "joint optimization for compressive video sensing and reconstruction under hardware constraints.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_39", "abstract": "", "cite_num": -1}, "453": {"title": "good line cutting: towards accurate pose tracking of line-assisted vo/vslam.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_32", "abstract": "", "cite_num": -1}, "539": {"title": "recurrent tubelet proposal and recognition networks for action detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_19", "abstract": "", "cite_num": -1}, "280": {"title": "textsnake: a flexible representation for detecting text of arbitrary shapes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_2", "abstract": "", "cite_num": -1}, "178": {"title": "deepvs: a deep learning based video saliency prediction approach.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_37", "abstract": "", "cite_num": -1}, "283": {"title": "retrospective encoders for video summarization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_24", "abstract": "", "cite_num": -1}, "592": {"title": "semi-supervised generative adversarial hashing for image retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_29", "abstract": "", "cite_num": -1}, "585": {"title": "deep shape matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_46", "abstract": "", "cite_num": -1}, "424": {"title": "ddrnet: depth map denoising and refinement for consumer depth cameras using cascaded cnns.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_10", "abstract": "", "cite_num": -1}, "48": {"title": "less is more: picking informative frames for video captioning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_22", "abstract": "", "cite_num": -1}, "320": {"title": "attribute-guided face generation using conditional cyclegan.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_18", "abstract": "", "cite_num": -1}, "721": {"title": "motion feature network: fixed motion filter for action recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_24", "abstract": "", "cite_num": -1}, "203": {"title": "depth estimation via affinity learned with convolutional spatial propagation network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_7", "abstract": "", "cite_num": -1}, "423": {"title": "fast, accurate, and lightweight super-resolution with cascading residual network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_16", "abstract": "", "cite_num": -1}, "389": {"title": "piggyback: adapting a single network to multiple tasks by learning to mask weights.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_5", "abstract": "", "cite_num": -1}, "223": {"title": "crossnet: an end-to-end reference-based super resolution network using cross-scale warping.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_6", "abstract": "", "cite_num": -1}, "609": {"title": "deepphys: video-based physiological measurement using convolutional attention networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_22", "abstract": "Non-contact video-based physiological measurement has many applications in health care and human-com\nputer interaction. Practical applications require measurements to be accurate even in the presence o\nf large head rotations. We propose the first end-to-end system for video-based measurement of heart \nand breathing rate using a deep convolutional network. The system features a new motion representati\non based on a skin reflection model and a new attention mechanism using appearance information to gu\nide motion estimation, both of which enable robust measurement under heterogeneous lighting and majo\nr motions. Our approach significantly outperforms all current state-of-the-art methods on both RGB a\nnd infrared video datasets. Furthermore, it allows spatial-temporal distributions of physiological s\nignals to be visualized via the attention mechanism.", "cite_num": -1}, "53": {"title": "learning blind video temporal consistency.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_11", "abstract": "", "cite_num": -1}, "493": {"title": "lapran: a scalable laplacian pyramid reconstructive adversarial network for flexible compressive sensing reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_30", "abstract": "", "cite_num": -1}, "216": {"title": "transferable adversarial perturbations.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_28", "abstract": "", "cite_num": -1}, "472": {"title": "learn-to-score: efficient 3d scene exploration by predicting view utility.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_27", "abstract": "", "cite_num": -1}, "139": {"title": "deep structure inference network for facial action unit recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_19", "abstract": "", "cite_num": -1}, "443": {"title": "learning class prototypes via structure alignment for zero-shot recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_8", "abstract": "", "cite_num": -1}, "301": {"title": "robust fitting in computer vision: easy or hard?", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_43", "abstract": "", "cite_num": -1}, "748": {"title": "deep multi-task learning to recognise subtle facial expressions of mental states.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_7", "abstract": "", "cite_num": -1}, "445": {"title": "contour knowledge transfer for salient object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_22", "abstract": "", "cite_num": -1}, "43": {"title": "efficient sliding window computation for nn-based template matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_25", "abstract": "", "cite_num": -1}, "757": {"title": "psdf fusion: probabilistic signed distance function for on-the-fly 3d data fusion and scene reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_43", "abstract": "", "cite_num": -1}, "46": {"title": "compressing the input for cnns with the first-order scattering transform.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_19", "abstract": "", "cite_num": -1}, "399": {"title": "unsupervised person re-identification by deep learning tracklet association.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_45", "abstract": "", "cite_num": -1}, "427": {"title": "self-calibration of cameras with euclidean image plane in case of two views and known relative rotation angle.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_26", "abstract": "", "cite_num": -1}, "0": {"title": "using object information for spotting text.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_33", "abstract": "", "cite_num": -1}, "200": {"title": "seeing deeply and bidirectionally: a deep learning approach for single image reflection removal.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_40", "abstract": "", "cite_num": -1}, "421": {"title": "reconstruction-based pairwise depth dataset for depth image enhancement using cnn.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_26", "abstract": "", "cite_num": -1}, "305": {"title": "product quantization network for fast image retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_12", "abstract": "", "cite_num": -1}, "447": {"title": "learning 3d human pose from structure and motion.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_41", "abstract": "", "cite_num": -1}, "437": {"title": "deep clustering for unsupervised learning of visual features.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_9", "abstract": "", "cite_num": -1}, "454": {"title": "unsupervised learning of multi-frame optical flow with occlusions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_42", "abstract": "", "cite_num": -1}, "329": {"title": "find and focus: retrieve and localize video events with natural language queries.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_13", "abstract": "", "cite_num": -1}, "446": {"title": "penalizing top performers: conservative loss for semantic segmentation adaptation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_35", "abstract": "", "cite_num": -1}, "276": {"title": "learning to forecast and refine residual motion for image-to-video generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_24", "abstract": "", "cite_num": -1}, "314": {"title": "pairwise confusion for fine-grained visual classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_5", "abstract": "", "cite_num": -1}, "581": {"title": "highly-economized multi-view binary compression for scalable image clustering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_44", "abstract": "", "cite_num": -1}, "451": {"title": "visual question answering as a meta learning task.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_14", "abstract": "", "cite_num": -1}, "483": {"title": "recycle-gan: unsupervised video retargeting.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_8", "abstract": "", "cite_num": -1}, "324": {"title": "layer-structured 3d scene inference via view synthesis.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_19", "abstract": "", "cite_num": -1}, "572": {"title": "skipnet: learning dynamic routing in convolutional networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_25", "abstract": "", "cite_num": -1}, "754": {"title": "shuffle-then-assemble: learning object-agnostic visual relationship features.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_3", "abstract": "", "cite_num": -1}, "359": {"title": "attention-based ensemble for deep metric learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_45", "abstract": "Deep metric learning aims to learn an embedding function, modeled as deep neural network. This embed\nding function usually puts semantically similar images close while dissimilar images far from each o\nther in the learned embedding space. Recently, ensemble has been applied to deep metric learning to \nyield state-of-the-art results. As one important aspect of ensemble, the learners should be diverse \nin their feature embeddings. To this end, we propose an attention-based ensemble, which uses multipl\ne attention masks, so that each learner can attend to different parts of the object. We also propose\n a divergence loss, which encourages diversity among the learners. The proposed method is applied to\n the standard benchmarks of deep metric learning and experimental results show that it outperforms t\nhe state-of-the-art methods by a significant margin on image retrieval tasks.", "cite_num": 14}, "278": {"title": "learning to detect and track visible and occluded body joints in a virtual world.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_27", "abstract": "", "cite_num": -1}, "372": {"title": "wilddash - creating hazard-aware benchmarks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_25", "abstract": "", "cite_num": -1}, "449": {"title": "contextual-based image inpainting: infer, match, and translate.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_1", "abstract": "", "cite_num": -1}, "108": {"title": "audio-visual event localization in unconstrained videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_16", "abstract": "", "cite_num": -1}, "766": {"title": "generating 3d faces using convolutional mesh autoencoders.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_43", "abstract": "", "cite_num": -1}, "405": {"title": "towards privacy-preserving visual recognition via adversarial training: a pilot study.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_37", "abstract": "", "cite_num": -1}, "211": {"title": "unveiling the power of deep tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_30", "abstract": "", "cite_num": -1}, "750": {"title": "semantic match consistency for long-term visual localization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_24", "abstract": "", "cite_num": -1}, "418": {"title": "hidden: hiding data with deep networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_40", "abstract": "", "cite_num": -1}, "375": {"title": "starmap for category-agnostic keypoint and viewpoint estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_20", "abstract": "", "cite_num": -1}, "212": {"title": "factorizable net: an efficient subgraph-based framework for scene graph generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_21", "abstract": "", "cite_num": -1}, "275": {"title": "self-supervised knowledge distillation using singular value decomposition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_21", "abstract": "", "cite_num": -1}, "240": {"title": "learning region features for object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_24", "abstract": "", "cite_num": -1}, "413": {"title": "task-aware image downscaling.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_25", "abstract": "", "cite_num": -1}, "517": {"title": "specular-to-diffuse translation for multi-view reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_12", "abstract": "", "cite_num": -1}, "131": {"title": "youtube-vos: sequence-to-sequence video object segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_36", "abstract": "", "cite_num": -1}, "349": {"title": "learning type-aware embeddings for fashion compatibility.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_24", "abstract": "", "cite_num": -1}, "723": {"title": "license plate detection and recognition in unconstrained scenarios.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_36", "abstract": "", "cite_num": -1}, "1": {"title": "viewpoint estimation - insights and model.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_16", "abstract": "", "cite_num": -1}, "519": {"title": "modeling varying camera-imu time offset in optimization-based visual-inertial odometry.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_30", "abstract": "", "cite_num": -1}, "610": {"title": "trackingnet: a large-scale dataset and benchmark for object tracking in the wild.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_19", "abstract": "", "cite_num": -1}, "709": {"title": "learning to separate object sounds by watching unlabeled video.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_3", "abstract": "", "cite_num": -1}, "380": {"title": "videomatch: matching based video object segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_4", "abstract": "", "cite_num": -1}, "487": {"title": "model-free consensus maximization for non-rigid shapes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_8", "abstract": "", "cite_num": -1}, "96": {"title": "analyzing clothing layer deformation statistics of 3d human motions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_15", "abstract": "", "cite_num": -1}, "260": {"title": "volumetric performance capture from minimal camera viewpoints.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_35", "abstract": "", "cite_num": -1}, "133": {"title": "acquisition of localization confidence for accurate object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_48", "abstract": "", "cite_num": -1}, "528": {"title": "a minimal closed-form solution for multi-perspective pose estimation using points and lines.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_29", "abstract": "", "cite_num": -1}, "387": {"title": "online detection of action start in untrimmed, streaming videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_33", "abstract": "", "cite_num": -1}, "97": {"title": "point-to-point regression pointnet for 3d hand pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_29", "abstract": "", "cite_num": -1}, "226": {"title": "exfuse: enhancing feature fusion for semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_17", "abstract": "", "cite_num": -1}, "352": {"title": "u-pc: unsupervised planogram compliance.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_36", "abstract": "", "cite_num": -1}, "162": {"title": "dynamic task prioritization for multitask learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_17", "abstract": "", "cite_num": -1}, "68": {"title": "vqa-e: explaining, elaborating, and enhancing your answers for visual questions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_34", "abstract": "", "cite_num": -1}, "515": {"title": "stereonet: guided hierarchical refinement for real-time edge-aware depth prediction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_35", "abstract": "", "cite_num": -1}, "106": {"title": "stagnet: an attentive semantic rnn for group activity recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_7", "abstract": "", "cite_num": -1}, "38": {"title": "urban zoning using higher-order markov random fields on multi-view imagery data.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_38", "abstract": "", "cite_num": -1}, "707": {"title": "deep image demosaicking using a cascade of convolutional residual denoising networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_19", "abstract": "", "cite_num": -1}, "130": {"title": "bi-box regression for pedestrian detection and occlusion estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_9", "abstract": "", "cite_num": -1}, "274": {"title": "lsq++: lower running time and higher recall in multi-codebook quantization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_30", "abstract": "", "cite_num": -1}, "536": {"title": "learning and matching multi-view descriptors for registration of point clouds.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_31", "abstract": "", "cite_num": -1}, "628": {"title": "connecting gaze, scene, and attention: generalized attention estimation via joint modeling of gaze and scene saliency.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_24", "abstract": "This paper addresses the challenging problem of estimating the general visual attention of people in\n images. Our proposed method is designed to work across multiple naturalistic social scenarios and p\nrovides a full picture of the subject\u2019s attention and gaze. In contrast, earlier works on gaze and a\nttention estimation have focused on constrained problems in more specific contexts. In particular, o\nur model explicitly represents the gaze direction and handles out-of-frame gaze targets. We leverage\n three different datasets using a multi-task learning approach. We evaluate our method on widely use\nd benchmarks for single-tasks such as gaze angle estimation and attention-within-an-image, as well a\ns on the new challenging task of generalized visual attention prediction. In addition, we have creat\ned extended annotations for the MMDB and GazeFollow datasets which are used in our experiments, whic\nh we will publicly release.", "cite_num": -1}, "28": {"title": "interpretable intuitive physics model.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_6", "abstract": "", "cite_num": -1}, "317": {"title": "instance-level human parsing via part grouping network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_47", "abstract": "", "cite_num": -1}, "747": {"title": "reinforced temporal attention and split-rate transfer for depth-based person re-identification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_44", "abstract": "We address the problem of person re-identification from commodity depth sensors. One challenge for d\nepth-based recognition is data scarcity. Our first contribution addresses this problem by introducin\ng split-rate RGB-to-Depth transfer, which leverages large RGB datasets more effectively than popular\n fine-tuning approaches. Our transfer scheme is based on the observation that the model parameters a\nt the bottom layers of a deep convolutional neural network can be directly shared between RGB and de\npth data while the remaining layers need to be fine-tuned rapidly. Our second contribution enhances \nre-identification for video by implementing temporal attention as a Bernoulli-Sigmoid unit acting up\non frame-level features. Since this unit is stochastic, the temporal attention parameters are traine\nd using reinforcement learning. Extensive experiments validate the accuracy of our method in person \nre-identification from depth sequences. Finally, in a scenario where subjects wear unseen clothes, w\ne show large performance gains compared to a state-of-the-art model which relies on RGB data.", "cite_num": -1}, "146": {"title": "objects that sound.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_27", "abstract": "", "cite_num": -1}, "635": {"title": "super-identity convolutional neural network for face hallucination.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_12", "abstract": "", "cite_num": -1}, "373": {"title": "faces as lighting probes via unsupervised deep highlight extraction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_20", "abstract": "", "cite_num": -1}, "123": {"title": "show, tell and discriminate: image captioning by self-retrieval with partially labeled data.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_21", "abstract": "", "cite_num": -1}, "327": {"title": "a+d net: training a shadow detector with adversarial shadow attenuation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_41", "abstract": "", "cite_num": -1}, "355": {"title": "towards human-level license plate recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_19", "abstract": "", "cite_num": -1}, "568": {"title": "a scalable exemplar-based subspace clustering algorithm for class-imbalanced data.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_5", "abstract": "", "cite_num": -1}, "112": {"title": "compositing-aware image search.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_31", "abstract": "", "cite_num": -1}, "298": {"title": "geodesc: learning local descriptors by integrating geometry constraints.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_11", "abstract": "", "cite_num": -1}, "591": {"title": "ctap: complementary temporal action proposal generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_5", "abstract": "", "cite_num": -1}, "60": {"title": "where will they go? predicting fine-grained adversarial multi-agent motion using conditional variational autoencoders.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_45", "abstract": "", "cite_num": -1}, "407": {"title": "recovering 3d planes from a single image via convolutional neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_6", "abstract": "", "cite_num": -1}, "444": {"title": "learning-based video motion magnification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_39", "abstract": "", "cite_num": -1}, "532": {"title": "diverse and coherent paragraph generation from images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_45", "abstract": "", "cite_num": -1}, "526": {"title": "hierarchy of alternating specialists for scene recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_28", "abstract": "", "cite_num": -1}, "655": {"title": "video compression through image interpolation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_26", "abstract": "", "cite_num": -1}, "392": {"title": "meta-tracker: fast and robust online adaptation for visual object trackers.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_35", "abstract": "", "cite_num": -1}, "626": {"title": "lifting layers: analysis and applications.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_4", "abstract": "", "cite_num": -1}, "667": {"title": "cross-modal hamming hashing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_13", "abstract": "", "cite_num": -1}, "763": {"title": "unified perceptual parsing for scene understanding.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_26", "abstract": "", "cite_num": -1}, "99": {"title": "joint map and symmetry synchronization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_16", "abstract": "", "cite_num": -1}, "438": {"title": "real-time hair rendering using sequential adversarial networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_7", "abstract": "", "cite_num": -1}, "192": {"title": "deep feature pyramid reconfiguration for object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_11", "abstract": "", "cite_num": -1}, "662": {"title": "weakly supervised region proposal network and object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_22", "abstract": "", "cite_num": -1}, "254": {"title": "compound memory networks for few-shot video classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_46", "abstract": "", "cite_num": -1}, "293": {"title": "from face recognition to models of identity: a bayesian approach to learning about unknown identities from unsupervised data.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_46", "abstract": "", "cite_num": -1}, "727": {"title": "correcting the triplet selection bias for triplet loss.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_5", "abstract": "", "cite_num": -1}, "676": {"title": "coded illumination and imaging for fluorescence based classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_31", "abstract": "", "cite_num": -1}, "257": {"title": "exploring visual relationship for image captioning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_42", "abstract": "", "cite_num": -1}, "391": {"title": "selfie video stabilization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_34", "abstract": "", "cite_num": -1}, "224": {"title": "segstereo: exploiting semantic information for disparity estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_39", "abstract": "", "cite_num": -1}, "596": {"title": "simple baselines for human pose estimation and tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_29", "abstract": "", "cite_num": -1}, "394": {"title": "does haze removal help cnn-based image classification?", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_42", "abstract": "", "cite_num": -1}, "215": {"title": "multi-scale spatially-asymmetric recalibration for image classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_31", "abstract": "", "cite_num": -1}, "335": {"title": "scale aggregation network for accurate and efficient crowd counting.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_45", "abstract": "", "cite_num": -1}, "152": {"title": "linear rgb-d slam for planar environments.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_21", "abstract": "", "cite_num": -1}, "621": {"title": "cross-modal ranking with soft consistency and noisy labels for robust rgb-t tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_49", "abstract": "", "cite_num": -1}, "699": {"title": "seeing tree structure from vibration.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_46", "abstract": "", "cite_num": -1}, "66": {"title": "large scale urban scene modeling from mvs meshes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_38", "abstract": "", "cite_num": -1}, "2": {"title": "revisiting rcnn: on awakening the classification power of faster rcnn.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_28", "abstract": "", "cite_num": -1}, "571": {"title": "bsn: boundary sensitive network for temporal action proposal generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_1", "abstract": "", "cite_num": -1}, "735": {"title": "3d face reconstruction from light field images: a model-free approach.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_31", "abstract": "", "cite_num": -1}, "198": {"title": "data-driven sparse structure selection for deep neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_19", "abstract": "", "cite_num": -1}, "40": {"title": "sparsely aggregated convolutional networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_12", "abstract": "", "cite_num": -1}, "459": {"title": "facial expression recognition with inconsistently annotated datasets.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_14", "abstract": "", "cite_num": -1}, "606": {"title": "beyond local reasoning for stereo confidence estimation with deep learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_20", "abstract": "", "cite_num": -1}, "5": {"title": "part-aligned bilinear representations for person re-identification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_25", "abstract": "", "cite_num": -1}, "145": {"title": "a zero-shot framework for sketch based image retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_19", "abstract": "", "cite_num": -1}, "756": {"title": "pyramidbox: a context-assisted single shot face detector.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_49", "abstract": "", "cite_num": -1}, "645": {"title": "image reassembly combining deep learning and shortest path problem.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_10", "abstract": "", "cite_num": -1}, "406": {"title": "towards realistic predictors.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_3", "abstract": "", "cite_num": -1}, "206": {"title": "multimodal dual attention memory for video story question answering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_41", "abstract": "We propose a video story question-answering (QA) architecture, Multimodal Dual Attention Memory (MDA\nM). The key idea is to use a dual attention mechanism with late fusion. MDAM uses self-attention to \nlearn the latent concepts in scene frames and captions. Given a question, MDAM uses the second atten\ntion over these latent concepts. Multimodal fusion is performed after the dual attention processes (\nlate fusion). Using this processing pipeline, MDAM learns to infer a high-level vision-language join\nt representation from an abstraction of the full video content. We evaluate MDAM on PororoQA and Mov\nieQA datasets which have large-scale QA annotations on cartoon videos and movies, respectively. For \nboth datasets, MDAM achieves new state-of-the-art results with significant margins compared to the r\nunner-up models. We confirm the best performance of the dual attention mechanism combined with late \nfusion by ablation studies. We also perform qualitative analysis by visualizing the inference mechan\nisms of MDAM.", "cite_num": 6}, "11": {"title": "improving dnn robustness to adversarial attacks using jacobian regularization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_32", "abstract": "Deep neural networks have lately shown tremendous performance in various applications including visi\non and speech processing tasks. However, alongside their ability to perform these tasks with such hi\ngh accuracy, it has been shown that they are highly susceptible to adversarial attacks: a small chan\nge of the input would cause the network to err with high confidence. This phenomenon exposes an inhe\nrent fault in these networks and their ability to generalize well. For this reason, providing robust\nness to adversarial attacks is an important challenge in networks training, which has led to an exte\nnsive research. In this work, we suggest a theoretically inspired novel approach to improve the netw\norks' robustness. Our method applies regularization using the Frobenius norm of the Jacobian of the \nnetwork, which is applied as post-processing, after regular training has finished. We demonstrate em\npirically that it leads to enhanced robustness results with a minimal change in the original network\n's accuracy.", "cite_num": 0}, "250": {"title": "recurrent squeeze-and-excitation context aggregation net for single image deraining.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_16", "abstract": "", "cite_num": -1}, "73": {"title": "quaternion convolutional neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_39", "abstract": "", "cite_num": -1}, "232": {"title": "wasserstein divergence for gans.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_40", "abstract": "", "cite_num": -1}, "659": {"title": "pairwise relational networks for face recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_39", "abstract": "", "cite_num": -1}, "64": {"title": "leveraging motion priors in videos for improving human segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_14", "abstract": "", "cite_num": -1}, "618": {"title": "attributes as operators: factorizing unseen attribute-object compositions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_11", "abstract": "", "cite_num": -1}, "650": {"title": "adaptive affinity fields for semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_36", "abstract": "", "cite_num": -1}, "20": {"title": "deep metric learning with hierarchical triplet loss.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_17", "abstract": "", "cite_num": -1}, "159": {"title": "rethinking spatiotemporal feature learning: speed-accuracy trade-offs in video classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_19", "abstract": "", "cite_num": -1}, "58": {"title": "a trilateral weighted sparse coding scheme for real-world image denoising.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_2", "abstract": "", "cite_num": -1}, "341": {"title": "visual tracking via spatially aligned correlation filters network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_29", "abstract": "", "cite_num": -1}, "251": {"title": "unsupervised cnn-based co-saliency detection with graphical optimization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_30", "abstract": "", "cite_num": -1}, "622": {"title": "x2face: a network for controlling face generation using images, audio, and pose codes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_41", "abstract": "", "cite_num": -1}, "306": {"title": "macro-micro adversarial network for human parsing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_26", "abstract": "", "cite_num": -1}, "330": {"title": "deep generative models for weakly-supervised multi-label classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_25", "abstract": "", "cite_num": -1}, "87": {"title": "collaborative deep reinforcement learning for multi-object tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_36", "abstract": "", "cite_num": -1}, "531": {"title": "egocentric activity prediction via event modulated attention.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_13", "abstract": "Predicting future activities from an egocentric viewpoint is of particular interest in assisted livi\nng. However, state-of-the-art egocentric activity understanding techniques are mostly NOT capable of\n predictive tasks, as their synchronous processing architecture performs poorly in either modeling e\nvent dependency or pruning temporal redundant features. This work explicitly addresses these issues \nby proposing an asynchronous gaze-event driven attentive activity prediction network. This network i\ns built on a gaze-event extraction module inspired by the fact that gaze moving in/out of a certain \nobject most probably indicates the occurrence/ending of a certain activity. The extracted gaze event\ns are input to: (1) an asynchronous module which reasons about the temporal dependency between event\ns and (2) a synchronous module which softly attends to informative temporal durations for more compa\nct and discriminative feature extraction. Both modules are seamlessly integrated for collaborative p\nrediction. Extensive experimental results on egocentric activity prediction as well as recognition w\nell demonstrate the effectiveness of the proposed method.", "cite_num": 3}, "422": {"title": "auggan: cross domain adaptation with gan-based data augmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_44", "abstract": "", "cite_num": -1}, "719": {"title": "improving shape deformation in unsupervised image-to-image translation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_40", "abstract": "", "cite_num": -1}, "589": {"title": "cgintrinsics: better intrinsic image decomposition through physically-based rendering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_23", "abstract": "", "cite_num": -1}, "665": {"title": "view-graph selection framework for sfm.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_33", "abstract": "", "cite_num": -1}, "535": {"title": "explaingan: model explanation via decision boundary crossing transformations.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_41", "abstract": "", "cite_num": -1}, "507": {"title": "self-supervised relative depth learning for urban scene understanding.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_2", "abstract": "", "cite_num": -1}, "761": {"title": "scaling egocentric vision: the dataset.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_44", "abstract": "", "cite_num": -1}, "238": {"title": "person re-identification with deep similarity-guided graph neural network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_30", "abstract": "", "cite_num": -1}, "117": {"title": "deep burst denoising.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_33", "abstract": "", "cite_num": -1}, "502": {"title": "deep kalman filtering network for video compression artifact reduction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_35", "abstract": "", "cite_num": -1}, "271": {"title": "focus, segment and erase: an efficient network for multi-label brain tumor segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_40", "abstract": "", "cite_num": -1}, "541": {"title": "learning shape priors for single-view 3d completion and reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_40", "abstract": "", "cite_num": -1}, "195": {"title": "orthogonal deep features decomposition for age-invariant face recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_45", "abstract": "", "cite_num": -1}, "741": {"title": "multi-view to novel view: synthesizing novel views with self-learned confidence.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_10", "abstract": "", "cite_num": -1}, "683": {"title": "deep autoencoder for combined human pose estimation and body model upscaling.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_48", "abstract": "", "cite_num": -1}, "657": {"title": "a deeply-initialized coarse-to-fine ensemble of regression trees for face alignment.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_36", "abstract": "", "cite_num": -1}, "337": {"title": "joint task-recursive learning for semantic segmentation and depth estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_15", "abstract": "", "cite_num": -1}, "410": {"title": "online dictionary learning for approximate archetypal analysis.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_30", "abstract": "", "cite_num": -1}, "488": {"title": "appearance-based gaze estimation via evaluation-guided asymmetric regression.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_7", "abstract": "", "cite_num": -1}, "401": {"title": "personlab: person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_17", "abstract": "", "cite_num": -1}, "561": {"title": "fully motion-aware network for video object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_33", "abstract": "", "cite_num": -1}, "425": {"title": "exploiting vector fields for geometric rectification of distorted document images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_11", "abstract": "", "cite_num": -1}, "506": {"title": "joint and progressive learning from high-dimensional data for multi-label classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_29", "abstract": "", "cite_num": -1}, "259": {"title": "accurate scene text detection through border semantics awareness and bootstrapping.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_22", "abstract": "", "cite_num": -1}, "573": {"title": "ridi: robust imu double integration.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_38", "abstract": "", "cite_num": -1}, "368": {"title": "pose partition networks for multi-person pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_42", "abstract": "", "cite_num": -1}, "34": {"title": "incremental non-rigid structure-from-motion with unknown focal length.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_46", "abstract": "", "cite_num": -1}, "578": {"title": "how good is my gan?", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_14", "abstract": "", "cite_num": -1}, "217": {"title": "lambda twist: an accurate fast robust perspective three point (p3p) solver.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_20", "abstract": "", "cite_num": -1}, "594": {"title": "snap angle prediction for 360 \u2218 panoramas.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_1", "abstract": "", "cite_num": -1}, "523": {"title": "learning deep representations with probabilistic knowledge transfer.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_17", "abstract": "", "cite_num": -1}, "505": {"title": "making deep heatmaps robust to partial occlusions for 3d object pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_8", "abstract": "", "cite_num": -1}, "140": {"title": "dynamic multimodal instance segmentation guided by natural language queries.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_39", "abstract": "We address the problem of segmenting an object given a natural language expression that describes it\n. Current techniques tackle this task by either (i) directly or recursively merging linguistic and v\nisual information in the channel dimension and then performing convolutions; or by (ii) mapping the \nexpression to a space in which it can be thought of as a filter, whose response is directly related \nto the presence of the object at a given spatial coordinate in the image, so that a convolution can \nbe applied to look for the object. We propose a novel method that integrates these two insights in o\nrder to fully exploit the recursive nature of language. Additionally, during the upsampling process,\n we take advantage of the intermediate information generated when downsampling the image, so that de\ntailed segmentations can be obtained. We compare our method against the state-of-the-art approaches \nin four standard datasets, in which it surpasses all previous methods in six of eight of the splits \nfor this task.", "cite_num": 3}, "182": {"title": "teaching machines to understand baseball games: large-scale baseball video database for multiple video understanding tasks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_25", "abstract": "", "cite_num": -1}, "484": {"title": "rolling shutter pose and ego-motion estimation using shape-from-template.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_29", "abstract": "", "cite_num": -1}, "679": {"title": "semantically aware urban 3d reconstruction with plane-based regularization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_29", "abstract": "", "cite_num": -1}, "383": {"title": "deep regression tracking with shrinkage loss.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_22", "abstract": "", "cite_num": -1}, "773": {"title": "improving spatiotemporal self-supervision by deep reinforcement learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_47", "abstract": "", "cite_num": -1}, "261": {"title": "on regularized losses for weakly-supervised cnn segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_31", "abstract": "", "cite_num": -1}, "533": {"title": "convnets and imagenet beyond accuracy: understanding mistakes and uncovering biases.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_31", "abstract": "", "cite_num": -1}, "567": {"title": "evaluating capability of deep neural networks for image classification via information plane.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_11", "abstract": "", "cite_num": -1}, "702": {"title": "encoder-decoder with atrous separable convolution for semantic image segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_49", "abstract": "", "cite_num": -1}, "142": {"title": "action search: spotting actions in videos and its application to temporal action localization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_16", "abstract": "", "cite_num": -1}, "362": {"title": "practical black-box attacks on deep neural networks using efficient query mechanisms.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_10", "abstract": "", "cite_num": -1}, "555": {"title": "folded recurrent neural networks for future video prediction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_44", "abstract": "", "cite_num": -1}, "557": {"title": "saliency preservation in low-resolution grayscale images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_15", "abstract": "", "cite_num": -1}, "287": {"title": "end-to-end joint semantic segmentation of actors and actions in video.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_43", "abstract": "", "cite_num": -1}, "705": {"title": "visual text correction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_10", "abstract": "", "cite_num": -1}, "710": {"title": "reverse attention for salient object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_15", "abstract": "Benefit from the quick development of deep learning techniques, salient object detection has achieve\nd remarkable progresses recently. However, there still exists following two major challenges that hi\nnder its application in embedded devices, low resolution output and heavy model weight. To this end,\n this paper presents an accurate yet compact deep network for efficient salient object detection. Mo\nre specifically, given a coarse saliency prediction in the deepest layer, we first employ residual l\nearning to learn side-output residual features for saliency refinement, which can be achieved with v\nery limited convolutional parameters while keep accuracy. Secondly, we further propose reverse atten\ntion to guide such side-output residual learning in a top-down manner. By erasing the current predic\nted salient regions from side-output features, the network can eventually explore the missing object\n parts and details which results in high resolution and accuracy. Experiments on six benchmark datas\nets demonstrate that the proposed approach compares favorably against state-of-the-art methods, and \nwith advantages in terms of simplicity, efficiency (45 FPS) and model size (81 MB).", "cite_num": 12}, "72": {"title": "on offline evaluation of vision-based driving models.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_15", "abstract": "", "cite_num": -1}, "545": {"title": "coded two-bucket cameras for computer vision.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_4", "abstract": "", "cite_num": -1}, "163": {"title": "choose your neuron: incorporating domain knowledge through neuron-importance.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_32", "abstract": "", "cite_num": -1}, "608": {"title": "summarizing first-person videos from third persons' points of views.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_5", "abstract": "", "cite_num": -1}, "101": {"title": "remote photoplethysmography correspondence feature for 3d mask face presentation attack detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_34", "abstract": "", "cite_num": -1}, "119": {"title": "multi-object tracking with neural gating using bilinear lstm.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_13", "abstract": "", "cite_num": -1}, "236": {"title": "interaction-aware spatio-temporal pyramid attention networks for action classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_23", "abstract": "Local features at neighboring spatial positions in feature maps have high correlation since their re\nceptive fields are often overlapped. Self-attention usually uses the weighted sum (or other function\ns) with internal elements of each local feature to obtain its weight score, which ignores interactio\nns among local features. To address this, we propose an effective interaction-aware self-attention m\nodel inspired by PCA to learn attention maps. Furthermore, since different layers in a deep network \ncapture feature maps of different scales, we use these feature maps to construct a spatial pyramid a\nnd then utilize multi-scale information to obtain more accurate attention scores, which are used to \nweight the local features in all spatial positions of feature maps to calculate attention maps. More\nover, our spatial pyramid attention is unrestricted to the number of its input feature maps so it is\n easily extended to a spatio-temporal version. Finally, our model is embedded in general CNNs to for\nm end-to-end attention networks for action classification. Experimental results show that our method\n achieves the state-of-the-art results on the UCF101, HMDB51 and untrimmed Charades.", "cite_num": 2}, "771": {"title": "deep domain generalization via conditional invariant adversarial networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_38", "abstract": "", "cite_num": -1}, "758": {"title": "multi-modal cycle-consistent generalized zero-shot learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_2", "abstract": "In generalized zero shot learning (GZSL), the set of classes are split into seen and unseen classes,\n where training relies on the semantic features of the seen and unseen classes and the visual repres\nentations of only the seen classes, while testing uses the visual representations of the seen and un\nseen classes. Current methods address GZSL by learning a transformation from the visual to the seman\ntic space, exploring the assumption that the distribution of classes in the semantic and visual spac\nes is relatively similar. Such methods tend to transform unseen testing visual representations into \none of the seen classes\u2019 semantic features instead of the semantic features of the correct unseen cl\nass, resulting in low accuracy GZSL classification. Recently, generative adversarial networks (GAN) \nhave been explored to synthesize visual representations of the unseen classes from their semantic fe\natures - the synthesized representations of the seen and unseen classes are then used to train the G\nZSL classifier. This approach has been shown to boost GZSL classification accuracy, but there is one\n important missing constraint: there is no guarantee that synthetic visual representations can gener\nate back their semantic feature in a multi-modal cycle-consistent manner. This missing constraint ca\nn result in synthetic visual representations that do not represent well their semantic features, whi\nch means that the use of this constraint can improve GAN-based approaches. In this paper, we propose\n the use of such constraint based on a new regularization for the GAN training that forces the gener\nated visual features to reconstruct their original semantic features. Once our model is trained with\n this multi-modal cycle-consistent semantic compatibility, we can then synthesize more representativ\ne visual representations for the seen and, more importantly, for the unseen classes. Our proposed ap\nproach shows the best GZSL classification results in the field in several publicly available dataset\ns.", "cite_num": 7}, "98": {"title": "joint 3d face reconstruction and dense alignment with position map regression network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_33", "abstract": "", "cite_num": -1}, "550": {"title": "planematch: patch coplanarity prediction for robust rgb-d reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_46", "abstract": "", "cite_num": -1}, "90": {"title": "in the eye of beholder: joint learning of gaze and actions in first person video.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_38", "abstract": "", "cite_num": -1}, "524": {"title": "srfeat: single image super-resolution with feature discrimination.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_27", "abstract": "", "cite_num": -1}, "74": {"title": "learning compression from limited unlabeled data.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_46", "abstract": "", "cite_num": -1}, "762": {"title": "compositional learning for human object interaction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_15", "abstract": "", "cite_num": -1}, "598": {"title": "unsupervised video object segmentation with motion-based bilateral networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_13", "abstract": "", "cite_num": -1}, "404": {"title": "t ^2 2 net: synthetic-to-realistic translation for solving single-image depth estimation tasks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_47", "abstract": "", "cite_num": -1}, "365": {"title": "deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_25", "abstract": "In this paper, we make two contributions to unsupervised domain adaptation (UDA) using the convoluti\nonal neural network (CNN). First, our approach transfers knowledge in all the convolutional layers t\nhrough attention alignment. Most previous methods align high-level representations, e.g., activation\ns of the fully connected (FC) layers. In these methods, however, the convolutional layers which unde\nrpin critical low-level domain knowledge cannot be updated directly towards reducing domain discrepa\nncy. Specifically, we assume that the discriminative regions in an image are relatively invariant to\n image style changes. Based on this assumption, we propose an attention alignment scheme on all the \ntarget convolutional layers to uncover the knowledge shared by the source domain. Second, we estimat\ne the posterior label distribution of the unlabeled data for target network training. Previous metho\nds, which iteratively update the pseudo labels by the target network and refine the target network b\ny the updated pseudo labels, are vulnerable to label estimation errors. Instead, our approach uses c\nategory distribution to calculate the cross-entropy loss for training, thereby ameliorating the erro\nr accumulation of the estimated labels. The two contributions allow our approach to outperform the s\ntate-of-the-art methods by +2.6% on the Office-31 dataset.", "cite_num": -1}, "644": {"title": "repeatability is not enough: learning affine regions via discriminability.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_18", "abstract": "", "cite_num": -1}, "243": {"title": "visual psychophysics for making face recognition algorithms more explainable.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_16", "abstract": "", "cite_num": -1}, "430": {"title": "exploiting temporal information for 3d human pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_5", "abstract": "", "cite_num": -1}, "544": {"title": "burst image deblurring using permutation invariant convolutional neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_45", "abstract": "", "cite_num": -1}, "639": {"title": "deep bilevel learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_38", "abstract": "", "cite_num": -1}, "514": {"title": "deep co-training for semi-supervised image recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_9", "abstract": "", "cite_num": -1}, "717": {"title": "bi-real net: enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_44", "abstract": "", "cite_num": -1}, "126": {"title": "out-of-distribution detection using an ensemble of self supervised leave-out classifiers.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_34", "abstract": "", "cite_num": -1}, "731": {"title": "a modulation module for multi-task learning with applications in image retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_25", "abstract": "", "cite_num": -1}, "322": {"title": "multi-attention multi-class constraint for fine-grained image recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_49", "abstract": "Attention-based learning for fine-grained image recognition remains a challenging task, where most o\nf the existing methods treat each object part in isolation, while neglecting the correlations among \nthem. In addition, the multi-stage or multi-scale mechanisms involved make the existing methods less\n efficient and hard to be trained end-to-end. In this paper, we propose a novel attention-based conv\nolutional neural network (CNN) which regulates multiple object parts among different input images. O\nur method first learns multiple attention region features of each input image through the one-squeez\ne multi-excitation (OSME) module, and then apply the multi-attention multi-class constraint (MAMC) i\nn a metric learning framework. For each anchor feature, the MAMC functions by pulling same-attention\n same-class features closer, while pushing different-attention or different-class features away. Our\n method can be easily trained end-to-end, and is highly efficient which requires only one training s\ntage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog species dataset that surpasses si\nmilar existing datasets by category coverage, data volume and annotation quality. Extensive experime\nnts are conducted to show the substantial improvements of our method on four benchmark datasets.", "cite_num": 7}, "210": {"title": "model adaptation with synthetic and real data for semantic dense foggy scene understanding.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_42", "abstract": "", "cite_num": -1}, "678": {"title": "grassmann pooling as compact homogeneous bilinear pooling for fine-grained visual classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_22", "abstract": "", "cite_num": -1}, "565": {"title": "learning human-object interactions by graph parsing neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_25", "abstract": "", "cite_num": -1}, "499": {"title": "deep discriminative model for video classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_24", "abstract": "", "cite_num": -1}, "462": {"title": "verisimilar image synthesis for accurate detection and recognition of texts in scenes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_16", "abstract": "", "cite_num": -1}, "703": {"title": "learning category-specific mesh reconstruction from image collections.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_23", "abstract": "", "cite_num": -1}, "269": {"title": "unsupervised domain adaptation for 3d keypoint estimation via view consistency.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_9", "abstract": "", "cite_num": -1}, "466": {"title": "shapecodes: self-supervised feature learning by lifting views to viewgrids.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_8", "abstract": "", "cite_num": -1}, "623": {"title": "sketchyscene: richly-annotated scene sketches.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_26", "abstract": "", "cite_num": -1}, "433": {"title": "single image highlight removal with a sparse and low-rank reflection model.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_17", "abstract": "", "cite_num": -1}, "666": {"title": "descending, lifting or smoothing: secrets of robust cost optimization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_34", "abstract": "", "cite_num": -1}, "643": {"title": "occlusion-aware hand pose estimation using hierarchical mixture density network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_49", "abstract": "", "cite_num": -1}, "273": {"title": "superpixel sampling networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_22", "abstract": "", "cite_num": -1}, "100": {"title": "visual reasoning with multi-hop feature modulation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_48", "abstract": "", "cite_num": -1}, "552": {"title": "deep imbalanced attribute classification using visual attention aggregation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_42", "abstract": "For many computer vision applications, such as image description and human identification, recognizi\nng the visual attributes of humans is an essential yet challenging problem. Its challenges originate\n from its multi-label nature, the large underlying class imbalance and the lack of spatial annotatio\nns. Existing methods follow either a computer vision approach while failing to account for class imb\nalance, or explore machine learning solutions, which disregard the spatial and semantic relations th\nat exist in the images. With that in mind, we propose an effective method that extracts and aggregat\nes visual attention masks at different scales. We introduce a loss function to handle class imbalanc\ne both at class and at an instance level and further demonstrate that penalizing attention masks wit\nh high prediction variance accounts for the weak supervision of the attention mechanism. By identify\ning and addressing these challenges, we achieve state-of-the-art results with a simple attention mec\nhanism in both PETA and WIDER-Attribute datasets without additional context or side information.", "cite_num": 9}, "177": {"title": "liquid pouring monitoring via rich sensory inputs.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_21", "abstract": "", "cite_num": -1}, "414": {"title": "deep video quality assessor: from spatio-temporal visual sensitivity to a convolutional neural aggregation network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_14", "abstract": "", "cite_num": -1}, "292": {"title": "part-activated deep reinforcement learning for action prediction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_26", "abstract": "", "cite_num": -1}, "700": {"title": "deep directional statistics: pose estimation with uncertainty quantification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_33", "abstract": "", "cite_num": -1}, "207": {"title": "diverse conditional image generation by stochastic regression with latent drop-out codes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_25", "abstract": "", "cite_num": -1}, "402": {"title": "consensus-driven propagation in massive unlabeled data for face recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_35", "abstract": "", "cite_num": -1}, "467": {"title": "shape reconstruction using volume sweeping and learned photoconsistency.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_48", "abstract": "", "cite_num": -1}, "471": {"title": "broadcasting convolutional network for visual relational reasoning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_46", "abstract": "", "cite_num": -1}, "612": {"title": "semi-dense 3d reconstruction with a stereo event camera.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_15", "abstract": "", "cite_num": -1}, "770": {"title": "deep feature factorization for concept discovery.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_21", "abstract": "", "cite_num": -1}, "376": {"title": "3dmv: joint 3d-multi-view prediction for 3d semantic scene segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_28", "abstract": "", "cite_num": -1}, "156": {"title": "a-contrario horizon-first vanishing point detection using second-order grouping laws.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_20", "abstract": "", "cite_num": -1}, "646": {"title": "deep cross-modal projection learning for image-text matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_42", "abstract": "", "cite_num": -1}, "186": {"title": "sdc-net: video prediction using spatially-displaced convolution.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_44", "abstract": "", "cite_num": -1}, "601": {"title": "pose guided human video generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_13", "abstract": "", "cite_num": -1}, "303": {"title": "c-wsl: count-guided weakly supervised localization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_10", "abstract": "", "cite_num": -1}, "367": {"title": "learning to solve nonlinear least squares for monocular stereo.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_18", "abstract": "", "cite_num": -1}, "580": {"title": "scale-awareness of light field camera based visual odometry.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_44", "abstract": "", "cite_num": -1}, "227": {"title": "sequential clique optimization for video object segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_32", "abstract": "", "cite_num": -1}, "14": {"title": "deep randomized ensembles for metric learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_44", "abstract": "", "cite_num": -1}, "47": {"title": "conditional prior networks for optical flow.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_17", "abstract": "", "cite_num": -1}, "497": {"title": "textual explanations for self-driving vehicles.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_35", "abstract": "", "cite_num": -1}, "439": {"title": "hard-aware point-to-set deep metric for person re-identification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_12", "abstract": "", "cite_num": -1}, "540": {"title": "towards end-to-end license plate detection and recognition: a large dataset and baseline.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_16", "abstract": "", "cite_num": -1}, "129": {"title": "the unmanned aerial vehicle benchmark: object detection and tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_23", "abstract": "", "cite_num": -1}, "120": {"title": "amc: automl for model compression and acceleration on mobile devices.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_48", "abstract": "", "cite_num": -1}, "560": {"title": "disentangling factors of variation with cycle-consistent variational auto-encoders.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_49", "abstract": "", "cite_num": -1}, "436": {"title": "deepkspd: learning kernel-matrix-based spd representation for fine-grained image recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_38", "abstract": "", "cite_num": -1}, "19": {"title": "diverse image-to-image translation via disentangled representations.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_3", "abstract": "", "cite_num": -1}, "299": {"title": "training binary weight networks via semi-binary decomposition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_39", "abstract": "", "cite_num": -1}, "154": {"title": "a unified framework for multi-view multi-class object pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_16", "abstract": "", "cite_num": -1}, "432": {"title": "dyan: a dynamical atoms-based network for video prediction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_11", "abstract": "", "cite_num": -1}, "647": {"title": "unsupervised domain adaptation for semantic segmentation via class-balanced self-training.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_18", "abstract": "", "cite_num": -1}, "469": {"title": "a geometric perspective on structured light coding.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_6", "abstract": "", "cite_num": -1}, "456": {"title": "beyond part models: person retrieval with refined part pooling (and a strong convolutional baseline).", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_30", "abstract": "", "cite_num": -1}, "197": {"title": "dense semantic and topological correspondence of 3d faces without landmarks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_32", "abstract": "", "cite_num": -1}, "476": {"title": "bidirectional feature pyramid network with recurrent attention residual modules for shadow detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_8", "abstract": "This paper presents a network to detect shadows by exploring and combining global context in deep la\nyers and local context in shallow layers of a deep convolutional neural network (CNN). There are two\n technical contributions in our network design. First, we formulate the recurrent attention residual\n (RAR) module to combine the contexts in two adjacent CNN layers and learn an attention map to selec\nt a residual and then refine the context features. Second, we develop a bidirectional feature pyrami\nd network (BFPN) to aggregate shadow contexts spanned across different CNN layers by deploying two s\neries of RAR modules in the network to iteratively combine and refine context features: one series t\no refine context features from deep to shallow layers, and another series from shallow to deep layer\ns. Hence, we can better suppress false detections and enhance shadow details at the same time. We ev\naluate our network on two common shadow detection benchmark datasets: SBU and UCF. Experimental resu\nlts show that our network outperforms the best existing method with 34.88% reduction on SBU and 34.5\n7% reduction on UCF for the balance error rate.", "cite_num": -1}, "658": {"title": "learning discriminative video representations using adversarial perturbations.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_42", "abstract": "", "cite_num": -1}, "755": {"title": "multiposenet: fast multi-person pose estimation using pose residual network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_26", "abstract": "", "cite_num": -1}, "256": {"title": "mutual learning to adapt for joint human parsing and pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_31", "abstract": "", "cite_num": -1}, "218": {"title": "contextvp: fully context-aware video prediction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_46", "abstract": "", "cite_num": -1}, "78": {"title": "structured siamese network for real-time visual tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_22", "abstract": "", "cite_num": -1}, "732": {"title": "spatio-temporal transformer network for video restoration.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_7", "abstract": "", "cite_num": -1}, "59": {"title": "zero-shot deep domain adaptation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_47", "abstract": "", "cite_num": -1}, "116": {"title": "joint blind motion deblurring and depth estimation of light field.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_18", "abstract": "", "cite_num": -1}, "136": {"title": "effective use of synthetic data for urban scene semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_6", "abstract": "", "cite_num": -1}, "686": {"title": "w-talc: weakly-supervised temporal activity localization and classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_35", "abstract": "", "cite_num": -1}, "385": {"title": "weakly-supervised video summarization using variational encoder-decoder and web prior.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_12", "abstract": "", "cite_num": -1}, "548": {"title": "joint learning of intrinsic images and semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_18", "abstract": "", "cite_num": -1}, "714": {"title": "deep model-based 6d pose refinement in rgb.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_49", "abstract": "", "cite_num": -1}, "231": {"title": "asynchronous, photometric feature tracking using events and frames.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_46", "abstract": "", "cite_num": -1}, "285": {"title": "learning monocular depth by distilling cross-domain stereo networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_30", "abstract": "", "cite_num": -1}, "583": {"title": "learning dynamic memory networks for object tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_10", "abstract": "", "cite_num": -1}, "457": {"title": "modular generative adversarial networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_10", "abstract": "", "cite_num": -1}, "193": {"title": "spherenet: learning spherical representations for detection and classification in omnidirectional images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_32", "abstract": "", "cite_num": -1}, "704": {"title": "domain transfer through deep activation matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_36", "abstract": "", "cite_num": -1}, "739": {"title": "pose proposal networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_21", "abstract": "", "cite_num": -1}, "166": {"title": "joint 3d tracking of a deformable object in interaction with a hand.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_30", "abstract": "", "cite_num": -1}, "262": {"title": "tracking emerges by colorizing videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_24", "abstract": "", "cite_num": -1}, "263": {"title": "dense pose transfer.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_8", "abstract": "", "cite_num": -1}, "244": {"title": "3d vehicle trajectory reconstruction in monocular video data using environment structure constraints.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_3", "abstract": "", "cite_num": -1}, "36": {"title": "visual-inertial object detection and mapping.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_19", "abstract": "", "cite_num": -1}, "768": {"title": "multimodal unsupervised image-to-image translation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_11", "abstract": "Unsupervised image-to-image translation is an important and challenging problem in computer vision. \nGiven an image in the source domain, the goal is to learn the conditional distribution of correspond\ning images in the target domain, without seeing any pairs of corresponding images. While this condit\nional distribution is inherently multimodal, existing approaches make an overly simplified assumptio\nn, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse out\nputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervi\nsed Image-to-image Translation (MUNIT) framework. We assume that the image representation can be dec\nomposed into a content code that is domain-invariant, and a style code that captures domain-specific\n properties. To translate an image to another domain, we recombine its content code with a random st\nyle code sampled from the style space of the target domain. We analyze the proposed framework and es\ntablish several theoretical results. Extensive experiments with comparisons to the state-of-the-art \napproaches further demonstrates the advantage of the proposed framework. Moreover, our framework all\nows users to control the style of translation outputs by providing an example style image. Code and \npretrained models are available at this https URL", "cite_num": 88}, "648": {"title": "adversarial open-world person re-identification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_18", "abstract": "", "cite_num": -1}, "288": {"title": "a systematic dnn weight pruning framework using alternating direction method of multipliers.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_12", "abstract": "", "cite_num": -1}, "400": {"title": "ml-locnet: improving object localization with multi-view learning network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_15", "abstract": "", "cite_num": -1}, "102": {"title": "local orthogonal-group testing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_28", "abstract": "", "cite_num": -1}, "339": {"title": "mt-vae: learning motion transformations to generate multimodal human dynamics.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_17", "abstract": "Long-term human motion can be represented as a series of motion modes\u2014motion sequences that capture \nshort-term temporal dynamics\u2014with transitions between them. We leverage this structure and present a\n novel Motion Transformation Variational Auto-Encoders (MT-VAE) for learning motion sequence generat\nion. Our model jointly learns a feature embedding for motion modes (that the motion sequence can be \nreconstructed from) and a feature transformation that represents the transition of one motion mode t\no the next motion mode. Our model is able to generate multiple diverse and plausible motion sequence\ns in the future from the same input. We apply our approach to both facial and full body motion, and \ndemonstrate applications like analogy-based motion transfer and video synthesis.", "cite_num": 5}, "737": {"title": "learning 3d shapes as multi-layered height-maps using 2d convolutional networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_5", "abstract": "", "cite_num": -1}, "569": {"title": "semi-supervised deep learning with memory.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_17", "abstract": "", "cite_num": -1}, "752": {"title": "zero-shot keyword spotting for visual speech recognition in-the-wild.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_32", "abstract": "", "cite_num": -1}, "764": {"title": "real-time mdnet.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_6", "abstract": "", "cite_num": -1}, "103": {"title": "hashing with binary matrix pursuit.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_21", "abstract": "", "cite_num": -1}, "386": {"title": "gal: geometric adversarial loss for single-view 3d-object reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_49", "abstract": "", "cite_num": -1}, "357": {"title": "polarimetric three-view geometry.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_2", "abstract": "", "cite_num": -1}, "688": {"title": "unpaired image captioning by language pivoting.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_31", "abstract": "", "cite_num": -1}, "356": {"title": "agil: learning attention from human for visuomotor tasks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_41", "abstract": "When intelligent agents learn visuomotor behaviors from human demonstrations, they may benefit from \nknowing where the human is allocating visual attention, which can be inferred from their gaze. A wea\nlth of information regarding intelligent decision making is conveyed by human gaze allocation; hence\n, exploiting such information has the potential to improve the agents\u2019 performance. With this motiva\ntion, we propose the AGIL (Attention Guided Imitation Learning) framework. We collect high-quality h\numan action and gaze data while playing Atari games in a carefully controlled experimental setting. \nUsing these data, we first train a deep neural network that can predict human gaze positions and vis\nual attention with high accuracy (the gaze network) and then train another network to predict human \nactions (the policy network). Incorporating the learned attention model from the gaze network into t\nhe policy network significantly improves the action prediction accuracy and task performance.", "cite_num": 7}, "109": {"title": "simultaneous edge alignment and learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_24", "abstract": "", "cite_num": -1}, "71": {"title": "dock: detecting objects by transferring common-sense knowledge.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_30", "abstract": "", "cite_num": -1}, "181": {"title": "mrf optimization with separable convex prior on partially ordered labels.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_21", "abstract": "", "cite_num": -1}, "562": {"title": "modality distillation with multiple stream networks for action recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_7", "abstract": "Diverse input data modalities can provide complementary cues for several tasks, usually leading to m\nore robust algorithms and better performance. However, while a (training) dataset could be accuratel\ny designed to include a variety of sensory inputs, it is often the case that not all modalities are \navailable in real life (testing) scenarios, where a model has to be deployed. This raises the challe\nnge of how to learn robust representations leveraging multimodal data in the training stage, while c\nonsidering limitations at test time, such as noisy or missing modalities. This paper presents a new \napproach for multimodal video action recognition, developed within the unified frameworks of distill\nation and privileged information, named generalized distillation. Particularly, we consider the case\n of learning representations from depth and RGB videos, while relying on RGB data only at test time.\n We propose a new approach to train an hallucination network that learns to distill depth features t\nhrough multiplicative connections of spatiotemporal representations, leveraging soft labels and hard\n labels, as well as distance between feature maps. We report state-of-the-art results on video actio\nn classification on the largest multimodal dataset available for this task, the NTU RGB+D, as well a\ns on the UWA3DII and Northwestern-UCLA.", "cite_num": 8}, "160": {"title": "dynamic filtering with large sampling field for convnets.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_12", "abstract": "", "cite_num": -1}, "127": {"title": "transductive centroid projection for semi-supervised large-scale recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_5", "abstract": "", "cite_num": -1}, "225": {"title": "physical primitive decomposition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_1", "abstract": "", "cite_num": -1}, "480": {"title": "a joint sequence fusion model for video question answering and retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_29", "abstract": "", "cite_num": -1}, "584": {"title": "contemplating visual emotions: understanding and overcoming dataset bias.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_36", "abstract": "", "cite_num": -1}, "670": {"title": "multimodal image alignment through a multiscale chain of neural networks with application to remote sensing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_40", "abstract": "We tackle here the problem of multimodal image non-rigid registration, which is of prime importance \nin remote sensing and medical imaging. The difficulties encountered by classical registration approa\nches include feature design and slow optimization by gradient descent. By analyzing these methods, w\ne note the significance of the notion of scale. We design easy-to-train, fully-convolutional neural \nnetworks able to learn scale-specific features. Once chained appropriately, they perform global regi\nstration in linear time, getting rid of gradient descent schemes by predicting directly the deformat\nion. We show their performance in terms of quality and speed through various tasks of remote sensing\n multimodal image alignment. In particular, we are able to register correctly cadastral maps of buil\ndings as well as road polylines onto RGB images, and outperform current keypoint matching methods.", "cite_num": 2}, "219": {"title": "separating reflection and transmission images in the wild.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_6", "abstract": "", "cite_num": -1}, "121": {"title": "autoloc: weakly-supervised temporal action localization in untrimmed videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_10", "abstract": "", "cite_num": -1}, "590": {"title": "the contextual loss for image transformation with non-aligned data.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_47", "abstract": "", "cite_num": -1}, "165": {"title": "weakly-supervised 3d hand pose estimation from monocular rgb images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_41", "abstract": "", "cite_num": -1}, "494": {"title": "face super-resolution guided by facial component heatmaps.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_14", "abstract": "", "cite_num": -1}, "742": {"title": "san: learning relationship between convolutional features for multi-scale object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_20", "abstract": "", "cite_num": -1}, "701": {"title": "goal-oriented visual question generation via intermediate rewards.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_12", "abstract": "", "cite_num": -1}, "150": {"title": "context refinement for object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_5", "abstract": "", "cite_num": -1}, "158": {"title": "open-world stereo video matching with deep rnn.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_7", "abstract": "", "cite_num": -1}, "290": {"title": "dynamic conditional networks for few-shot learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_2", "abstract": "", "cite_num": -1}, "79": {"title": "deeply learned compositional models for human pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_12", "abstract": "", "cite_num": -1}, "245": {"title": "generalizing a person retrieval model hetero- and homogeneously.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_11", "abstract": "", "cite_num": -1}, "575": {"title": "towards robust neural networks via random self-ensemble.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_23", "abstract": "", "cite_num": -1}, "512": {"title": "shufflenet v2: practical guidelines for efficient cnn architecture design.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_8", "abstract": "", "cite_num": -1}, "692": {"title": "imagine this! scripts to compositions to videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_37", "abstract": "", "cite_num": -1}, "527": {"title": "multi-scale residual network for image super-resolution.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_32", "abstract": "", "cite_num": -1}, "656": {"title": "multi-scale context intertwining for semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_37", "abstract": "", "cite_num": -1}, "9": {"title": "advise: symbolism and external knowledge for decoding advertisements.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_51", "abstract": "", "cite_num": -1}, "697": {"title": "floornet: a unified framework for floorplan reconstruction from 3d scans.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_13", "abstract": "", "cite_num": -1}, "85": {"title": "deep recursive hdri: inverse tone mapping using generative adversarial networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_37", "abstract": "", "cite_num": -1}, "397": {"title": "an adversarial approach to hard triplet generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_31", "abstract": "", "cite_num": -1}, "537": {"title": "transductive semi-supervised deep learning using min-max features.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_19", "abstract": "", "cite_num": -1}, "690": {"title": "spidercnn: deep learning on point sets with parameterized convolutional filters.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_6", "abstract": "", "cite_num": -1}, "508": {"title": "generative adversarial network with spatial attention for face attribute editing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_26", "abstract": "Face attribute editing aims at editing the face image with the given attribute. Most existing works \nemploy Generative Adversarial Network (GAN) to operate face attribute editing. However, these method\ns inevitably change the attribute-irrelevant regions, as shown in Fig. 1. Therefore, we introduce th\ne spatial attention mechanism into GAN framework (referred to as SaGAN), to only alter the attribute\n-specific region and keep the rest unchanged. Our approach SaGAN consists of a generator and a discr\niminator. The generator contains an attribute manipulation network (AMN) to edit the face image, and\n a spatial attention network (SAN) to localize the attribute-specific region which restricts the alt\nernation of AMN within this region. The discriminator endeavors to distinguish the generated images \nfrom the real ones, and classify the face attribute. Experiments demonstrate that our approach can a\nchieve promising visual results, and keep those attribute-irrelevant regions unchanged. Besides, our\n approach can benefit the face recognition by data augmentation.", "cite_num": 14}, "29": {"title": "end-to-end view synthesis for light field imaging with pseudo 4dcnn.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_21", "abstract": "", "cite_num": -1}, "23": {"title": "cubenet: equivariance to 3d rotation and translation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_35", "abstract": "", "cite_num": -1}, "94": {"title": "iterative crowd counting.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_17", "abstract": "", "cite_num": -1}, "91": {"title": "deformable pose traversal convolution for 3d action and gesture recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_9", "abstract": "", "cite_num": -1}, "775": {"title": "escaping from collapsing modes in a constrained space.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_13", "abstract": "", "cite_num": -1}, "326": {"title": "joint camera spectral sensitivity selection and hyperspectral image recovery.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_48", "abstract": "", "cite_num": -1}, "745": {"title": "video summarization using fully convolutional sequence networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_22", "abstract": "", "cite_num": -1}, "148": {"title": "bodynet: volumetric inference of 3d human body shapes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_2", "abstract": "", "cite_num": -1}, "33": {"title": "deep volumetric video from very sparse multi-view performance capture.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_21", "abstract": "", "cite_num": -1}, "185": {"title": "extreme network compression via filter group approximation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_19", "abstract": "", "cite_num": -1}, "722": {"title": "ts ^2 2 c: tight box mining with surrounding segmentation context for weakly supervised object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_27", "abstract": "", "cite_num": -1}, "321": {"title": "affinity derivation and graph merge for instance segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_42", "abstract": "", "cite_num": -1}, "746": {"title": "resound: towards action recognition without representation bias.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_32", "abstract": "", "cite_num": -1}, "577": {"title": "predicting future instance segmentation by forecasting convolutional features.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_36", "abstract": "", "cite_num": -1}, "239": {"title": "progressive structure from motion.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_2", "abstract": "", "cite_num": -1}, "715": {"title": "a hybrid model for identity obfuscation by face replacement.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_34", "abstract": "", "cite_num": -1}, "593": {"title": "efficient relative attribute learning using graph neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_34", "abstract": "", "cite_num": -1}, "347": {"title": "attend and rectify: a gated attention mechanism for fine-grained recovery.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_22", "abstract": "We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained rec\nognition. It learns to attend to lower-level feature activations without requiring part annotations \nand uses these activations to update and rectify the output likelihood distribution. In contrast to \nother approaches, the proposed mechanism is modular, architecture-independent and efficient both in \nterms of parameters and computation required. Experiments show that networks augmented with our appr\noach systematically improve their classification accuracy and become more robust to clutter. As a re\nsult, Wide Residual Networks augmented with our proposal surpasses the state of the art classificati\non accuracies in CIFAR-10, the Adience gender recognition task, Stanford dogs, and UEC Food-100.", "cite_num": 3}, "32": {"title": "joint person segmentation and identification in synchronized first- and third-person videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_39", "abstract": "", "cite_num": -1}, "279": {"title": "integral human pose regression.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_33", "abstract": "", "cite_num": -1}, "234": {"title": "deep virtual stereo odometry: leveraging deep depth prediction for monocular direct sparse odometry.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_50", "abstract": "", "cite_num": -1}, "390": {"title": "lip movements generation at a glance.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_32", "abstract": "", "cite_num": -1}, "640": {"title": "comparator networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_48", "abstract": "", "cite_num": -1}, "458": {"title": "a dataset and architecture for visual reasoning with a working memory.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_44", "abstract": "", "cite_num": -1}, "325": {"title": "occlusion-aware r-cnn: detecting pedestrians in a crowd.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_39", "abstract": "", "cite_num": -1}, "682": {"title": "inner space preserving generative pose machine.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_44", "abstract": "", "cite_num": -1}, "138": {"title": "single image intrinsic decomposition without a single intrinsic image.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_13", "abstract": "", "cite_num": -1}, "51": {"title": "deep boosting for image denoising.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_1", "abstract": "", "cite_num": -1}, "42": {"title": "shapestacks: learning vision-based physical intuition for generalised object stacking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_43", "abstract": "", "cite_num": -1}, "627": {"title": "image inpainting for irregular holes using partial convolutions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_6", "abstract": "", "cite_num": -1}, "134": {"title": "dependency-aware attention control for unconstrained face recognition with image sets.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_34", "abstract": "This paper targets the problem of image set-based face verification and identification. Unlike tradi\ntional single media (an image or video) setting, we encounter a set of heterogeneous contents contai\nning orderless images and videos. The importance of each image is usually considered either equal or\n based on their independent quality assessment. How to model the relationship of orderless images wi\nthin a set remains a challenge. We address this problem by formulating it as a Markov Decision Proce\nss (MDP) in the latent space. Specifically, we first present a dependency-aware attention control (D\nAC) network, which resorts to actor-critic reinforcement learning for sequential attention decision \nof each image embedding to fully exploit the rich correlation cues among the unordered images. Moreo\nver, we introduce its sample-efficient variant with off-policy experience replay to speed up the lea\nrning process. The pose-guided representation scheme can further boost the performance at the extrem\nes of the pose variation.", "cite_num": 4}, "733": {"title": "museum exhibit identification challenge for the supervised domain adaptation and beyond.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_48", "abstract": "", "cite_num": -1}, "296": {"title": "boosted attention: leveraging human attention for image captioning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_5", "abstract": "Visual attention has shown usefulness in image captioning, with the goal of enabling a caption model\n to selectively focus on regions of interest. Existing models typically rely on top-down language in\nformation and learn attention implicitly by optimizing the captioning objectives. While somewhat eff\nective, the learned top-down attention can fail to focus on correct regions of interest without dire\nct supervision of attention. Inspired by the human visual system which is driven by not only the tas\nk-specific top-down signals but also the visual stimuli, we in this work propose to use both types o\nf attention for image captioning. In particular, we highlight the complementary nature of the two ty\npes of attention and develop a model (Boosted Attention) to integrate them for image captioning. We \nvalidate the proposed approach with state-of-the-art performance across various evaluation metrics.", "cite_num": 5}, "168": {"title": "using lip to gloss over faces in single-stage face detection networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_39", "abstract": "", "cite_num": -1}, "559": {"title": "image generation from sketch constraint using contextual gan.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_13", "abstract": "", "cite_num": -1}, "25": {"title": "temporal modular networks for retrieving complex compositional activities in videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_34", "abstract": "", "cite_num": -1}, "377": {"title": "fictitious gan: training gans with historical models.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_8", "abstract": "", "cite_num": -1}, "570": {"title": "universal sketch perceptual grouping.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_36", "abstract": "", "cite_num": -1}, "338": {"title": "unsupervised video object segmentation using motion saliency-guided spatio-temporal propagation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_48", "abstract": "", "cite_num": -1}, "613": {"title": "learning to reconstruct high-quality 3d shapes with cascaded fully convolutional networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_38", "abstract": "", "cite_num": -1}, "691": {"title": "visual coreference resolution in visual dialog using neural module networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_10", "abstract": "", "cite_num": -1}, "172": {"title": "lq-nets: learned quantization for highly accurate and compact deep neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_23", "abstract": "", "cite_num": -1}, "228": {"title": "deep fundamental matrix estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_18", "abstract": "", "cite_num": -1}, "10": {"title": "programmable triangulation light curtains.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_2", "abstract": "", "cite_num": -1}, "6": {"title": "temporal relational reasoning in videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_49", "abstract": "", "cite_num": -1}, "61": {"title": "stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_40", "abstract": "", "cite_num": -1}, "22": {"title": "ask, acquire, and attack: data-free uap generation using class impressions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_2", "abstract": "", "cite_num": -1}, "478": {"title": "deepim: deep iterative matching for 6d pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_42", "abstract": "", "cite_num": -1}, "295": {"title": "mask textspotter: an end-to-end trainable neural network for spotting text with arbitrary shapes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_5", "abstract": "", "cite_num": -1}, "381": {"title": "recovering accurate 3d human pose in the wild using imus and a moving camera.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_37", "abstract": "", "cite_num": -1}, "92": {"title": "sub-gan: an unsupervised generative model via subspaces.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_43", "abstract": "", "cite_num": -1}, "473": {"title": "advio: an authentic dataset for visual-inertial odometry.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_26", "abstract": "", "cite_num": -1}, "179": {"title": "attention-aware deep adversarial hashing for cross-modal retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_36", "abstract": "Due to the rapid growth of multi-modal data, hashing methods for cross-modal retrieval have received\n considerable attention. However, finding content similarities between different modalities of data \nis still challenging due to an existing heterogeneity gap. To further address this problem, we propo\nse an adversarial hashing network with an attention mechanism to enhance the measurement of content \nsimilarities by selectively focusing on the informative parts of multi-modal data. The proposed new \ndeep adversarial network consists of three building blocks: (1) the feature learning module to obtai\nn the feature representations; (2) the attention module to generate an attention mask, which is used\n to divide the feature representations into the attended and unattended feature representations; and\n (3) the hashing module to learn hash functions that preserve the similarities between different mod\nalities. In our framework, the attention and hashing modules are trained in an adversarial way: the \nattention module attempts to make the hashing module unable to preserve the similarities of multi-mo\ndal data w.r.t. the unattended feature representations, while the hashing module aims to preserve th\ne similarities of multi-modal data w.r.t. the attended and unattended feature representations. Exten\nsive evaluations on several benchmark datasets demonstrate that the proposed method brings substanti\nal improvements over other state-of-the-art cross-modal hashing methods.", "cite_num": 8}, "614": {"title": "\"factual\" or \"emotional\": stylized image captioning with adaptive learning and attention.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_32", "abstract": "Generating stylized captions for an image is an emerging topic in image captioning. Given an image a\ns input, it requires the system to generate a caption that has a specific style (e.g., humorous, rom\nantic, positive, and negative) while describing the image content semantically accurately. In this p\naper, we propose a novel stylized image captioning model that effectively takes both requirements in\nto consideration. To this end, we first devise a new variant of LSTM, named style-factual LSTM, as t\nhe building block of our model. It uses two groups of matrices to capture the factual and stylized k\nnowledge, respectively, and automatically learns the word-level weights of the two groups based on p\nrevious context. In addition, when we train the model to capture stylized elements, we propose an ad\naptive learning approach based on a reference factual model, it provides factual knowledge to the mo\ndel as the model learns from stylized caption labels, and can adaptively compute how much informatio\nn to supply at each time step. We evaluate our model on two stylized image captioning datasets, whic\nh contain humorous/romantic captions and positive/negative captions, respectively. Experiments shows\n that our proposed model outperforms the state-of-the-art approaches, without using extra ground tru\nth supervision.", "cite_num": 1}, "167": {"title": "saliency detection in 360 ^\\circ \u2218 videos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_30", "abstract": "", "cite_num": -1}, "80": {"title": "long-term tracking in the wild: a benchmark.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_41", "abstract": "", "cite_num": -1}, "44": {"title": "detnet: design backbone for object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_21", "abstract": "", "cite_num": -1}, "173": {"title": "saas: speed as a supervisor for semi-supervised learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_10", "abstract": "", "cite_num": -1}, "15": {"title": "improving deep visual representation for person re-identification by global and local image-language association.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_4", "abstract": "", "cite_num": -1}, "170": {"title": "parn: pyramidal affine regression networks for dense semantic correspondence.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_22", "abstract": "", "cite_num": -1}, "382": {"title": "pixel2mesh: generating 3d mesh models from single rgb images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_4", "abstract": "", "cite_num": -1}, "725": {"title": "unsupervised hard example mining from videos for improved object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_19", "abstract": "", "cite_num": -1}, "118": {"title": "geolocation estimation of photos using a hierarchical model and scene classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_35", "abstract": "", "cite_num": -1}, "272": {"title": "group normalization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_1", "abstract": "", "cite_num": -1}, "348": {"title": "conditional image-text embedding networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_16", "abstract": "", "cite_num": -1}, "30": {"title": "object-centered image stitching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_50", "abstract": "", "cite_num": -1}, "54": {"title": "decouple learning for parameterized image operators.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_27", "abstract": "", "cite_num": -1}, "542": {"title": "deep video generation, prediction and completion of human action sequences.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_23", "abstract": "", "cite_num": -1}, "27": {"title": "isnn: impact sound neural network for audio-visual object classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_34", "abstract": "", "cite_num": -1}, "151": {"title": "curriculumnet: weakly supervised learning from large-scale web images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_9", "abstract": "", "cite_num": -1}, "743": {"title": "interpolating convolutional neural networks using batch normalization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_35", "abstract": "", "cite_num": -1}, "582": {"title": "articulatedfusion: real-time reconstruction of motion, geometry and segmentation using a single depth camera.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_20", "abstract": "", "cite_num": -1}, "371": {"title": "3d scene flow from 4d light field gradients.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_41", "abstract": "", "cite_num": -1}, "684": {"title": "generative semantic manipulation with mask-contrasting gan.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_34", "abstract": "", "cite_num": -1}, "16": {"title": "real-time 'actor-critic' tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_20", "abstract": "", "cite_num": -1}, "115": {"title": "learning priors for semantic 3d reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_20", "abstract": "", "cite_num": -1}, "520": {"title": "affine correspondences between central cameras for rapid relative pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_30", "abstract": "", "cite_num": -1}, "696": {"title": "quantized densely connected u-nets for efficient landmark localization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_21", "abstract": "", "cite_num": -1}, "187": {"title": "deblurring natural image using super-gaussian fields.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_28", "abstract": "", "cite_num": -1}, "641": {"title": "single image water hazard detection using fcn with reflection attention units.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_7", "abstract": "Water bodies, such as puddles and flooded areas, on and off road pose significant risks to autonomou\ns cars. Detecting water from moving camera is a challenging task as water surface is highly refracti\nve, and its appearance varies with viewing angle, surrounding scene, weather conditions. In this pap\ner, we present a water puddle detection method based on a Fully Convolutional Network (FCN) with our\n newly proposed Reflection Attention Units (RAUs). An RAU is a deep network unit designed to embody \nthe physics of reflection on water surface from sky and nearby scene. To verify the performance of o\nur proposed method, we collect 11455 color stereo images with polarizers, and 985 of left images are\n annotated and divided into 2 datasets: On Road (ONR) dataset and Off Road (OFR) dataset. We show th\nat FCN-8s with RAUs improves significantly precision and recall metrics as compared to FCN-8s, DeepL\nab V2 and Gaussian Mixture Model (GMM). We also show that focal loss function can improve the perfor\nmance of FCN-8s network due to the extreme imbalance of water versus ground classification problem.", "cite_num": -1}, "492": {"title": "attention-gan for object transfiguration in wild images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_11", "abstract": "This paper studies the object transfiguration problem in wild images. The generative network in clas\nsical GANs for object transfiguration often undertakes a dual responsibility: to detect the objects \nof interests and to convert the object from source domain to target domain. In contrast, we decompos\ne the generative network into two separat networks, each of which is only dedicated to one particula\nr sub-task. The attention network predicts spatial attention maps of images, and the transformation \nnetwork focuses on translating objects. Attention maps produced by attention network are encouraged \nto be sparse, so that major attention can be paid to objects of interests. No matter before or after\n object transfiguration, attention maps should remain constant. In addition, learning attention netw\nork can receive more instructions, given the available segmentation annotations of images. Experimen\ntal results demonstrate the necessity of investigating attention in object transfiguration, and that\n the proposed algorithm can learn accurate attention to improve quality of generated images.", "cite_num": 6}, "554": {"title": "second-order democratic aggregation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_38", "abstract": "", "cite_num": -1}, "490": {"title": "hierarchical metric learning and matching for 2d and 3d geometric correspondences.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_49", "abstract": "", "cite_num": -1}, "564": {"title": "few-shot human motion prediction via meta-learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_27", "abstract": "", "cite_num": -1}, "69": {"title": "deep adaptive attention for joint facial action unit detection and face alignment.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_43", "abstract": "Facial action unit (AU) detection and face alignment are two highly correlated tasks since facial la\nndmarks can provide precise AU locations to facilitate the extraction of meaningful local features f\nor AU detection. Most existing AU detection works often treat face alignment as a preprocessing and \nhandle the two tasks independently. In this paper, we propose a novel end-to-end deep learning frame\nwork for joint AU detection and face alignment, which has not been explored before. In particular, m\nulti-scale shared features are learned firstly, and high-level features of face alignment are fed in\nto AU detection. Moreover, to extract precise local features, we propose an adaptive attention learn\ning module to refine the attention map of each AU adaptively. Finally, the assembled local features \nare integrated with face alignment features and global features for AU detection. Experiments on BP4\nD and DISFA benchmarks demonstrate that our framework significantly outperforms the state-of-the-art\n methods for AU detection.", "cite_num": -1}, "434": {"title": "neural graph matching networks for fewshot 3d action recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_40", "abstract": "", "cite_num": -1}, "632": {"title": "dpp-net: device-aware progressive search for pareto-optimal neural architectures.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_32", "abstract": "", "cite_num": -1}, "75": {"title": "r2p2: a reparameterized pushforward policy for diverse, precise generative path forecasting.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_47", "abstract": "", "cite_num": -1}, "740": {"title": "ec-net: an edge-aware point set consolidation network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_24", "abstract": "", "cite_num": -1}, "135": {"title": "video object segmentation with joint re-identification and attention-aware mask propagation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_6", "abstract": "The problem of video object segmentation can become extremely challenging when multiple instances co\n-exist. While each instance may exhibit large scale and pose variations, the problem is compounded w\nhen instances occlude each other causing failures in tracking. In this study, we formulate a deep re\ncurrent network that is capable of segmenting and tracking objects in video simultaneously by their \ntemporal continuity, yet able to re-identify them when they re-appear after a prolonged occlusion. W\ne combine temporal propagation and re-identification functionalities into a single framework that ca\nn be trained end-to-end. In particular, we present a re-identification module with template expansio\nn to retrieve missing objects despite their large appearance changes. In addition, we contribute an \nattention-based recurrent mask propagation approach that is robust to distractors not belonging to t\nhe target segment. Our approach achieves a new state-of-the-art \\(\\mathcal {G}\\)-mean of 68.2 on the\n challenging DAVIS 2017 benchmark (test-dev set), outperforming the winning solution. Project Page: \nhttp://mmlab.ie.cuhk.edu.hk/projects/DyeNet/.", "cite_num": 16}, "26": {"title": "person search via a mask-guided two-stream cnn model.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_45", "abstract": "", "cite_num": -1}, "574": {"title": "learning 3d keypoint descriptors for non-rigid shape matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_1", "abstract": "", "cite_num": -1}, "62": {"title": "accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_40", "abstract": "", "cite_num": -1}, "744": {"title": "unsupervised holistic image generation from key local patches.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_2", "abstract": "", "cite_num": -1}, "525": {"title": "actor-centric relation network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_20", "abstract": "", "cite_num": -1}, "213": {"title": "stacked cross attention for image-text matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_13", "abstract": "In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment \nbetween objects or other salient stuffs (e.g. snow, sky, lawn) and the corresponding words in senten\nces allows to capture fine-grained interplay between vision and language, and makes image-text match\ning more interpretable. Prior works either simply aggregate the similarity of all possible pairs of \nregions and words without attending differentially to more and less important words or regions, or u\nse a multi-step attentional process to capture limited number of semantic alignments which is less i\nnterpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignmen\nts using both image regions and words in sentence as context and infer the image-text similarity. Ou\nr approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K\n, our approach outperforms the current best methods by 22.1% in text retrieval from image query, and\n 18.2% in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sen\ntence retrieval by 17.8% and image retrieval by 16.6% (based on Recall@1 using the 5K test set).", "cite_num": 9}, "602": {"title": "variational wasserstein clustering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_20", "abstract": "", "cite_num": -1}, "435": {"title": "exploring the limits of weakly supervised pretraining.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_12", "abstract": "", "cite_num": -1}, "265": {"title": "learning to fuse proposals from multiple scanline optimizations in semi-global matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_45", "abstract": "", "cite_num": -1}, "720": {"title": "geometric constrained joint lane segmentation and lane boundary detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_30", "abstract": "", "cite_num": -1}, "247": {"title": "nam: non-adversarial unsupervised domain mapping.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_27", "abstract": "", "cite_num": -1}, "180": {"title": "learning efficient single-stage pedestrian detectors by asymptotic localization fitting.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_38", "abstract": "", "cite_num": -1}, "183": {"title": "efficient global point cloud registration by matching rotation invariant features through translation search.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_28", "abstract": "", "cite_num": -1}, "343": {"title": "two at once: enhancing learning and generalization capacities via ibn-net.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_29", "abstract": "", "cite_num": -1}, "489": {"title": "a framework for evaluating 6-dof object trackers.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_36", "abstract": "", "cite_num": -1}, "759": {"title": "super-resolution and sparse view ct reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_9", "abstract": "", "cite_num": -1}, "642": {"title": "fighting fake news: image splice detection via learned self-consistency.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_7", "abstract": "", "cite_num": -1}, "674": {"title": "nneval: neural network based evaluation metric for image captioning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_3", "abstract": "", "cite_num": -1}, "214": {"title": "a dataset for lane instance segmentation in urban environments.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_33", "abstract": "", "cite_num": -1}, "37": {"title": "sod-mtgan: small object detection via multi-task generative adversarial network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_13", "abstract": "", "cite_num": -1}, "184": {"title": "shift-net: image inpainting via deep feature rearrangement.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_1", "abstract": "", "cite_num": -1}, "384": {"title": "eco: efficient convolutional network for online video understanding.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_43", "abstract": "", "cite_num": -1}, "222": {"title": "learning to blend photos.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_5", "abstract": "", "cite_num": -1}, "765": {"title": "x-ray computed tomography through scatter.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_3", "abstract": "", "cite_num": -1}, "563": {"title": "cnn-ps: cnn-based photometric stereo for general non-convex surfaces.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_1", "abstract": "", "cite_num": -1}, "464": {"title": "the devil of face recognition is in the noise.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_47", "abstract": "", "cite_num": -1}, "161": {"title": "efficient 6-dof tracking of handheld objects from an egocentric viewpoint.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_26", "abstract": "", "cite_num": -1}, "354": {"title": "hgmr: hierarchical gaussian mixtures for adaptive 3d registration.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_43", "abstract": "", "cite_num": -1}, "49": {"title": "person search in videos with one portrait through visual and temporal links.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_26", "abstract": "", "cite_num": -1}, "428": {"title": "key-word-aware network for referring expression image segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_3", "abstract": "", "cite_num": -1}, "65": {"title": "pairwise body-part attention for recognizing human-object interactions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_4", "abstract": "In human-object interactions (HOI) recognition, conventional methods consider the human body as a wh\nole and pay a uniform attention to the entire body region. They ignore the fact that normally, human\n interacts with an object by using some parts of the body. In this paper, we argue that different bo\ndy parts should be paid with different attention in HOI recognition, and the correlations between di\nfferent body parts should be further considered. This is because our body parts always work collabor\natively. We propose a new pairwise body-part attention model which can learn to focus on crucial par\nts, and their correlations for HOI recognition. A novel attention based feature selection method and\n a feature representation scheme that can capture pairwise correlations between body parts are intro\nduced in the model. Our proposed approach achieved 4% improvement over the state-of-the-art results \nin HOI recognition on the HICO dataset. We will make our model and source codes publicly available.", "cite_num": 3}, "52": {"title": "modeling visual context is key to augmenting object detection datasets.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_23", "abstract": "", "cite_num": -1}, "553": {"title": "srda: generating instance segmentation annotation via scanning, reasoning and domain adaptation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_8", "abstract": "", "cite_num": -1}, "266": {"title": "deep expander networks: efficient deep networks from graph theory.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_2", "abstract": "", "cite_num": -1}, "291": {"title": "improving sequential determinantal point processes for supervised video summarization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_32", "abstract": "", "cite_num": -1}, "468": {"title": "a closed-form solution to photorealistic image stylization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_28", "abstract": "", "cite_num": -1}, "522": {"title": "reenactgan: learning to reenact faces via boundary transfer.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_37", "abstract": "", "cite_num": -1}, "767": {"title": "person search by multi-scale matching.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_33", "abstract": "", "cite_num": -1}, "511": {"title": "cornernet: detecting objects as paired keypoints.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_45", "abstract": "", "cite_num": -1}, "495": {"title": "eigendecomposition-free training of deep networks with zero eigenvalue-based losses.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_47", "abstract": "", "cite_num": -1}, "736": {"title": "localization recall precision (lrp): a new performance metric for object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_31", "abstract": "", "cite_num": -1}, "677": {"title": "maximum margin metric learning over discriminative nullspace for person re-identification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_8", "abstract": "", "cite_num": -1}, "328": {"title": "flow-grounded spatial-temporal video prediction from still images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_37", "abstract": "", "cite_num": -1}, "242": {"title": "rethinking the form of latent states in image captioning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_18", "abstract": "", "cite_num": -1}, "408": {"title": "dual-agent deep reinforcement learning for deformable face tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_47", "abstract": "", "cite_num": -1}, "358": {"title": "online multi-object tracking with dual matching attention networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_23", "abstract": "In this paper, we propose an online Multi-Object Tracking (MOT) approach which integrates the merits\n of single object tracking and data association methods in a unified framework to handle noisy detec\ntions and frequent interactions between targets. Specifically, for applying single object tracking i\nn MOT, we introduce a cost-sensitive tracking loss based on the state-of-the-art visual tracker, whi\nch encourages the model to focus on hard negative distractors during online learning. For data assoc\niation, we propose Dual Matching Attention Networks (DMAN) with both spatial and temporal attention \nmechanisms. The spatial attention module generates dual attention maps which enable the network to f\nocus on the matching patterns of the input image pair, while the temporal attention module adaptivel\ny allocates different levels of attention to different samples in the tracklet to suppress noisy obs\nervations. Experimental results on the MOT benchmark datasets show that the proposed algorithm perfo\nrms favorably against both online and offline trackers in terms of identity-preserving metrics.", "cite_num": 18}, "205": {"title": "deepgum: learning deep robust regression with a gaussian-uniform mixture model.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_13", "abstract": "", "cite_num": -1}, "396": {"title": "fine-grained video categorization with redundancy reduction attention.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_9", "abstract": "For fine-grained categorization tasks, videos could serve as a better source than static images as v\nideos have a higher chance of containing discriminative patterns. Nevertheless, a video sequence cou\nld also contain a lot of redundant and irrelevant frames. How to locate critical information of inte\nrest is a challenging task. In this paper, we propose a new network structure, known as Redundancy R\neduction Attention (RRA), which learns to focus on multiple discriminative patterns by suppressing r\nedundant feature channels. Specifically, it firstly summarizes the video by weight-summing all featu\nre vectors in the feature maps of selected frames with a spatio-temporal soft attention, and then pr\nedicts which channels to suppress or to enhance according to this summary with a learned non-linear \ntransform. Suppression is achieved by modulating the feature maps and threshing out weak activations\n. The updated feature maps are then used in the next iteration. Finally, the video is classified bas\ned on multiple summaries. The proposed method achieves outstanding performances in multiple video cl\nassification datasets. Furthermore, we have collected two large-scale video datasets, YouTube-Birds \nand YouTube-Cars, for future researches on fine-grained video categorization. The datasets are avail\nable at http://www.cs.umd.edu/~chenzhu/fgvc.", "cite_num": 5}, "751": {"title": "monocular depth estimation with affinity, vertical pooling, and label enhancement.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_14", "abstract": "", "cite_num": -1}, "479": {"title": "interactive boundary prediction for object selection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_2", "abstract": "", "cite_num": -1}, "45": {"title": "image super-resolution using very deep residual channel attention networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_18", "abstract": "Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). H\nowever, we observe that deeper networks for image SR are more difficult to train. The low-resolution\n inputs and features contain abundant low-frequency information, which is treated equally across cha\nnnels, hence hindering the representational ability of CNNs. To solve these problems, we propose the\n very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residu\nal (RIR) structure to form very deep network, which consists of several residual groups with long sk\nip connections. Each residual group contains some residual blocks with short skip connections. Meanw\nhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections\n, making the main network focus on learning high-frequency information. Furthermore, we propose a ch\nannel attention mechanism to adaptively rescale channel-wise features by considering interdependenci\nes among channels. Extensive experiments show that our RCAN achieves better accuracy and visual impr\novements against state-of-the-art methods.", "cite_num": -1}, "204": {"title": "facial dynamics interpreter network: what are the important relations between local dynamics for facial trait estimation?", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_29", "abstract": "", "cite_num": -1}, "772": {"title": "probabilistic video generation using holistic attribute control.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_28", "abstract": "", "cite_num": -1}, "233": {"title": "single shot scene text retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_43", "abstract": "", "cite_num": -1}, "588": {"title": "3d-coded: 3d correspondences by deep deformation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_15", "abstract": "", "cite_num": -1}, "625": {"title": "deep attention neural tensor network for visual question answering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_2", "abstract": "Visual question answering (VQA) has drawn great attention in cross-modal learning problems, which en\nables a machine to answer a natural language question given a reference image. Significant progress \nhas been made by learning rich embedding features from images and questions by bilinear models, whil\ne neglects the key role from answers. In this paper, we propose a novel deep attention neural tensor\n network (DA-NTN) for visual question answering, which can discover the joint correlations over imag\nes, questions and answers with tensor-based representations. First, we model one of the pairwise int\neraction (e.g., image and question) by bilinear features, which is further encoded with the third di\nmension (e.g., answer) to be a triplet by bilinear tensor product. Second, we decompose the correlat\nion of different triplets by different answer and question types, and further propose a slice-wise a\nttention module on tensor to select the most discriminative reasoning process for inference. Third, \nwe optimize the proposed DA-NTN by learning a label regression with KL-divergence losses. Such a des\nign enables scalable training and fast convergence over a large number of answer set. We integrate t\nhe proposed DA-NTN structure into the state-of-the-art VQA models (e.g., MLB and MUTAN). Extensive e\nxperiments demonstrate the superior accuracy than the original MLB and MUTAN models, with 1.98%, 1.7\n0% relative increases on VQA-2.0 dataset, respectively.", "cite_num": 7}, "220": {"title": "rt-gene: real-time eye gaze estimation in natural environments.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_21", "abstract": "", "cite_num": -1}, "461": {"title": "end-to-end deep structured models for drawing crosswalks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_25", "abstract": "", "cite_num": -1}, "132": {"title": "saliency benchmarking made easy: separating models, maps and metrics.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_47", "abstract": "", "cite_num": -1}, "199": {"title": "look deeper into depth: monocular depth estimation with semantic booster and attention-driven loss.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_4", "abstract": "Monocular depth estimation benefits greatly from learning based techniques. By studying the training\n data, we observe that the per-pixel depth values in existing datasets typically exhibit a long-tail\ned distribution. However, most previous approaches treat all the regions in the training data equall\ny regardless of the imbalanced depth distribution, which restricts the model performance particularl\ny on distant depth regions. In this paper, we investigate the long tail property and delve deeper in\nto the distant depth regions (i.e. the tail part) to propose an attention-driven loss for the networ\nk supervision. In addition, to better leverage the semantic information for monocular depth estimati\non, we propose a synergy network to automatically learn the information sharing strategies between t\nhe two tasks. With the proposed attention-driven loss and synergy network, the depth estimation and \nsemantic labeling tasks can be mutually improved. Experiments on the challenging indoor dataset show\n that the proposed approach achieves state-of-the-art performance on both monocular depth estimation\n and semantic labeling tasks.", "cite_num": -1}, "417": {"title": "understanding degeneracies and ambiguities in attribute transfer.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_43", "abstract": "", "cite_num": -1}, "680": {"title": "bayesian semantic instance segmentation in open set world.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_1", "abstract": "", "cite_num": -1}, "155": {"title": "k-convexity shape priors for segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_3", "abstract": "", "cite_num": -1}, "426": {"title": "learning to capture light fields through a coded aperture camera.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_26", "abstract": "", "cite_num": -1}, "57": {"title": "clustering convolutional kernels to compress deep neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_14", "abstract": "", "cite_num": -1}, "653": {"title": "recurrent fusion network for image captioning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_31", "abstract": "", "cite_num": -1}, "440": {"title": "move forward and tell: a progressive generator of video descriptions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_29", "abstract": "", "cite_num": -1}, "660": {"title": "learning single-view 3d reconstruction with limited pose supervision.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_6", "abstract": "", "cite_num": -1}, "248": {"title": "lifelong learning via progressive distillation and retrospection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_27", "abstract": "", "cite_num": -1}, "268": {"title": "the sound of pixels.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_35", "abstract": "", "cite_num": -1}, "543": {"title": "discriminative region proposal adversarial networks for high-quality image-to-image translation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_47", "abstract": "", "cite_num": -1}, "636": {"title": "receptive field block net for accurate and fast object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_24", "abstract": "", "cite_num": -1}, "340": {"title": "interpretable basis decomposition for visual explanation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_8", "abstract": "", "cite_num": -1}, "13": {"title": "a segmentation-aware deep fusion network for compressed sensing mri.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_4", "abstract": "", "cite_num": -1}, "566": {"title": "bop: benchmark for 6d object pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_2", "abstract": "", "cite_num": -1}, "277": {"title": "end-to-end learning of driving models with surround-view cameras and route planners.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_27", "abstract": "", "cite_num": -1}, "342": {"title": "hbe: hand branch ensemble network for real-time 3d hand pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_31", "abstract": "", "cite_num": -1}, "724": {"title": "stereo computation for a single mixture image.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_27", "abstract": "", "cite_num": -1}, "500": {"title": "tackling 3d tof artifacts through learning and the flat dataset.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_23", "abstract": "", "cite_num": -1}, "83": {"title": "hierarchical relational networks for group activity recognition and retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_44", "abstract": "", "cite_num": -1}, "455": {"title": "coloring with words: guiding image colorization through text-based palette generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_27", "abstract": "", "cite_num": -1}, "672": {"title": "object detection in video with spatiotemporal sampling networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_21", "abstract": "", "cite_num": -1}, "12": {"title": "image manipulation with perceptual discriminators.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_36", "abstract": "", "cite_num": -1}, "294": {"title": "distractor-aware siamese networks for visual object tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_7", "abstract": "", "cite_num": -1}, "633": {"title": "learning to segment via cut-and-paste.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_3", "abstract": "", "cite_num": -1}, "604": {"title": "graph adaptive knowledge transfer for unsupervised domain adaptation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_3", "abstract": "", "cite_num": -1}, "661": {"title": "uncertainty estimates and multi-hypotheses networks for optical flow.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_40", "abstract": "", "cite_num": -1}, "634": {"title": "structural consistency and controllability for diverse colorization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_37", "abstract": "", "cite_num": -1}, "255": {"title": "multiple-gaze geometry: inferring novel 3d locations from gazes observed in monocular video.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_38", "abstract": "", "cite_num": -1}, "297": {"title": "open set domain adaptation by backpropagation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_10", "abstract": "", "cite_num": -1}, "323": {"title": "explainable neural computation via stack neural module networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_4", "abstract": "", "cite_num": -1}, "465": {"title": "multi-fiber networks for video recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_22", "abstract": "", "cite_num": -1}, "518": {"title": "multiresolution tree networks for 3d point cloud processing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_7", "abstract": "", "cite_num": -1}, "93": {"title": "double jpeg detection in mixed jpeg quality factors using deep convolutional neural network.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_39", "abstract": "", "cite_num": -1}, "300": {"title": "constrained optimization based low-rank approximation of deep neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_45", "abstract": "", "cite_num": -1}, "310": {"title": "quantization mimic: towards very tiny cnn for object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_17", "abstract": "", "cite_num": -1}, "360": {"title": "proximal dehaze-net: a prior learning-based deep network for single image dehazing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_43", "abstract": "", "cite_num": -1}, "730": {"title": "deep regionlets for object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_49", "abstract": "", "cite_num": -1}, "619": {"title": "convolutional networks with adaptive inference graphs.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_1", "abstract": "", "cite_num": -1}, "3": {"title": "question-guided hybrid convolution for visual question answering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_29", "abstract": "", "cite_num": -1}, "460": {"title": "combining 3d model contour energy and keypoints for object tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_4", "abstract": "", "cite_num": -1}, "617": {"title": "synthetically supervised feature learning for scene text recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_27", "abstract": "", "cite_num": -1}, "708": {"title": "local spectral graph convolution for point set feature learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_4", "abstract": "", "cite_num": -1}, "713": {"title": "stereo relative pose from line and point feature triplets.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_40", "abstract": "", "cite_num": -1}, "599": {"title": "question type guided attention in visual question answering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_10", "abstract": "Visual Question Answering (VQA) requires integration of feature maps with drastically different stru\nctures. Image descriptors have structures at multiple spatial scales, while lexical inputs inherentl\ny follow a temporal sequence and naturally cluster into semantically different question types. A lot\n of previous works use complex models to extract feature representations but neglect to use high-lev\nel information summary such as question types in learning. In this work, we propose Question Type-gu\nided Attention (QTA). It utilizes the information of question type to dynamically balance between bo\nttom-up and top-down visual features, respectively extracted from ResNet and Faster R-CNN networks. \nWe experiment with multiple VQA architectures with extensive input ablation studies over the TDIUC d\nataset and show that QTA systematically improves the performance by more than 5% across multiple que\nstion type categories such as \u201cActivity Recognition\u201d, \u201cUtility\u201d and \u201cCounting\u201d on TDIUC dataset comp\nared to the state-of-art. By adding QTA on the state-of-art model MCB, we achieve 3% improvement in \noverall accuracy. Finally, we propose a multi-task extension to predict question types which general\nizes QTA to applications that lack question type, with a minimal performance loss.", "cite_num": 5}, "286": {"title": "deep variational metric learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_42", "abstract": "", "cite_num": -1}, "50": {"title": "extending layered models to 3d motion.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_27", "abstract": "", "cite_num": -1}, "237": {"title": "learning data terms for non-blind deblurring.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_46", "abstract": "", "cite_num": -1}, "597": {"title": "real-to-virtual domain unification for end-to-end autonomous driving.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_33", "abstract": "", "cite_num": -1}, "334": {"title": "where are the blobs: counting by localization with point supervision.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_34", "abstract": "", "cite_num": -1}, "63": {"title": "video re-localization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_4", "abstract": "", "cite_num": -1}, "143": {"title": "incremental multi-graph matching via diversity and randomness based graph clustering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_9", "abstract": "", "cite_num": -1}, "753": {"title": "face de-spoofing: anti-spoofing via noise modeling.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_18", "abstract": "", "cite_num": -1}, "412": {"title": "3d recurrent neural networks with context fusion for point cloud semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_25", "abstract": "", "cite_num": -1}, "558": {"title": "monocular depth estimation using whole strip masking and reliability-based refinement.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_3", "abstract": "", "cite_num": -1}, "208": {"title": "implicit 3d orientation learning for 6d object detection from rgb images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_43", "abstract": "", "cite_num": -1}, "734": {"title": "maskconnect: connectivity learning by gradient descent.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_22", "abstract": "", "cite_num": -1}, "516": {"title": "bisenet: bilateral segmentation network for real-time semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_20", "abstract": "", "cite_num": -1}, "649": {"title": "learning to zoom: a saliency-based sampling layer for neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_4", "abstract": "", "cite_num": -1}, "513": {"title": "deeptam: deep tracking and mapping.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_50", "abstract": "", "cite_num": -1}, "551": {"title": "efficient semantic scene completion network with spatial group convolution.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_45", "abstract": "", "cite_num": -1}, "364": {"title": "a style-aware content loss for real-time hd style transfer.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_43", "abstract": "", "cite_num": -1}, "345": {"title": "visual question generation for class acquisition of unknown objects.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_30", "abstract": "", "cite_num": -1}, "190": {"title": "gridface: face rectification via learning local homography transformations.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_1", "abstract": "", "cite_num": -1}, "600": {"title": "rcaa: relational context-aware agents for person search.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_6", "abstract": "", "cite_num": -1}, "654": {"title": "salient objects in clutter: bringing salient object detection to the foreground.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_12", "abstract": "", "cite_num": -1}, "651": {"title": "neural procedural reconstruction for residential buildings.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_45", "abstract": "", "cite_num": -1}, "615": {"title": "improving generalization via scalable neighborhood component analysis.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_42", "abstract": "", "cite_num": -1}, "107": {"title": "small-scale pedestrian detection based on topological line localization and temporal feature aggregation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_33", "abstract": "", "cite_num": -1}, "302": {"title": "massively parallel video networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_40", "abstract": "", "cite_num": -1}, "171": {"title": "cplanet: enhancing image geolocalization by combinatorial partitioning of maps.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_33", "abstract": "", "cite_num": -1}, "333": {"title": "espnet: efficient spatial pyramid of dilated convolutions for semantic segmentation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_34", "abstract": "", "cite_num": -1}, "308": {"title": "proxy clouds for live rgb-d stream processing and consolidation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_16", "abstract": "", "cite_num": -1}, "510": {"title": "estimating the success of unsupervised image to image translation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_14", "abstract": "", "cite_num": -1}, "695": {"title": "into the twilight zone: depth estimation using joint structure-stereo optimization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_7", "abstract": "", "cite_num": -1}, "411": {"title": "deep component analysis via alternating direction neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_50", "abstract": "", "cite_num": -1}, "579": {"title": "psanet: point-wise spatial attention network for scene parsing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_17", "abstract": "We notice information flow in convolutional neural networks is restricted inside local neighborhood \nregions due to the physical design of convolutional filters, which limits the overall understanding \nof complex scenes. In this paper, we propose the point-wise spatial attention network (PSANet) to re\nlax the local neighborhood constraint. Each position on the feature map is connected to all the othe\nr ones through a self-adaptively learned attention mask. Moreover, information propagation in bi-dir\nection for scene parsing is enabled. Information at other positions can be collected to help the pre\ndiction of the current position and vice versa, information at the current position can be distribut\ned to assist the prediction of other ones. Our proposed approach achieves top performance on various\n competitive scene parsing datasets, including ADE20K, PASCAL VOC 2012 and Cityscapes, demonstrating\n its effectiveness and generality.", "cite_num": -1}, "307": {"title": "memory aware synapses: learning what (not) to forget.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_9", "abstract": "", "cite_num": -1}, "174": {"title": "deep reinforcement learning with iterative shift for visual tracking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_42", "abstract": "", "cite_num": -1}, "366": {"title": "handmap: robust hand pose estimation via intermediate dense guidance map supervision.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_15", "abstract": "", "cite_num": -1}, "164": {"title": "robust optical flow in rainy scenes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_18", "abstract": "", "cite_num": -1}, "89": {"title": "propagating lstm: 3d pose estimation based on joint interdependency.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_8", "abstract": "", "cite_num": -1}, "77": {"title": "jointly discovering visual objects and spoken words from raw sensory input.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_40", "abstract": "", "cite_num": -1}, "760": {"title": "elegant: exchanging latent encodings with gan for transferring multiple face attributes.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_11", "abstract": "", "cite_num": -1}, "153": {"title": "hand pose estimation via latent 2.5d heatmap regression.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_8", "abstract": "", "cite_num": -1}, "620": {"title": "cross-modal and hierarchical modeling of video and text.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_23", "abstract": "", "cite_num": -1}, "374": {"title": "generalized loss-sensitive adversarial learning with manifold margins.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_6", "abstract": "", "cite_num": -1}, "496": {"title": "zoom-net: mining deep feature interactions for visual relationship recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_20", "abstract": "", "cite_num": -1}, "664": {"title": "start, follow, read: end-to-end full-page handwriting recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_23", "abstract": "", "cite_num": -1}, "546": {"title": "cbam: convolutional block attention module.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_1", "abstract": "We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for \nfeed-forward convolutional neural networks. Given an intermediate feature map, our module sequential\nly infers attention maps along two separate dimensions, channel and spatial, then the attention maps\n are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightwei\nght and general module, it can be integrated into any CNN architectures seamlessly with negligible o\nverheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive ex\nperiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show c\nonsistent improvements in classification and detection performances with various models, demonstrati\nng the wide applicability of CBAM. The code and models will be publicly available.", "cite_num": 103}, "229": {"title": "learning warped guidance for blind face restoration.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_17", "abstract": "", "cite_num": -1}, "416": {"title": "fully-convolutional point networks for large-scale point clouds.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_37", "abstract": "", "cite_num": -1}, "188": {"title": "materials for masses: svbrdf acquisition with a single mobile phone image.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_5", "abstract": "", "cite_num": -1}, "637": {"title": "deep cross-modality adaptation via semantics preserving adversarial learning for sketch-based 3d shape retrieval.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_37", "abstract": "", "cite_num": -1}, "137": {"title": "domain adaptation through synthesis for unsupervised person re-identification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_12", "abstract": "", "cite_num": -1}, "24": {"title": "straight to the facts: learning knowledge base retrieval for factual visual question answering.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_28", "abstract": "", "cite_num": -1}, "587": {"title": "multi-class model fitting by energy minimization and mode-seeking.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01270-0_14", "abstract": "", "cite_num": -1}, "246": {"title": "neural stereoscopic image style transfer.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_4", "abstract": "", "cite_num": -1}, "485": {"title": "dft-based transformation invariant pooling layer for visual classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01264-9_6", "abstract": "", "cite_num": -1}, "331": {"title": "videos as space-time region graphs.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_25", "abstract": "", "cite_num": -1}, "81": {"title": "swapnet: image based garment transfer.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_41", "abstract": "", "cite_num": -1}, "482": {"title": "parallel feature pyramid network for object detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_15", "abstract": "", "cite_num": -1}, "452": {"title": "deep bilinear learning for rgb-d action recognition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_21", "abstract": "", "cite_num": -1}, "125": {"title": "busternet: detecting copy-move image forgery with source/target localization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_11", "abstract": "", "cite_num": -1}, "253": {"title": "attentive semantic alignment with offset-aware correlation kernels.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_22", "abstract": "", "cite_num": -1}, "353": {"title": "rendering portraitures from monocular camera and beyond.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_3", "abstract": "", "cite_num": -1}, "448": {"title": "fast and accurate camera covariance computation for large 3d reconstruction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_42", "abstract": "", "cite_num": -1}, "503": {"title": "refocusgan: scene refocusing using a single image.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_31", "abstract": "", "cite_num": -1}, "395": {"title": "diverse feature visualizations reveal invariances in early layers of deep neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_14", "abstract": "", "cite_num": -1}, "318": {"title": "robust anchor embedding for unsupervised video person re-identification in the wild.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_11", "abstract": "", "cite_num": -1}, "689": {"title": "diagnosing error in temporal action detectors.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_16", "abstract": "", "cite_num": -1}, "128": {"title": "icnet for real-time semantic segmentation on high-resolution images.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01219-9_25", "abstract": "", "cite_num": -1}, "363": {"title": "sidekick policy learning for active visual exploration.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_26", "abstract": "", "cite_num": -1}, "486": {"title": "learning so(3) equivariant representations with spherical cnns.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_4", "abstract": "", "cite_num": -1}, "157": {"title": "what do i annotate next? an empirical study of active learning for action localization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_13", "abstract": "", "cite_num": -1}, "694": {"title": "learning visual question answering by bootstrapping hard attention.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_1", "abstract": "Attention mechanisms in biological perception are thought to select subsets of perceptual informatio\nn for more sophisticated processing which would be prohibitive to perform on all sensory inputs. In \ncomputer vision, however, there has been relatively little exploration of hard attention, where some\n information is selectively ignored, in spite of the success of soft attention, where information is\n re-weighted and aggregated, but never filtered out. Here, we introduce a new approach for hard atte\nntion and find it achieves very competitive performance on a recently-released visual question answe\nring datasets, equalling and in some cases surpassing similar soft attention architectures while ent\nirely ignoring some features. Even though the hard attention mechanism is thought to be non-differen\ntiable, we found that the feature magnitudes correlate with semantic relevance, and provide a useful\n signal for our mechanism\u2019s attentional selection criterion. Because hard attention selects importan\nt features of the input information, it can also be more efficient than analogous soft attention mec\nhanisms. This is especially important for recent approaches that use non-local pairwise operations, \nwhereby computational and memory costs are quadratic in the size of the set of features.", "cite_num": 9}, "415": {"title": "deep high dynamic range imaging with large foreground motions.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_8", "abstract": "", "cite_num": -1}, "738": {"title": "df-net: unsupervised joint learning of depth and flow using cross-task consistency.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01228-1_3", "abstract": "", "cite_num": -1}, "370": {"title": "graph r-cnn for scene graph generation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_41", "abstract": "", "cite_num": -1}, "95": {"title": "coreset-based neural network compression.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_28", "abstract": "", "cite_num": -1}, "431": {"title": "hybridnet: classification and reconstruction cooperation for semi-supervised learning.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_10", "abstract": "", "cite_num": -1}, "289": {"title": "direct sparse odometry with rolling shutter.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_42", "abstract": "", "cite_num": -1}, "429": {"title": "spatio-temporal channel correlation networks for action classification.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_18", "abstract": "", "cite_num": -1}, "698": {"title": "audio-visual scene analysis with self-supervised multisensory features.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01231-1_39", "abstract": "", "cite_num": -1}, "221": {"title": "adding attentiveness to the neurons in recurrent neural networks.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_9", "abstract": "", "cite_num": -1}, "70": {"title": "estimating depth from rgb and sparse sensing.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_11", "abstract": "", "cite_num": -1}, "86": {"title": "eliminating the blind spot: adapting 3d object detection and monocular depth estimation to 360 ^\\circ \u2218 panoramic imagery.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01261-8_48", "abstract": "", "cite_num": -1}, "630": {"title": "hybridfusion: real-time performance capture using a single depth sensor and sparse imus.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_24", "abstract": "", "cite_num": -1}, "685": {"title": "perturbation robust representations of topological persistence diagrams.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_38", "abstract": "", "cite_num": -1}, "84": {"title": "zero-annotation object detection with web knowledge transfer.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_23", "abstract": "", "cite_num": -1}, "463": {"title": "statistically-motivated second-order pooling.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_37", "abstract": "", "cite_num": -1}, "7": {"title": "unsupervised geometry-aware representation for 3d human pose estimation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_46", "abstract": "", "cite_num": -1}, "534": {"title": "neural network encapsulation.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_16", "abstract": "", "cite_num": -1}, "607": {"title": "riemannian walk for incremental learning: understanding forgetting and intransigence.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_33", "abstract": "", "cite_num": -1}, "521": {"title": "robust image stitching with multiple registrations.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01216-8_4", "abstract": "", "cite_num": -1}, "687": {"title": "netadapt: platform-aware neural network adaptation for mobile applications.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01249-6_18", "abstract": "", "cite_num": -1}, "55": {"title": "concept mask: large-scale segmentation from semantic concepts.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01258-8_33", "abstract": "", "cite_num": -1}, "149": {"title": "video object segmentation by learning location-sensitive embeddings.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_31", "abstract": "", "cite_num": -1}, "491": {"title": "variable ring light imaging: capturing transient subsurface scattering with an ordinary camera.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01252-6_37", "abstract": "", "cite_num": -1}, "749": {"title": "predicting gaze in egocentric video by learning task-dependent attention transition.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_46", "abstract": "We present a new computational model for gaze prediction in egocentric videos by exploring patterns \nin temporal shift of gaze fixations (attention transition) that are dependent on egocentric manipula\ntion tasks. Our assumption is that the high-level context of how a task is completed in a certain wa\ny has a strong influence on attention transition and should be modeled for gaze prediction in natura\nl dynamic scenes. Specifically, we propose a hybrid model based on deep neural networks which integr\nates task-dependent attention transition with bottom-up saliency prediction. In particular, the task\n-dependent attention transition is learned with a recurrent neural network to exploit the temporal c\nontext of gaze fixations, e.g. looking at a cup after moving gaze away from a grasped bottle. Experi\nments on public egocentric activity datasets show that our model significantly outperforms state-of-\nthe-art gaze prediction methods and is able to learn meaningful transition of human attention.", "cite_num": 9}, "774": {"title": "adversarial geometry-aware human motion prediction.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_48", "abstract": "", "cite_num": -1}, "312": {"title": "fast and accurate intrinsic symmetry detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_26", "abstract": "", "cite_num": -1}, "41": {"title": "gray-box adversarial training.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01267-0_13", "abstract": "", "cite_num": -1}, "379": {"title": "how local is the local diversity? reinforcing sequential determinantal point processes with dynamic ground sets for supervised video summarization.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01237-3_10", "abstract": "", "cite_num": -1}, "549": {"title": "normalized blind deconvolution.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01234-2_41", "abstract": "", "cite_num": -1}, "675": {"title": "learning to anonymize faces for privacy preserving action detection.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01246-5_38", "abstract": "", "cite_num": -1}, "611": {"title": "vso: visual semantic odometry.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01225-0_15", "abstract": "", "cite_num": -1}, "141": {"title": "a dataset of flash and ambient illumination pairs from the crowd.", "conf": "eccv", "time": "2018", "url": "https://doi.org/10.1007/978-3-030-01240-3_39", "abstract": "", "cite_num": -1}}
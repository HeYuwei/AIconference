{"191": {"title": "learning independent object motion from unlabelled stereoscopic videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cao_Learning_Independent_Object_Motion_From_Unlabelled_Stereoscopic_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "235": {"title": "scratchdet: training single-shot object detectors from scratch", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_ScratchDet_Training_Single-Shot_Object_Detectors_From_Scratch_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "475": {"title": "motion estimation of non-holonomic ground vehicles from a single feature correspondence measured over n views", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_Motion_Estimation_of_Non-Holonomic_Ground_Vehicles_From_a_Single_Feature_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "778": {"title": "content-aware multi-level guidance for interactive instance segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Majumder_Content-Aware_Multi-Level_Guidance_for_Interactive_Instance_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "616": {"title": "object-aware aggregation with bidirectional temporal graph for video captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Object-Aware_Aggregation_With_Bidirectional_Temporal_Graph_for_Video_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "889": {"title": "few-shot adaptive faster r-cnn", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "284": {"title": "image-question-answer synergistic network for visual dialog", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_Image-Question-Answer_Synergistic_Network_for_Visual_Dialog_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "8": {"title": "latent filter scaling for multimodal unsupervised image-to-image translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Alharbi_Latent_Filter_Scaling_for_Multimodal_Unsupervised_Image-To-Image_Translation_CVPR_2019_paper.html", "abstract": "In multimodal unsupervised image-to-image translation tasks, the goal is to translate an image from \nthe source domain to many images in the target domain. We present a simple method that produces high\ner quality images than current state-of-the-art while maintaining the same amount of multimodal dive\nrsity. Previous methods follow the unconditional approach of trying to map the latent code directly \nto a full-size image. This leads to complicated network architectures with several introduced hyperp\narameters to tune. By treating the latent code as a modifier of the convolutional filters, we produc\ne multimodal output while maintaining the traditional Generative Adversarial Network (GAN) loss and \nwithout additional hyperparameters. The only tuning required by our method controls the tradeoff bet\nween variability and quality of generated images. Furthermore, we achieve disentanglement between so\nurce domain content and target domain style for free as a by-product of our formulation. We perform \nqualitative and quantitative experiments showing the advantages of our method compared with the stat\ne-of-the art on multiple benchmark image-to-image translation datasets.", "cite_num": 1}, "18": {"title": "data augmentation using learned transformations for one-shot medical image segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Data_Augmentation_Using_Learned_Transformations_for_One-Shot_Medical_Image_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1039": {"title": "planercnn: 3d plane detection and reconstruction from a single image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_PlaneRCNN_3D_Plane_Detection_and_Reconstruction_From_a_Single_Image_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "816": {"title": "structured knowledge distillation for semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "393": {"title": "a late fusion cnn for digital matting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_A_Late_Fusion_CNN_for_Digital_Matting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1042": {"title": "learning context graph for person search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_Learning_Context_Graph_for_Person_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "419": {"title": "cross-classification clustering: an efficient multi-object tracking technique for 3-d instance segmentation in connectomics", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Meirovitch_Cross-Classification_Clustering_An_Efficient_Multi-Object_Tracking_Technique_for_3-D_Instance_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "110": {"title": "a neural temporal model for human motion prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gopalakrishnan_A_Neural_Temporal_Model_for_Human_Motion_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "35": {"title": "towards robust curve text detection with conditional spatial expansion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Towards_Robust_Curve_Text_Detection_With_Conditional_Spatial_Expansion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1142": {"title": "toward realistic image compositing with adversarial learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Toward_Realistic_Image_Compositing_With_Adversarial_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "450": {"title": "learning instance activation maps for weakly supervised instance segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Learning_Instance_Activation_Maps_for_Weakly_Supervised_Instance_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "241": {"title": "region proposal by guided anchoring", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Region_Proposal_by_Guided_Anchoring_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "668": {"title": "learning to sample", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dovrat_Learning_to_Sample_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "441": {"title": "adaptive pyramid context network for semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Adaptive_Pyramid_Context_Network_for_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "945": {"title": "partial order pruning: for best speed/accuracy trade-off in neural architecture search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Partial_Order_Pruning_For_Best_SpeedAccuracy_Trade-Off_in_Neural_Architecture_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1008": {"title": "combinatorial persistency criteria for multicut and max-cut", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lange_Combinatorial_Persistency_Criteria_for_Multicut_and_Max-Cut_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1199": {"title": "x2ct-gan: reconstructing ct from biplanar x-rays with generative adversarial networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ying_X2CT-GAN_Reconstructing_CT_From_Biplanar_X-Rays_With_Generative_Adversarial_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "624": {"title": "dissimilarity coefficient based weakly supervised object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Arun_Dissimilarity_Coefficient_Based_Weakly_Supervised_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "474": {"title": "pointconv: deep convolutional networks on 3d point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_PointConv_Deep_Convolutional_Networks_on_3D_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "361": {"title": "decoders matter for semantic segmentation: data-dependent decoding enables flexible feature aggregation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tian_Decoders_Matter_for_Semantic_Segmentation_Data-Dependent_Decoding_Enables_Flexible_Feature_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1260": {"title": "horizonnet: learning room layout with 1d representation and pano stretch data augmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_HorizonNet_Learning_Room_Layout_With_1D_Representation_and_Pano_Stretch_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "716": {"title": "single-frame regularization for temporally stable cnns", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Eilertsen_Single-Frame_Regularization_for_Temporally_Stable_CNNs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "147": {"title": "neural task graphs: generalizing to unseen tasks from a single video demonstration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_Neural_Task_Graphs_Generalizing_to_Unseen_Tasks_From_a_Single_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "189": {"title": "triply supervised decoder networks for joint detection and segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cao_Triply_Supervised_Decoder_Networks_for_Joint_Detection_and_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "346": {"title": "self-supervised 3d hand pose estimation through training by fitting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wan_Self-Supervised_3D_Hand_Pose_Estimation_Through_Training_by_Fitting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "249": {"title": "a robust local spectral descriptor for matching non-rigid shapes with incompatible shape structures", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_A_Robust_Local_Spectral_Descriptor_for_Matching_Non-Rigid_Shapes_With_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "169": {"title": "geometry-consistent generative adversarial networks for one-sided unsupervised domain mapping", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Geometry-Consistent_Generative_Adversarial_Networks_for_One-Sided_Unsupervised_Domain_Mapping_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "67": {"title": "actively seeking and learning from live data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Teney_Actively_Seeking_and_Learning_From_Live_Data_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1147": {"title": "learning to learn how to learn: self-adaptive visual navigation using meta-learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wortsman_Learning_to_Learn_How_to_Learn_Self-Adaptive_Visual_Navigation_Using_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "313": {"title": "image super-resolution by neural texture transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Image_Super-Resolution_by_Neural_Texture_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "842": {"title": "fml: face model learning from videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tewari_FML_Face_Model_Learning_From_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "933": {"title": "detection based defense against adversarial examples from the steganalysis point of view", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Detection_Based_Defense_Against_Adversarial_Examples_From_the_Steganalysis_Point_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "113": {"title": "using unknown occluders to recover hidden scenes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yedidia_Using_Unknown_Occluders_to_Recover_Hidden_Scenes_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1178": {"title": "neural scene decomposition for multi-person motion capture", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rhodin_Neural_Scene_Decomposition_for_Multi-Person_Motion_Capture_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "4": {"title": "learning regularity in skeleton trajectories for anomaly detection in videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Morais_Learning_Regularity_in_Skeleton_Trajectories_for_Anomaly_Detection_in_Videos_CVPR_2019_paper.html", "abstract": "Appearance features have been widely used in video anomaly detection even though they contain comple\nx entangled factors. We propose a new method to model the normal patterns of human movements in surv\neillance video for anomaly detection using dynamic skeleton features. We decompose the skeletal move\nments into two sub-components: global body movement and local body posture. We model the dynamics an\nd interaction of the coupled features in our novel Message-Passing Encoder-Decoder Recurrent Network\n. We observed that the decoupled features collaboratively interact in our spatio-temporal model to a\nccurately identify human-related irregular events from surveillance video sequences. Compared to tra\nditional appearance-based models, our method achieves superior outlier detection performance. Our mo\ndel also offers \"open-box\" examination and decision explanation made possible by the semantically un\nderstandable features and a network architecture supporting interpretability.\r", "cite_num": 0}, "1227": {"title": "r3 adversarial network for cross model face recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_R3_Adversarial_Network_for_Cross_Model_Face_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1255": {"title": "revisiting local descriptor based image-to-class measure for few-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Revisiting_Local_Descriptor_Based_Image-To-Class_Measure_for_Few-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1096": {"title": "deep spectral clustering using dual autoencoder network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Deep_Spectral_Clustering_Using_Dual_Autoencoder_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "194": {"title": "cross-modal relationship inference for grounding referring expressions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Cross-Modal_Relationship_Inference_for_Grounding_Referring_Expressions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "789": {"title": "what and how well you performed? a multitask learning approach to action quality assessment", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Parmar_What_and_How_Well_You_Performed_A_Multitask_Learning_Approach_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "576": {"title": "a simple baseline for audio-visual scene-aware dialog", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Schwartz_A_Simple_Baseline_for_Audio-Visual_Scene-Aware_Dialog_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1175": {"title": "an iterative and cooperative top-down and bottom-up inference network for salient object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_An_Iterative_and_Cooperative_Top-Down_and_Bottom-Up_Inference_Network_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1226": {"title": "two-stream adaptive graph convolutional networks for skeleton-based action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "201": {"title": "sixray: a large-scale security inspection x-ray benchmark for prohibited item discovery in overlapping images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Miao_SIXray_A_Large-Scale_Security_Inspection_X-Ray_Benchmark_for_Prohibited_Item_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "504": {"title": "2.5d visual sound", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_2.5D_Visual_Sound_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1057": {"title": "homomorphic latent space interpolation for unpaired image-to-image translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Homomorphic_Latent_Space_Interpolation_for_Unpaired_Image-To-Image_Translation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "124": {"title": "gradient matching generative networks for zero-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sariyildiz_Gradient_Matching_Generative_Networks_for_Zero-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1101": {"title": "uniformface: learning deep equidistributed representation for face recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Duan_UniformFace_Learning_Deep_Equidistributed_Representation_for_Face_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "790": {"title": "panoptic feature pyramid networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kirillov_Panoptic_Feature_Pyramid_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1048": {"title": "bringing a blurry frame alive at high frame-rate with an event camera", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pan_Bringing_a_Blurry_Frame_Alive_at_High_Frame-Rate_With_an_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1214": {"title": "eliminating exposure bias and metric mismatch in multiple object tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Maksai_Eliminating_Exposure_Bias_and_Metric_Mismatch_in_Multiple_Object_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "586": {"title": "learning for single-shot confidence calibration in deep neural networks through stochastic inferences", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Seo_Learning_for_Single-Shot_Confidence_Calibration_in_Deep_Neural_Networks_Through_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1040": {"title": "noise-tolerant paradigm for training face recognition cnns", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Noise-Tolerant_Paradigm_for_Training_Face_Recognition_CNNs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1158": {"title": "foreground-aware image inpainting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xiong_Foreground-Aware_Image_Inpainting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "629": {"title": "learning to reconstruct people in clothing from a single rgb camera", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Alldieck_Learning_to_Reconstruct_People_in_Clothing_From_a_Single_RGB_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "498": {"title": "iterative reorganization with weak spatial constraints: solving arbitrary jigsaw puzzles for unsupervised representation learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wei_Iterative_Reorganization_With_Weak_Spatial_Constraints_Solving_Arbitrary_Jigsaw_Puzzles_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "509": {"title": "two body problem: collaborative visual task completion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jain_Two_Body_Problem_Collaborative_Visual_Task_Completion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "931": {"title": "modeling point clouds with self-attention and gumbel subset sampling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Modeling_Point_Clouds_With_Self-Attention_and_Gumbel_Subset_Sampling_CVPR_2019_paper.html", "abstract": "Geometric deep learning is increasingly important thanks to the popularity of 3D sensors. Inspired b\ny the recent advances in NLP domain, the self-attention transformer is introduced to consume the poi\nnt clouds. We develop Point Attention Transformers (PATs), using a parameter-efficient Group Shuffle\n Attention (GSA) to replace the costly Multi-Head Attention. We demonstrate its ability to process s\nize-varying inputs, and prove its permutation equivariance. Besides, prior work uses heuristics depe\nndence on the input data (e.g., Furthest Point Sampling) to hierarchically select subsets of input p\noints. Thereby, we for the first time propose an end-to-end learnable and task-agnostic sampling ope\nration, named Gumbel Subset Sampling (GSS), to select a representative subset of input points. Equip\nped with Gumbel-Softmax, it produces a \"soft\" continuous subset in training phase, and a \"hard\" disc\nrete subset in test phase. By selecting representative subsets in a hierarchical fashion, the networ\nks learn a stronger representation of the input sets with lower computation cost. Experiments on cla\nssification and segmentation benchmarks show the effectiveness and efficiency of our methods. Furthe\nrmore, we propose a novel application, to process event camera stream as point clouds, and achieve a\n state-of-the-art performance on DVS128 Gesture Dataset.", "cite_num": 0}, "56": {"title": "deep virtual networks for memory efficient inference of multiple tasks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Deep_Virtual_Networks_for_Memory_Efficient_Inference_of_Multiple_Tasks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "369": {"title": "learning linear transformations for fast image and video style transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_Linear_Transformations_for_Fast_Image_and_Video_Style_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "892": {"title": "disentangling adversarial robustness and generalization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Stutz_Disentangling_Adversarial_Robustness_and_Generalization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1034": {"title": "improving semantic segmentation via video propagation and label relaxation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Improving_Semantic_Segmentation_via_Video_Propagation_and_Label_Relaxation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1274": {"title": "roi-10d: monocular lifting of 2d detection to 6d pose and metric shape", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Manhardt_ROI-10D_Monocular_Lifting_of_2D_Detection_to_6D_Pose_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1210": {"title": "exploiting kernel sparsity and entropy for interpretable cnn compression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Exploiting_Kernel_Sparsity_and_Entropy_for_Interpretable_CNN_Compression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1129": {"title": "semantic image synthesis with spatially-adaptive normalization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "378": {"title": "generative dual adversarial network for generalized zero-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_Generative_Dual_Adversarial_Network_for_Generalized_Zero-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "860": {"title": "roi pooled correlation filters for visual tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_ROI_Pooled_Correlation_Filters_for_Visual_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "961": {"title": "low-rank tensor completion with a new tensor nuclear norm induced by invertible linear transforms", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lu_Low-Rank_Tensor_Completion_With_a_New_Tensor_Nuclear_Norm_Induced_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "785": {"title": "multi-level multimodal common semantic space for image-phrase grounding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Akbari_Multi-Level_Multimodal_Common_Semantic_Space_for_Image-Phrase_Grounding_CVPR_2019_paper.html", "abstract": "We address the problem of phrase grounding by learning a multi-level common semantic space shared by\n the textual and visual modalities. We exploit multiple levels of feature maps of a Deep Convolution\nal Neural Network, as well as contextualized word and sentence embeddings extracted from a character\n-based language model. Following dedicated non-linear mappings for visual features at each level, wo\nrd, and sentence embeddings, we obtain multiple instantiations of our common semantic space in which\n comparisons between any target text and the visual content is performed with cosine similarity. We \nguide the model by a multi-level multimodal attention mechanism which outputs attended visual featur\nes at each level. The best level is chosen to be compared with text content for maximizing the perti\nnence scores of image-sentence pairs of the ground truth. Experiments conducted on three publicly av\nailable datasets show significant performance gains (20%-60% relative) over the state-of-the-art in \nphrase localization and set a new performance record on those datasets. We provide a detailed ablati\non study to show the contribution of each element of our approach and release our code on GitHub.\r", "cite_num": 0}, "638": {"title": "crowdpose: efficient crowded scenes pose estimation and a new benchmark", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_CrowdPose_Efficient_Crowded_Scenes_Pose_Estimation_and_a_New_Benchmark_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "928": {"title": "learning to generate synthetic data via compositing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tripathi_Learning_to_Generate_Synthetic_Data_via_Compositing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "409": {"title": "synthesizing environment-aware activities via activity sketches", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liao_Synthesizing_Environment-Aware_Activities_via_Activity_Sketches_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1293": {"title": "parametric noise injection: trainable randomness to improve deep neural network robustness against adversarial attack", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Parametric_Noise_Injection_Trainable_Randomness_to_Improve_Deep_Neural_Network_CVPR_2019_paper.html", "abstract": "Recent development in the field of Deep Learning have exposed the underlying vulnerability of Deep N\neural Network (DNN) against adversarial examples. In image classification, an adversarial example is\n a carefully modified image that is visually imperceptible to the original image but can cause DNN m\nodel to misclassify it. Training the network with Gaussian noise is an effective technique to perfor\nm model regularization, thus improving model robustness against input variation. Inspired by this cl\nassical method, we explore to utilize the regularization characteristic of noise injection to improv\ne DNN's robustness against adversarial attack. In this work, we propose Parametric-Noise-Injection (\nPNI) which involves trainable Gaussian noise injection at each layer on either activation or weights\n through solving the min-max optimization problem, embedded with adversarial training. These paramet\ners are trained explicitly to achieve improved robustness. To the best of our knowledge, this is the\n first work that uses trainable noise injection to improve network robustness against adversarial at\ntacks, rather than manually configuring the injected noise level through cross-validation. The exten\nsive results show that our proposed PNI technique effectively improves the robustness against a vari\nety of powerful white-box and black-box attacks such as PGD, C & W, FGSM, transferable attack and ZO\nO attack. Last but not the least, PNI method improves both clean- and perturbed-data accuracy in com\nparison to the state-of-the-art defense methods, which outperforms current unbroken PGD defense by 1\n.1 % and 6.8 % on clean test data and perturbed test data respectively using Resnet-20 architecture.\n", "cite_num": 4}, "1143": {"title": "dual residual networks leveraging the potential of paired operations for image restoration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Dual_Residual_Networks_Leveraging_the_Potential_of_Paired_Operations_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "332": {"title": "multi-level context ultra-aggregation for stereo matching", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nie_Multi-Level_Context_Ultra-Aggregation_for_Stereo_Matching_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1083": {"title": "weakly supervised complementary parts models for fine-grained image classification from the bottom up", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ge_Weakly_Supervised_Complementary_Parts_Models_for_Fine-Grained_Image_Classification_From_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1116": {"title": "parallel optimal transport gan", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Avraham_Parallel_Optimal_Transport_GAN_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1235": {"title": "generalizable person re-identification by domain-invariant mapping network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Song_Generalizable_Person_Re-Identification_by_Domain-Invariant_Mapping_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "983": {"title": "unsupervised 3d pose estimation with geometric self-supervision", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Unsupervised_3D_Pose_Estimation_With_Geometric_Self-Supervision_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "270": {"title": "modularized textual grounding for counterfactual resilience", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fang_Modularized_Textual_Grounding_for_Counterfactual_Resilience_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "351": {"title": "generalising fine-grained sketch-based image retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pang_Generalising_Fine-Grained_Sketch-Based_Image_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "304": {"title": "animating arbitrary objects via deep motion transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Siarohin_Animating_Arbitrary_Objects_via_Deep_Motion_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "230": {"title": "iterative alignment network for continuous sign language recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pu_Iterative_Alignment_Network_for_Continuous_Sign_Language_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "663": {"title": "aognets: compositional grammatical architectures for deep learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_AOGNets_Compositional_Grammatical_Architectures_for_Deep_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "769": {"title": "beyond volumetric albedo -- a surface optimization framework for non-line-of-sight imaging", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tsai_Beyond_Volumetric_Albedo_--_A_Surface_Optimization_Framework_for_Non-Line-Of-Sight_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "952": {"title": "repnet: weakly supervised training of an adversarial reprojection network for 3d human pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wandt_RepNet_Weakly_Supervised_Training_of_an_Adversarial_Reprojection_Network_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "792": {"title": "a general and adaptive robust loss function", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "841": {"title": "blind geometric distortion correction on images through deep learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Blind_Geometric_Distortion_Correction_on_Images_Through_Deep_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "603": {"title": "occupancy networks: learning 3d reconstruction in function space", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "830": {"title": "structured binary neural networks for accurate image classification and semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhuang_Structured_Binary_Neural_Networks_for_Accurate_Image_Classification_and_Semantic_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1062": {"title": "pyramidal person re-identification via multi-loss dynamic training", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Pyramidal_Person_Re-IDentification_via_Multi-Loss_Dynamic_Training_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "281": {"title": "large-scale distributed second-order optimization using kronecker-factored approximate curvature for deep convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Osawa_Large-Scale_Distributed_Second-Order_Optimization_Using_Kronecker-Factored_Approximate_Curvature_for_Deep_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "336": {"title": "all about structure: adapting structural information across domains for boosting semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_All_About_Structure_Adapting_Structural_Information_Across_Domains_for_Boosting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "693": {"title": "informative object annotations: tell me something i don't know", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bracha_Informative_Object_Annotations_Tell_Me_Something_I_Dont_Know_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "916": {"title": "deep global generalized gaussian networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Deep_Global_Generalized_Gaussian_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "631": {"title": "locating objects without bounding boxes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ribera_Locating_Objects_Without_Bounding_Boxes_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "316": {"title": "focus is all you need: loss functions for event-based vision", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gallego_Focus_Is_All_You_Need_Loss_Functions_for_Event-Based_Vision_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "879": {"title": "zoom-in-to-check: boosting video interpolation via instance-level discrimination", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yuan_Zoom-In-To-Check_Boosting_Video_Interpolation_via_Instance-Level_Discrimination_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "538": {"title": "exploring object relation in mean teacher for cross-domain detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cai_Exploring_Object_Relation_in_Mean_Teacher_for_Cross-Domain_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "258": {"title": "bridgenet: a continuity-aware probabilistic network for age estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_BridgeNet_A_Continuity-Aware_Probabilistic_Network_for_Age_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "420": {"title": "feature space perturbations yield more transferable adversarial examples", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "652": {"title": "depth coefficients for depth completion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Imran_Depth_Coefficients_for_Depth_Completion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "267": {"title": "learning metrics from teachers: compact networks for image embedding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Learning_Metrics_From_Teachers_Compact_Networks_for_Image_Embedding_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "669": {"title": "end-to-end learned random walker for seeded image segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cerrone_End-To-End_Learned_Random_Walker_for_Seeded_Image_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "530": {"title": "inverse procedural modeling of knitwear", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Trunz_Inverse_Procedural_Modeling_of_Knitwear_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "799": {"title": "accelerating convolutional neural networks via activation map compression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Georgiadis_Accelerating_Convolutional_Neural_Networks_via_Activation_Map_Compression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "711": {"title": "ocgan: one-class novelty detection using gans with constrained latent representations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Perera_OCGAN_One-Class_Novelty_Detection_Using_GANs_With_Constrained_Latent_Representations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "88": {"title": "dynamic recursive neural network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_Dynamic_Recursive_Neural_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "481": {"title": "libra r-cnn: towards balanced learning for object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pang_Libra_R-CNN_Towards_Balanced_Learning_for_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "31": {"title": "an attention enhanced graph convolutional lstm network for skeleton-based action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Si_An_Attention_Enhanced_Graph_Convolutional_LSTM_Network_for_Skeleton-Based_Action_CVPR_2019_paper.html", "abstract": "Skeleton-based action recognition is an important task that requires the adequate understanding of m\novement characteristics of a human action from the given skeleton sequence. Recent studies have show\nn that exploring spatial and temporal features of the skeleton sequence is vital for this task. Neve\nrtheless, how to effectively extract discriminative spatial and temporal features is still a challen\nging problem. In this paper, we propose a novel Attention Enhanced Graph Convolutional LSTM Network \n(AGC-LSTM) for human action recognition from skeleton data. The proposed AGC-LSTM can not only captu\nre discriminative features in spatial configuration and temporal dynamics but also explore the co-oc\ncurrence relationship between spatial and temporal domains. We also present a temporal hierarchical \narchitecture to increase temporal receptive fields of the top AGC-LSTM layer, which boosts the abili\nty to learn the high-level semantic representation and significantly reduces the computation cost. F\nurthermore, to select discriminative spatial information, the attention mechanism is employed to enh\nance information of key joints in each AGC-LSTM layer. Experimental results on two datasets are prov\nided: NTU RGB+D dataset and Northwestern-UCLA dataset. The comparison results demonstrate the effect\niveness of our approach and show that our approach outperforms the state-of-the-art methods on both \ndatasets.\r", "cite_num": 2}, "122": {"title": "modeling local geometric structure of 3d point clouds using geo-cnn", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lan_Modeling_Local_Geometric_Structure_of_3D_Point_Clouds_Using_Geo-CNN_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "595": {"title": "quasi-unsupervised color constancy", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bianco_Quasi-Unsupervised_Color_Constancy_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "954": {"title": "fast interactive object annotation with curve-gcn", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ling_Fast_Interactive_Object_Annotation_With_Curve-GCN_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "176": {"title": "learning a deep convnet for multi-label classification with partial labels", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Durand_Learning_a_Deep_ConvNet_for_Multi-Label_Classification_With_Partial_Labels_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "955": {"title": "p3sgd: patient privacy preserving sgd for regularizing deep cnns in pathological image classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_P3SGD_Patient_Privacy_Preserving_SGD_for_Regularizing_Deep_CNNs_in_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "175": {"title": "zoom to learn, learn to zoom", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Zoom_to_Learn_Learn_to_Zoom_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "956": {"title": "wide-area crowd counting via ground-plane density maps and multi-view fusion cnns", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Wide-Area_Crowd_Counting_via_Ground-Plane_Density_Maps_and_Multi-View_Fusion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "869": {"title": "building efficient deep neural networks with unitary group convolutions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Building_Efficient_Deep_Neural_Networks_With_Unitary_Group_Convolutions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1098": {"title": "learning attraction field representation for robust line segment detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xue_Learning_Attraction_Field_Representation_for_Robust_Line_Segment_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "104": {"title": "relational action forecasting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Relational_Action_Forecasting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1252": {"title": "machine vision guided 3d medical image compression for efficient transmission and accurate segmentation in the clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Machine_Vision_Guided_3D_Medical_Image_Compression_for_Efficient_Transmission_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "282": {"title": "leveraging the invariant side of generative zero-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Leveraging_the_Invariant_Side_of_Generative_Zero-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "196": {"title": "nddr-cnn: layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_NDDR-CNN_Layerwise_Feature_Fusing_in_Multi-Task_CNNs_by_Neural_Discriminative_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "114": {"title": "multi-task multi-sensor fusion for 3d object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liang_Multi-Task_Multi-Sensor_Fusion_for_3D_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "39": {"title": "speed invariant time surface for learning to detect corner points with event-based cameras", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Manderscheid_Speed_Invariant_Time_Surface_for_Learning_to_Detect_Corner_Points_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "807": {"title": "deeper and wider siamese networks for real-time visual tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "442": {"title": "factor graph attention", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Schwartz_Factor_Graph_Attention_CVPR_2019_paper.html", "abstract": "Dialog is an effective way to exchange information, but subtle details and nuances are extremely imp\nortant. While significant progress has paved a path   to address visual dialog with algorithms,  det\nails and nuances remain a challenge. Attention mechanisms have demonstrated compelling results to ex\ntract details in visual question answering and also provide a convincing framework for visual dialog\n due to their interpretability and effectiveness. However, the many data utilities that accompany vi\nsual dialog challenge  existing attention techniques. We address this issue and develop a general at\ntention mechanism for visual dialog which operates on any number of data utilities. To this end, we \ndesign a factor graph based attention mechanism which combines any number of utility representations\n. We illustrate the applicability of the proposed approach on the challenging and recently introduce\nd VisDial datasets, outperforming recent state-of-the-art methods by 1.1% for VisDial0.9 and by 2% f\nor VisDial1.0 on MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%. \r", "cite_num": 1}, "1173": {"title": "noise-aware unsupervised deep lidar-stereo fusion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cheng_Noise-Aware_Unsupervised_Deep_Lidar-Stereo_Fusion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1000": {"title": "point cloud oversegmentation with graph-structured deep metric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Landrieu_Point_Cloud_Oversegmentation_With_Graph-Structured_Deep_Metric_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "403": {"title": "deep supervised cross-modal retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhen_Deep_Supervised_Cross-Modal_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "971": {"title": "kervolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Kervolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1207": {"title": "shape robust text detection with progressive scale expansion network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "252": {"title": "soft labels for ordinal regression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Diaz_Soft_Labels_for_Ordinal_Regression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "388": {"title": "lsta: long short-term attention for egocentric action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.html", "abstract": "Egocentric activity recognition is one of the most challenging tasks in video analysis. It requires \na fine-grained discrimination of small objects and their manipulation. While some methods base on st\nrong supervision and attention mechanisms, they are either annotation consuming or do not take spati\no-temporal patterns into account. In this paper we propose LSTA as a mechanism to focus on features \nfrom spatial relevant parts while attention is being tracked smoothly across the video sequence. We \ndemonstrate the effectiveness of LSTA on egocentric activity recognition with an end-to-end trainabl\ne two-stream architecture, achieving state-of-the-art performance on four standard benchmarks.\r", "cite_num": 3}, "840": {"title": "searching for a robust neural architecture in four gpu hours", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Searching_for_a_Robust_Neural_Architecture_in_Four_GPU_Hours_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "718": {"title": "dm-gan: dynamic memory generative adversarial networks for text-to-image synthesis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_DM-GAN_Dynamic_Memory_Generative_Adversarial_Networks_for_Text-To-Image_Synthesis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1161": {"title": "4d spatio-temporal convnets: minkowski convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Choy_4D_Spatio-Temporal_ConvNets_Minkowski_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "470": {"title": "deep incremental hashing network for efficient image retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Deep_Incremental_Hashing_Network_for_Efficient_Image_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "21": {"title": "ray-space projection model for light field camera", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Ray-Space_Projection_Model_for_Light_Field_Camera_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "911": {"title": "learning to film from professional human motion videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_Learning_to_Film_From_Professional_Human_Motion_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "76": {"title": "single image depth estimation trained via depth from defocus cues", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gur_Single_Image_Depth_Estimation_Trained_via_Depth_From_Defocus_Cues_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "712": {"title": "ev-gait: event-based robust gait recognition using dynamic vision sensors", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_EV-Gait_Event-Based_Robust_Gait_Recognition_Using_Dynamic_Vision_Sensors_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1170": {"title": "bad slam: bundle adjusted direct rgb-d slam", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1155": {"title": "side window filtering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yin_Side_Window_Filtering_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "529": {"title": "improving the performance of unimodal dynamic hand-gesture recognition with multimodal training", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Abavisani_Improving_the_Performance_of_Unimodal_Dynamic_Hand-Gesture_Recognition_With_Multimodal_CVPR_2019_paper.html", "abstract": "We present an efficient approach for leveraging the knowledge from multiple modalities in training u\nnimodal 3D convolutional neural networks (3D-CNNs) for the task of dynamic hand gesture recognition.\n Instead of explicitly combining multimodal information, which is commonplace in many state-of-the-a\nrt methods, we propose a different framework in which we embed the knowledge of multiple modalities \nin individual networks so that each unimodal network can achieve an improved performance. In particu\nlar, we dedicate separate networks per available modality and enforce them to collaborate and learn \nto develop networks with common semantics and better representations. We introduce a \"spatiotemporal\n semantic alignment\" loss (SSA) to align the content of the features from different networks. In add\nition, we regularize this loss with our proposed \"focal regularization parameter\" to avoid negative \nknowledge transfer. Experimental results show that our framework improves the test time recognition \naccuracy of unimodal networks, and provides the state-of-the-art performance on various dynamic hand\n gesture recognition datasets.", "cite_num": 1}, "1264": {"title": "spectral metric for dataset complexity assessment", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1277": {"title": "exploring the bounds of the utility of context for object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Barnea_Exploring_the_Bounds_of_the_Utility_of_Context_for_Object_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "671": {"title": "spatiotemporal cnn for video object segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Spatiotemporal_CNN_for_Video_Object_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "264": {"title": "radial distortion triangulation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kukelova_Radial_Distortion_Triangulation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "854": {"title": "compressing convolutional neural networks via factorized convolutional filters", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Compressing_Convolutional_Neural_Networks_via_Factorized_Convolutional_Filters_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "706": {"title": "learning structure-and-motion-aware rolling shutter correction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhuang_Learning_Structure-And-Motion-Aware_Rolling_Shutter_Correction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "105": {"title": "steady-state non-line-of-sight imaging", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Steady-State_Non-Line-Of-Sight_Imaging_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "311": {"title": "leveraging crowdsourced gps data for road extraction from aerial imagery", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Leveraging_Crowdsourced_GPS_Data_for_Road_Extraction_From_Aerial_Imagery_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1215": {"title": "learning video representations from correspondence proposals", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Learning_Video_Representations_From_Correspondence_Proposals_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "315": {"title": "noise2void - learning denoising from single noisy images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Krull_Noise2Void_-_Learning_Denoising_From_Single_Noisy_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1233": {"title": "bubblenets: learning to select the guidance frame in video object segmentation by deep sorting frames", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Griffin_BubbleNets_Learning_to_Select_the_Guidance_Frame_in_Video_Object_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "556": {"title": "heavy rain image restoration: integrating physics model and conditional adversarial learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Heavy_Rain_Image_Restoration_Integrating_Physics_Model_and_Conditional_Adversarial_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "726": {"title": "semantic alignment: finding semantically consistent ground-truth for facial landmark detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Semantic_Alignment_Finding_Semantically_Consistent_Ground-Truth_for_Facial_Landmark_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "798": {"title": "it's not about the journey; it's about the destination: following soft paths under question-guidance for visual reasoning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Haurilet_Its_Not_About_the_Journey_Its_About_the_Destination_Following_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1091": {"title": "instance-level meta normalization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jia_Instance-Level_Meta_Normalization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1045": {"title": "representation flow for action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Piergiovanni_Representation_Flow_for_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1146": {"title": "recurrent neural network for (un-)supervised learning of monocular video visual odometry and depth", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Recurrent_Neural_Network_for_Un-Supervised_Learning_of_Monocular_Video_Visual_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "144": {"title": "knowledge adaptation for efficient semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Knowledge_Adaptation_for_Efficient_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "899": {"title": "probabilistic permutation synchronization using the riemannian structure of the birkhoff polytope", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Birdal_Probabilistic_Permutation_Synchronization_Using_the_Riemannian_Structure_of_the_Birkhoff_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "781": {"title": "apollocar3d: a large 3d car instance understanding benchmark for autonomous driving", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Song_ApolloCar3D_A_Large_3D_Car_Instance_Understanding_Benchmark_for_Autonomous_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "939": {"title": "learning to learn relation for important people detection in still images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_to_Learn_Relation_for_Important_People_Detection_in_Still_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "888": {"title": "you reap what you sow: using videos to generate high precision object proposals for weakly-supervised object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Singh_You_Reap_What_You_Sow_Using_Videos_to_Generate_High_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1052": {"title": "describing like humans: on diversity in image captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Describing_Like_Humans_On_Diversity_in_Image_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "111": {"title": "an end-to-end network for generating social relationship graphs", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Goel_An_End-To-End_Network_for_Generating_Social_Relationship_Graphs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "877": {"title": "attention-guided unified network for panoptic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Attention-Guided_Unified_Network_for_Panoptic_Segmentation_CVPR_2019_paper.html", "abstract": "This paper studies panoptic segmentation, a recently proposed task which segments foreground (FG) ob\njects at the instance level as well as background (BG) contents at the semantic level. Existing meth\nods mostly dealt with these two problems separately, but in this paper, we reveal the underlying rel\nationship between them, in particular, FG objects provide complementary cues to assist BG understand\ning. Our approach, named the Attention-guided Unified Network (AUNet), is a unified framework with t\nwo branches for FG and BG segmentation simultaneously. Two sources of attentions are added to the BG\n branch, namely, RPN and FG segmentation mask to provide object-level and pixel-level attentions, re\nspectively. Our approach is generalized to different backbones with consistent accuracy gain in both\n FG and BG segmentation, and also sets new state-of-the-arts both in the MS-COCO (46.5% PQ) and City\nscapes (59.0% PQ) benchmarks.", "cite_num": 7}, "344": {"title": "strong-weak distribution alignment for adaptive object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Saito_Strong-Weak_Distribution_Alignment_for_Adaptive_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "777": {"title": "on stabilizing generative adversarial training with noise", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jenni_On_Stabilizing_Generative_Adversarial_Training_With_Noise_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1248": {"title": "adaptive weighting multi-field-of-view cnn for semantic segmentation in pathology", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tokunaga_Adaptive_Weighting_Multi-Field-Of-View_CNN_for_Semantic_Segmentation_in_Pathology_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "681": {"title": "learning words by drawing images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Suris_Learning_Words_by_Drawing_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "17": {"title": "a decomposition algorithm for the sparse generalized eigenvalue problem", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yuan_A_Decomposition_Algorithm_for_the_Sparse_Generalized_Eigenvalue_Problem_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "350": {"title": "skin-based identification from multispectral image data using cnns", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Uemori_Skin-Based_Identification_From_Multispectral_Image_Data_Using_CNNs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "477": {"title": "h+o: unified egocentric recognition of 3d hand-object poses and interactions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tekin_HO_Unified_Egocentric_Recognition_of_3D_Hand-Object_Poses_and_Interactions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "547": {"title": "multi-channel attention selection gan with cascaded semantic guidance for cross-view image translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_Multi-Channel_Attention_Selection_GAN_With_Cascaded_Semantic_Guidance_for_Cross-View_CVPR_2019_paper.html", "abstract": "Cross-view image translation is challenging because it involves images with drastically different vi\news and severe deformation. In this paper, we propose a novel approach named Multi-Channel Attention\n SelectionGAN (SelectionGAN) that makes it possible to generate images of natural scenes in arbitrar\ny viewpoints, based on an image of the scene and a novel semantic map. The proposed SelectionGAN exp\nlicitly utilizes the semantic information and consists of two stages. In the first stage, the condit\nion image and the target semantic map are fed into a cycled semantic-guided generation network to pr\noduce initial coarse results. In the second stage, we refine the initial results by using a multi-ch\nannel attention selection mechanism. Moreover, uncertainty maps automatically learned from attention\ns are used to guide the pixel loss for better network optimization. Extensive experiments on Dayton,\n CVUSA and Ego2Top datasets show that our model is able to generate significantly better results tha\nn the state-of-the-art methods. The source code, data and trained models are available at https://gi\nthub.com/Ha0Tang/SelectionGAN.\r", "cite_num": 3}, "845": {"title": "guaranteed matrix completion under multiple linear transformations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Guaranteed_Matrix_Completion_Under_Multiple_Linear_Transformations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1191": {"title": "ecc: platform-independent energy-constrained deep neural network compression via a bilinear regression model", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_ECC_Platform-Independent_Energy-Constrained_Deep_Neural_Network_Compression_via_a_Bilinear_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "82": {"title": "domain-specific batch normalization for unsupervised domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Domain-Specific_Batch_Normalization_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1131": {"title": "on finding gray pixels", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qian_On_Finding_Gray_Pixels_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "501": {"title": "deeplight: learning illumination for unconstrained mobile mixed reality", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/LeGendre_DeepLight_Learning_Illumination_for_Unconstrained_Mobile_Mixed_Reality_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "309": {"title": "pedestrian detection with autoregressive network phases", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "969": {"title": "did it change? learning to detect point-of-interest changes for proactive map updates", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Revaud_Did_It_Change_Learning_to_Detect_Point-Of-Interest_Changes_for_Proactive_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "209": {"title": "vrstc: occlusion-free video person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hou_VRSTC_Occlusion-Free_Video_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1187": {"title": "transferable interactiveness knowledge for human-object interaction detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Transferable_Interactiveness_Knowledge_for_Human-Object_Interaction_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1021": {"title": "interpreting cnns via decision trees", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "729": {"title": "fast and robust multi-person 3d pose estimation from multiple views", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Fast_and_Robust_Multi-Person_3D_Pose_Estimation_From_Multiple_Views_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "319": {"title": "signet: semantic instance aided unsupervised 3d geometry perception", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Meng_SIGNet_Semantic_Instance_Aided_Unsupervised_3D_Geometry_Perception_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "728": {"title": "object tracking by reconstruction with view-specific discriminative correlation filters", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kart_Object_Tracking_by_Reconstruction_With_View-Specific_Discriminative_Correlation_Filters_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1037": {"title": "event-based high dynamic range image and very high frame rate video generation using conditional generative adversarial networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Event-Based_High_Dynamic_Range_Image_and_Very_High_Frame_Rate_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "605": {"title": "hybrid task cascade for instance segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Hybrid_Task_Cascade_for_Instance_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "202": {"title": "improving referring expression grounding with cross-modal attention-guided erasing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Improving_Referring_Expression_Grounding_With_Cross-Modal_Attention-Guided_Erasing_CVPR_2019_paper.html", "abstract": "Referring expression grounding aims at locating certain objects or persons in an image with a referr\ning expression, where the key challenge is to comprehend and align various types of information from\n visual and textual domain, such as visual attributes, location and interactions with surrounding re\ngions. Although the attention mechanism has been successfully applied for cross-modal alignments, pr\nevious attention models focus on only the most dominant features of both modalities, and neglect the\n fact that there could be multiple comprehensive textual-visual correspondences between images and r\neferring expressions. To tackle this issue, we design a novel cross-modal attention-guided erasing a\npproach, where we discard the most dominant information from either textual or visual domains to gen\nerate difficult training samples online, and to drive the model to discover complementary textual-vi\nsual correspondences. Extensive experiments demonstrate the effectiveness of our proposed method, wh\nich achieves state-of-the-art performance on three referring expression grounding datasets.", "cite_num": 2}, "673": {"title": "self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Self-Supervised_Spatio-Temporal_Representation_Learning_for_Videos_by_Predicting_Motion_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "843": {"title": "single image deraining: a comprehensive benchmark analysis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Single_Image_Deraining_A_Comprehensive_Benchmark_Analysis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1035": {"title": "deep high-resolution representation learning for human pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Deep_High-Resolution_Representation_Learning_for_Human_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1153": {"title": "spatial attentive single-image deraining with a high quality real rain dataset", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Spatial_Attentive_Single-Image_Deraining_With_a_High_Quality_Real_Rain_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "398": {"title": "sim-real joint reinforcement transfer for 3d indoor navigation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Sim-Real_Joint_Reinforcement_Transfer_for_3D_Indoor_Navigation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "453": {"title": "associatively segmenting instances and semantics in point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Associatively_Segmenting_Instances_and_Semantics_in_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "539": {"title": "on the continuity of rotation representations in neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_On_the_Continuity_of_Rotation_Representations_in_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "280": {"title": "deep fitting degree scoring network for monocular 3d object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Deep_Fitting_Degree_Scoring_Network_for_Monocular_3D_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "178": {"title": "patch-based progressive 3d point set upsampling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yifan_Patch-Based_Progressive_3D_Point_Set_Upsampling_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "283": {"title": "multiview 2d/3d rigid registration via a point-of-interest network for tracking and triangulation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liao_Multiview_2D3D_Rigid_Registration_via_a_Point-Of-Interest_Network_for_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1115": {"title": "a variational auto-encoder model for stochastic point processes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mehrasa_A_Variational_Auto-Encoder_Model_for_Stochastic_Point_Processes_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "592": {"title": "progressive feature alignment for unsupervised domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Progressive_Feature_Alignment_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1013": {"title": "ssn: learning sparse switchable normalization via sparsestmax", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shao_SSN_Learning_Sparse_Switchable_Normalization_via_SparsestMax_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "585": {"title": "gaussian temporal awareness networks for action localization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Long_Gaussian_Temporal_Awareness_Networks_for_Action_Localization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "424": {"title": "a flexible convolutional solver for fast style transfers", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Puy_A_Flexible_Convolutional_Solver_for_Fast_Style_Transfers_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "48": {"title": "partnet: a large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mo_PartNet_A_Large-Scale_Benchmark_for_Fine-Grained_and_Hierarchical_Part-Level_3D_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1110": {"title": "im2pencil: controllable pencil illustration from photographs", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Im2Pencil_Controllable_Pencil_Illustration_From_Photographs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "914": {"title": "speech2face: learning the face behind a voice", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Oh_Speech2Face_Learning_the_Face_Behind_a_Voice_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "320": {"title": "spherical fractal convolutional neural networks for point cloud recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rao_Spherical_Fractal_Convolutional_Neural_Networks_for_Point_Cloud_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1070": {"title": "lp-3dcnn: unveiling local phase in 3d convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kumawat_LP-3DCNN_Unveiling_Local_Phase_in_3D_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "721": {"title": "eigen: ecologically-inspired genetic approach for neural network structure searching from scratch", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ren_EIGEN_Ecologically-Inspired_GENetic_Approach_for_Neural_Network_Structure_Searching_From_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "203": {"title": "re-identification with consistent attentive siamese networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Re-Identification_With_Consistent_Attentive_Siamese_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "905": {"title": "pms-net: robust haze removal based on patch map for single images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_PMS-Net_Robust_Haze_Removal_Based_on_Patch_Map_for_Single_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1122": {"title": "deep flow-guided video inpainting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Deep_Flow-Guided_Video_Inpainting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "423": {"title": "action recognition from single timestamp supervision in untrimmed videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Moltisanti_Action_Recognition_From_Single_Timestamp_Supervision_in_Untrimmed_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "389": {"title": "shape unicode: a unified shape representation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Muralikrishnan_Shape_Unicode_A_Unified_Shape_Representation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "907": {"title": "learning to learn image classifiers with visual analogy", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_Learning_to_Learn_Image_Classifiers_With_Visual_Analogy_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "223": {"title": "barrage of random transforms for adversarially robust defense", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Raff_Barrage_of_Random_Transforms_for_Adversarially_Robust_Defense_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "609": {"title": "a variational em framework with adaptive edge selection for blind motion deblurring", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_A_Variational_EM_Framework_With_Adaptive_Edge_Selection_for_Blind_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "53": {"title": "deep metric learning beyond binary supervision", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Deep_Metric_Learning_Beyond_Binary_Supervision_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1261": {"title": "attention-based dropout layer for weakly supervised object localization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Choe_Attention-Based_Dropout_Layer_for_Weakly_Supervised_Object_Localization_CVPR_2019_paper.html", "abstract": "Weakly Supervised Object Localization (WSOL) techniques learn the object location only using image-l\nevel labels, without location annotations. A common limitation for these techniques is that they cov\ner only the most discriminative part of the object, not the entire object. To address this problem, \nwe propose an Attention-based Dropout Layer (ADL), which utilizes the self-attention mechanism to pr\nocess the feature maps of the model. The proposed method is composed of two key components: 1) hidin\ng the most discriminative part from the model for capturing the integral extent of object, and 2) hi\nghlighting the informative region for improving the recognition power of the model. Based on extensi\nve experiments, we demonstrate that the proposed method is effective to improve the accuracy of WSOL\n, achieving a new state-of-the-art localization accuracy in CUB-200-2011 dataset. We also show that \nthe proposed method is much more efficient in terms of both parameter and computation overheads than\n existing techniques.\r", "cite_num": 1}, "493": {"title": "generating 3d adversarial point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xiang_Generating_3D_Adversarial_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "870": {"title": "feature transfer learning for face recognition with under-represented data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yin_Feature_Transfer_Learning_for_Face_Recognition_With_Under-Represented_Data_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "216": {"title": "learning from noisy labels by regularized estimation of annotator confusion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tanno_Learning_From_Noisy_Labels_by_Regularized_Estimation_of_Annotator_Confusion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "966": {"title": "relational knowledge distillation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "472": {"title": "gotta adapt 'em all: joint pixel and feature-level domain adaptation for recognition in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tran_Gotta_Adapt_Em_All_Joint_Pixel_and_Feature-Level_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1025": {"title": "rf-net: an end-to-end image matching network based on receptive field", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shen_RF-Net_An_End-To-End_Image_Matching_Network_Based_on_Receptive_Field_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "139": {"title": "local temporal bilinear pooling for fine-grained action parsing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Local_Temporal_Bilinear_Pooling_for_Fine-Grained_Action_Parsing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "967": {"title": "art2real: unfolding the reality of artworks via semantically-aware image-to-image translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tomei_Art2Real_Unfolding_the_Reality_of_Artworks_via_Semantically-Aware_Image-To-Image_Translation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "443": {"title": "kernel transformer networks for compact spherical convolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Su_Kernel_Transformer_Networks_for_Compact_Spherical_Convolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "301": {"title": "multi-task learning of hierarchical vision-language representation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nguyen_Multi-Task_Learning_of_Hierarchical_Vision-Language_Representation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "748": {"title": "local features and visual words emerge in activations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Simeoni_Local_Features_and_Visual_Words_Emerge_in_Activations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "445": {"title": "class-balanced loss based on effective number of samples", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "856": {"title": "tangent-normal adversarial regularization for semi-supervised learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Tangent-Normal_Adversarial_Regularization_for_Semi-Supervised_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1056": {"title": "peeking into the future: predicting future person activities and locations in videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liang_Peeking_Into_the_Future_Predicting_Future_Person_Activities_and_Locations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "43": {"title": "scalable convolutional neural network for image compressed sensing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Scalable_Convolutional_Neural_Network_for_Image_Compressed_Sensing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "985": {"title": "fast neural architecture search of compact semantic segmentation models via auxiliary cells", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nekrasov_Fast_Neural_Architecture_Search_of_Compact_Semantic_Segmentation_Models_via_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "757": {"title": "pattern-affinitive propagation across depth, surface normal and semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Pattern-Affinitive_Propagation_Across_Depth_Surface_Normal_and_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1064": {"title": "how to make a pizza: learning a compositional layer-based gan model", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Papadopoulos_How_to_Make_a_Pizza_Learning_a_Compositional_Layer-Based_GAN_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1138": {"title": "deepcaps: going deeper with capsule networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rajasegaran_DeepCaps_Going_Deeper_With_Capsule_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "837": {"title": "crdoco: pixel-level domain transfer with cross-domain consistency", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_CrDoCo_Pixel-Level_Domain_Transfer_With_Cross-Domain_Consistency_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "46": {"title": "dual attention network for scene segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html", "abstract": "In this paper, we address the scene segmentation task by capturing rich contextual dependencies base\nd on the selfattention mechanism. Unlike previous works that capture contexts by multi-scale feature\ns fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with t\nheir global dependencies. Specifically, we append two types of attention modules on top of tradition\nal dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respect\nively. The position attention module selectively aggregates the features at each position by a weigh\nted sum of the features at all positions. Similar features would be related to each other regardless\n of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent c\nhannel maps by integrating associated features among all channel maps. We sum the outputs of the two\n attention modules to further improve feature representation which contributes to more precise segme\nntation results. We achieve new state-of-the-art segmentation performance on three challenging scene\n segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Me\nan IoU score of 81.5% on Cityscapes test set is achieved without using coarse data. We make the code\n and trained model publicly available at this https URL", "cite_num": 27}, "1086": {"title": "tell me where i am: object-level scene context prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qiao_Tell_Me_Where_I_Am_Object-Level_Scene_Context_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "399": {"title": "attribute-aware face aging with wavelet-based generative adversarial networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Attribute-Aware_Face_Aging_With_Wavelet-Based_Generative_Adversarial_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "427": {"title": "learning to localize through compressed binary maps", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wei_Learning_to_Localize_Through_Compressed_Binary_Maps_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "0": {"title": "mvtec ad -- a comprehensive real-world dataset for unsupervised anomaly detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.html", "abstract": "The detection of anomalous structures in natural image data is of utmost importance for numerous tas\nks in the field of computer vision. The development of methods for unsupervised anomaly detection re\nquires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly \nDetection (MVTec AD) dataset containing 5354 high-resolution color images of different object and te\nxture categories. It contains normal, i.e., defect-free, images intended for training and images wit\nh anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different\n types of defects such as scratches, dents, contaminations, and various structural changes. In addit\nion, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough eva\nluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectur\nes such as convolutional autoencoders, generative adversarial networks, and feature descriptors usin\ng pre-trained convolutional neural networks, as well as classical computer vision methods. This init\nial benchmark indicates that there is considerable room for improvement. To the best of our knowledg\ne, this is the first comprehensive, multi-object, multi-defect dataset for anomaly detection that pr\novides pixel-accurate ground truth regions and focuses on real-world applications. \r", "cite_num": 0}, "200": {"title": "deep embedding learning with discriminative sampling policy", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Duan_Deep_Embedding_Learning_With_Discriminative_Sampling_Policy_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "421": {"title": "contrast prior and fluid pyramid integration for rgbd salient object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Contrast_Prior_and_Fluid_Pyramid_Integration_for_RGBD_Salient_Object_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "305": {"title": "end-to-end efficient representation learning via cascading combinatorial optimization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jeong_End-To-End_Efficient_Representation_Learning_via_Cascading_Combinatorial_Optimization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "447": {"title": "3d hand shape and pose estimation from a single rgb image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ge_3D_Hand_Shape_and_Pose_Estimation_From_a_Single_RGB_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "437": {"title": "local detection of stereo occlusion boundaries", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Local_Detection_of_Stereo_Occlusion_Boundaries_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "454": {"title": "learning to explore intrinsic saliency for stereoscopic video", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Learning_to_Explore_Intrinsic_Saliency_for_Stereoscopic_Video_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "901": {"title": "variational autoencoders pursue pca directions (by accident)", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rolinek_Variational_Autoencoders_Pursue_PCA_Directions_by_Accident_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "947": {"title": "dense relational captioning: triple-stream networks for relationship-based captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Dense_Relational_Captioning_Triple-Stream_Networks_for_Relationship-Based_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "783": {"title": "joint discriminative and generative learning for person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Joint_Discriminative_and_Generative_Learning_for_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "329": {"title": "reasoning-rcnn: unifying adaptive global reasoning into large-scale object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Reasoning-RCNN_Unifying_Adaptive_Global_Reasoning_Into_Large-Scale_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "446": {"title": "shape2motion: joint analysis of motion parts and attributes from 3d shapes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Shape2Motion_Joint_Analysis_of_Motion_Parts_and_Attributes_From_3D_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "276": {"title": "learning to remember: a synaptic plasticity driven framework for continual learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "314": {"title": "gspn: generative shape proposal network for 3d instance segmentation in point cloud", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yi_GSPN_Generative_Shape_Proposal_Network_for_3D_Instance_Segmentation_in_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1113": {"title": "a skeleton-bridged deep learning approach for generating meshes of complex topologies from single rgb images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_A_Skeleton-Bridged_Deep_Learning_Approach_for_Generating_Meshes_of_Complex_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "887": {"title": "a main/subsidiary network framework for simplifying binary neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_A_MainSubsidiary_Network_Framework_for_Simplifying_Binary_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "581": {"title": "surface reconstruction from normals: a robust dgp-based discontinuity preservation approach", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Surface_Reconstruction_From_Normals_A_Robust_DGP-Based_Discontinuity_Preservation_Approach_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "903": {"title": "learning actor relation graphs for group activity recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Learning_Actor_Relation_Graphs_for_Group_Activity_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "451": {"title": "mask scoring r-cnn", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_Mask_Scoring_R-CNN_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "483": {"title": "conditional adversarial generative flow for controllable image synthesis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Conditional_Adversarial_Generative_Flow_for_Controllable_Image_Synthesis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "324": {"title": "clevr-ref+: diagnosing visual reasoning with referring expressions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_CLEVR-Ref_Diagnosing_Visual_Reasoning_With_Referring_Expressions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "572": {"title": "distilled person re-identification: towards a more scalable system", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1204": {"title": "activity driven weakly supervised object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Activity_Driven_Weakly_Supervised_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1159": {"title": "do better imagenet models transfer better?", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kornblith_Do_Better_ImageNet_Models_Transfer_Better_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "754": {"title": "box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Song_Box-Driven_Class-Wise_Region_Masking_and_Filling_Rate_Guided_Loss_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "359": {"title": "t-net: parametrizing fully convolutional nets with a single high-order tensor", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kossaifi_T-Net_Parametrizing_Fully_Convolutional_Nets_With_a_Single_High-Order_Tensor_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "278": {"title": "\"double-dip\": unsupervised image decomposition via coupled deep-image-priors", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gandelsman_Double-DIP_Unsupervised_Image_Decomposition_via_Coupled_Deep-Image-Priors_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "372": {"title": "semantics disentangling for text-to-image generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yin_Semantics_Disentangling_for_Text-To-Image_Generation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "449": {"title": "self-supervised spatiotemporal learning via video clip order prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Self-Supervised_Spatiotemporal_Learning_via_Video_Clip_Order_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "108": {"title": "magsac: marginalizing sample consensus", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Barath_MAGSAC_Marginalizing_Sample_Consensus_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "766": {"title": "ms-tcn: multi-stage temporal convolutional network for action segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Abu_Farha_MS-TCN_Multi-Stage_Temporal_Convolutional_Network_for_Action_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "405": {"title": "mhp-vos: multiple hypotheses propagation for video object segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_MHP-VOS_Multiple_Hypotheses_Propagation_for_Video_Object_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "211": {"title": "enhanced bayesian compression via deep reinforcement learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yuan_Enhanced_Bayesian_Compression_via_Deep_Reinforcement_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "922": {"title": "dvc: an end-to-end deep video compression framework", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lu_DVC_An_End-To-End_Deep_Video_Compression_Framework_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1029": {"title": "automatic adaptation of object detectors to new domains using self-training", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/RoyChowdhury_Automatic_Adaptation_of_Object_Detectors_to_New_Domains_Using_Self-Training_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "847": {"title": "deep tree learning for zero-shot face anti-spoofing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Deep_Tree_Learning_for_Zero-Shot_Face_Anti-Spoofing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "750": {"title": "label efficient semi-supervised learning via graph filtering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Label_Efficient_Semi-Supervised_Learning_via_Graph_Filtering_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "418": {"title": "distant supervised centroid shift: a simple and efficient approach to visual domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liang_Distant_Supervised_Centroid_Shift_A_Simple_and_Efficient_Approach_to_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "375": {"title": "adversarial defense through network profiling based path extraction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qiu_Adversarial_Defense_Through_Network_Profiling_Based_Path_Extraction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1088": {"title": "aanet: attribute attention network for person re-identifications", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tay_AANet_Attribute_Attention_Network_for_Person_Re-Identifications_CVPR_2019_paper.html", "abstract": "This paper proposes Attribute Attention Network (AANet), a new architecture that integrates person a\nttributes and attribute attention maps into a classification framework to solve the person re-identi\nfication (re-ID) problem. Many person re-ID models typically employ semantic cues such as body parts\n or human pose to improve the re-ID performance. Attribute information, however, is often not utiliz\ned. The proposed AANet leverages on a baseline model that uses body parts and integrates the key att\nribute information in an unified learning framework. The AANet consists of a global person ID task, \na part detection task and a crucial attribute detection task. By estimating the class responses of i\nndividual attributes and combining them to form the attribute attention map (AAM), a very strong dis\ncriminatory representation is constructed. The proposed AANet outperforms the best state-of-the-art \nmethod [??] using ResNet-50 by 3.36% in mAP and 3.12% in Rank-1 accuracy on DukeMTMC-reID dataset. O\nn Market1501 dataset, AANet achieves 92.38% mAP and 95.10% Rank-1 accuracy with re-ranking,  outperf\norming [??], another state of the art method using ResNet-152, by 1.42% in mAP and 0.47% in Rank-1 a\nccuracy. In addition, AANet can perform person attribute prediction (e.g., gender, hair length, clot\nhing length etc.), and localize the attributes in the query image.\r", "cite_num": 0}, "1007": {"title": "textured neural avatars", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shysheya_Textured_Neural_Avatars_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "212": {"title": "unsupervised embedding learning via invariant and spreading instance feature", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ye_Unsupervised_Embedding_Learning_via_Invariant_and_Spreading_Instance_Feature_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "275": {"title": "exploring context and visual pattern of relationship for scene graph generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Exploring_Context_and_Visual_Pattern_of_Relationship_for_Scene_Graph_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "240": {"title": "thinking outside the pool: active training image creation for relative attributes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Thinking_Outside_the_Pool_Active_Training_Image_Creation_for_Relative_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1097": {"title": "graphonomy: universal human parsing via graph transfer learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gong_Graphonomy_Universal_Human_Parsing_via_Graph_Transfer_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "413": {"title": "visual localization by learning objects-of-interest dense match regression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Weinzaepfel_Visual_Localization_by_Learning_Objects-Of-Interest_Dense_Match_Regression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "517": {"title": "deepsdf: learning continuous signed distance functions for shape representation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "131": {"title": "lo-net: deep real-time lidar odometry", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_LO-Net_Deep_Real-Time_Lidar_Odometry_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "349": {"title": "a variational pan-sharpening with local gradient constraints", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fu_A_Variational_Pan-Sharpening_With_Local_Gradient_Constraints_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "723": {"title": "multispectral imaging for fine-grained recognition of powders on complex backgrounds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhi_Multispectral_Imaging_for_Fine-Grained_Recognition_of_Powders_on_Complex_Backgrounds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1245": {"title": "gait recognition via disentangled representation learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Gait_Recognition_via_Disentangled_Representation_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1218": {"title": "semantically aligned bias reducing zero shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Paul_Semantically_Aligned_Bias_Reducing_Zero_Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1": {"title": "basnet: boundary-aware salient object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qin_BASNet_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "519": {"title": "3d human pose estimation in video with temporal convolutions and semi-supervised training", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pavllo_3D_Human_Pose_Estimation_in_Video_With_Temporal_Convolutions_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "793": {"title": "learning view priors for single-view 3d reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kato_Learning_View_Priors_for_Single-View_3D_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "610": {"title": "hierarchical cross-modal talking face generation with dynamic pixel-wise loss", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Hierarchical_Cross-Modal_Talking_Face_Generation_With_Dynamic_Pixel-Wise_Loss_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "709": {"title": "improved road connectivity by joint learning of orientation and segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Batra_Improved_Road_Connectivity_by_Joint_Learning_of_Orientation_and_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "380": {"title": "deep defocus map estimation using domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_Deep_Defocus_Map_Estimation_Using_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "487": {"title": "bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wong_Bilateral_Cyclic_Constraint_and_Adaptive_Regularization_for_Unsupervised_Monocular_Depth_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "96": {"title": "joint face detection and facial motion retargeting for multiple faces", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chaudhuri_Joint_Face_Detection_and_Facial_Motion_Retargeting_for_Multiple_Faces_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "260": {"title": "explicit spatial encoding for deep local descriptors", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mukundan_Explicit_Spatial_Encoding_for_Deep_Local_Descriptors_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "133": {"title": "strand-accurate multi-view hair capture", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nam_Strand-Accurate_Multi-View_Hair_Capture_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "528": {"title": "isospectralization, or how to hear shape, style, and correspondence", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cosmo_Isospectralization_or_How_to_Hear_Shape_Style_and_Correspondence_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "387": {"title": "actional-structural graph convolutional networks for skeleton-based action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Actional-Structural_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "926": {"title": "reliable and efficient image cropping: a grid anchor based approach", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_Reliable_and_Efficient_Image_Cropping_A_Grid_Anchor_Based_Approach_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1033": {"title": "a compact embedding for facial expression similarity", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Vemulapalli_A_Compact_Embedding_for_Facial_Expression_Similarity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "97": {"title": "collaborative spatiotemporal feature learning for video action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Collaborative_Spatiotemporal_Feature_Learning_for_Video_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "226": {"title": "efficient decision-based black-box adversarial attacks on face recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Efficient_Decision-Based_Black-Box_Adversarial_Attacks_on_Face_Recognition_CVPR_2019_paper.html", "abstract": "Face recognition has obtained remarkable progress in recent years due to the great improvement of de\nep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, \nwhich can cause fateful consequences in real-world face recognition applications with security-sensi\ntive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the \nmodels before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face \nrecognition models in the decision-based black-box attack setting, where the attackers have no acces\ns to the model parameters and gradients, but can only acquire hard-label predictions by sending quer\nies to the target model. This attack setting is more practical in real-world face recognition system\ns. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which\n can model the local geometries of the search directions and reduce the dimension of the search spac\ne. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum\n perturbation to an input face image with fewer queries. We also apply the proposed method to attack\n a real-world face recognition system successfully.", "cite_num": 1}, "352": {"title": "privacy preserving image-based localization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Speciale_Privacy_Preserving_Image-Based_Localization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "162": {"title": "panoptic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "68": {"title": "robustness verification of classification deep neural networks via linear programming", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_Robustness_Verification_of_Classification_Deep_Neural_Networks_via_Linear_Programming_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1100": {"title": "the pros and cons: rank-aware temporal attention for skill determination in long videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Doughty_The_Pros_and_Cons_Rank-Aware_Temporal_Attention_for_Skill_Determination_CVPR_2019_paper.html", "abstract": "We present a new model to determine relative skill from long videos, through learnable temporal atte\nntion modules. Skill determination is formulated as a ranking problem, making it suitable for common\n and generic tasks. However, for long videos, parts of the video are irrelevant for assessing skill,\n and there may be variability in the skill exhibited throughout a video. We therefore propose a meth\nod which assesses the relative overall level of skill in a long video by attending to its skill-rele\nvant parts. Our approach trains temporal attention modules, learned with only video-level supervisio\nn, using a novel rank-aware loss function. In addition to attending to task relevant video parts, ou\nr proposed loss jointly trains two attention modules to separately attend to video parts which are i\nndicative of higher (pros) and lower (cons) skill. We evaluate our approach on the EPIC-Skills datas\net and additionally annotate a larger dataset from YouTube videos for skill determination with five \npreviously unexplored tasks. Our method outperforms previous approaches and classic softmax attentio\nn on both datasets by over 4% pairwise accuracy, and as much as 12% on individual tasks. We also dem\nonstrate our model's ability to attend to rank-aware parts of the video.", "cite_num": 1}, "515": {"title": "global second-order pooling convolutional networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Global_Second-Order_Pooling_Convolutional_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "838": {"title": "clusternet: deep hierarchical cluster network with rigorously rotation-invariant representation for point cloud analysis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_ClusterNet_Deep_Hierarchical_Cluster_Network_With_Rigorously_Rotation-Invariant_Representation_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "106": {"title": "self-supervised learning of 3d human pose using multi-view geometry", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kocabas_Self-Supervised_Learning_of_3D_Human_Pose_Using_Multi-View_Geometry_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1046": {"title": "pcan: 3d attention map learning using contextual information for point cloud based retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_PCAN_3D_Attention_Map_Learning_Using_Contextual_Information_for_Point_CVPR_2019_paper.html", "abstract": "Point cloud based retrieval for place recognition is an emerging problem in vision field. The main c\nhallenge is how to find an efficient way to encode the local features into a discriminative global d\nescriptor. In this paper, we propose a Point Contextual Attention Network (PCAN), which can predict \nthe significance of each local point feature based on point context. Our network makes it possible t\no pay more attention to the task-relevent features when aggregating local features. Experiments on v\narious benchmark datasets show that the proposed network can provide outperformance than current sta\nte-of-the-art approaches.", "cite_num": 0}, "38": {"title": "context-aware visual compatibility prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cucurull_Context-Aware_Visual_Compatibility_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "786": {"title": "information maximizing visual question generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Krishna_Information_Maximizing_Visual_Question_Generation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "917": {"title": "attending to discriminative certainty for domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kurmi_Attending_to_Discriminative_Certainty_for_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "707": {"title": "high flux passive imaging with single-photon sensors", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ingle_High_Flux_Passive_Imaging_With_Single-Photon_Sensors_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "130": {"title": "livesketch: query perturbations for guided sketch-based visual search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Collomosse_LiveSketch_Query_Perturbations_for_Guided_Sketch-Based_Visual_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "274": {"title": "adversarial structure matching for structured prediction tasks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hwang_Adversarial_Structure_Matching_for_Structured_Prediction_Tasks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "536": {"title": "joint representative selection and feature learning: a semi-supervised approach", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Joint_Representative_Selection_and_Feature_Learning_A_Semi-Supervised_Approach_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "628": {"title": "monocular depth estimation using relative depth maps", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_Monocular_Depth_Estimation_Using_Relative_Depth_Maps_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1225": {"title": "cross-task weakly supervised learning from instructional videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhukov_Cross-Task_Weakly_Supervised_Learning_From_Instructional_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "828": {"title": "hybrid-attention based decoupled metric learning for zero-shot image retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Hybrid-Attention_Based_Decoupled_Metric_Learning_for_Zero-Shot_Image_Retrieval_CVPR_2019_paper.html", "abstract": "In zero-shot image retrieval (ZSIR) task, embedding learning becomes more attractive, however, many \nmethods follow the traditional metric learning idea and omit the problems behind zero-shot settings.\n In this paper, we first emphasize the importance of learning visual discriminative metric and preve\nnting the partial/selective learning behavior of learner in ZSIR, and then propose the Decoupled Met\nric Learning (DeML) framework to achieve these individually. Instead of coarsely optimizing an unifi\ned metric, we decouple it into multiple attention-specific parts so as to recurrently induce the dis\ncrimination and explicitly enhance the generalization. And they are mainly achieved by our object-at\ntention module based on random walk graph propagation and the channel-attention module based on the \nadversary constraint, respectively. We demonstrate the necessity of addressing the vital problems in\n ZSIR on the popular benchmarks, outperforming the state-of-the-art methods by a significant margin.\n Code is available at http://www.bhchen.cn\r", "cite_num": 2}, "28": {"title": "tracking by animation: unsupervised learning of multi-object attentive trackers", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Tracking_by_Animation_Unsupervised_Learning_of_Multi-Object_Attentive_Trackers_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "317": {"title": "sfnet: learning object-aware semantic correspondence", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_SFNet_Learning_Object-Aware_Semantic_Correspondence_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "747": {"title": "events-to-video: bringing modern computer vision to event cameras", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rebecq_Events-To-Video_Bringing_Modern_Computer_Vision_to_Event_Cameras_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1292": {"title": "see more, know more: unsupervised video object segmentation with co-attention siamese networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lu_See_More_Know_More_Unsupervised_Video_Object_Segmentation_With_Co-Attention_CVPR_2019_paper.html", "abstract": "We introduce a novel network, called as CO-attention Siamese Network (COSNet), to address the unsupe\nrvised video object segmentation task from a holistic view. We emphasize the importance of inherent \ncorrelation among video frames and incorporate a global co-attention mechanism to improve further th\ne state-of-the-art deep learning based solutions that primarily focus on learning discriminative for\neground representations over appearance and motion in short-term temporal segments. The co-attention\n layers in our network provide efficient and competent stages for capturing global correlations and \nscene context by jointly computing and appending co-attention responses into a joint feature space. \nWe train COSNet with pairs of video frames, which naturally augments training data and allows increa\nsed learning capacity. During the segmentation stage, the co-attention model encodes useful informat\nion by processing multiple reference frames together, which is leveraged to infer the frequently rea\nppearing and salient foreground objects better. We propose a unified and end-to-end trainable framew\nork where different co-attention variants can be derived for mining the rich context within videos. \nOur extensive experiments over three large benchmarks manifest that COSNet outperforms the current a\nlternatives by a large margin. We will publicly release our implementation and models.\r", "cite_num": 0}, "146": {"title": "handwriting recognition in low-resource scripts using adversarial learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bhunia_Handwriting_Recognition_in_Low-Resource_Scripts_Using_Adversarial_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "635": {"title": "adacos: adaptively scaling cosine logits for effectively learning deep face representations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_AdaCos_Adaptively_Scaling_Cosine_Logits_for_Effectively_Learning_Deep_Face_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "373": {"title": "learning correspondence from the cycle-consistency of time", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_Correspondence_From_the_Cycle-Consistency_of_Time_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "123": {"title": "octree guided cnn with spherical kernels for 3d point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lei_Octree_Guided_CNN_With_Spherical_Kernels_for_3D_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "327": {"title": "multi-task self-supervised object detection via recycling of bounding box annotations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_Multi-Task_Self-Supervised_Object_Detection_via_Recycling_of_Bounding_Box_Annotations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1006": {"title": "craves: controlling robotic arm with a vision-based economic system", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zuo_CRAVES_Controlling_Robotic_Arm_With_a_Vision-Based_Economic_System_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "355": {"title": "importance estimation for neural network pruning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Molchanov_Importance_Estimation_for_Neural_Network_Pruning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "568": {"title": "spherical regression: learning viewpoints, surface normals and 3d rotations on n-spheres", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liao_Spherical_Regression_Learning_Viewpoints_Surface_Normals_and_3D_Rotations_on_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "112": {"title": "a convex relaxation for multi-graph matching", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Swoboda_A_Convex_Relaxation_for_Multi-Graph_Matching_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "298": {"title": "linkage based face clustering via graph convolution network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Linkage_Based_Face_Clustering_via_Graph_Convolution_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "591": {"title": "selflow: self-supervised learning of optical flow", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_SelFlow_Self-Supervised_Learning_of_Optical_Flow_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1176": {"title": "real-time self-adaptive deep stereo", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tonioni_Real-Time_Self-Adaptive_Deep_Stereo_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1103": {"title": "self-supervised learning via conditional motion propagation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhan_Self-Supervised_Learning_via_Conditional_Motion_Propagation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "878": {"title": "learning joint reconstruction of hands and manipulated objects", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hasson_Learning_Joint_Reconstruction_of_Hands_and_Manipulated_Objects_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1132": {"title": "towards natural and accurate future motion prediction of humans and animals", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Towards_Natural_and_Accurate_Future_Motion_Prediction_of_Humans_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1234": {"title": "group-wise correlation stereo network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_Group-Wise_Correlation_Stereo_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "60": {"title": "training deep learning based image denoisers from undersampled measurements without ground truth and without image prior", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhussip_Training_Deep_Learning_Based_Image_Denoisers_From_Undersampled_Measurements_Without_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1028": {"title": "fast spatio-temporal residual network for video super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Fast_Spatio-Temporal_Residual_Network_for_Video_Super-Resolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1287": {"title": "c3ae: exploring the limits of compact model for age estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_C3AE_Exploring_the_Limits_of_Compact_Model_for_Age_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1140": {"title": "sensitive-sample fingerprinting of deep neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Sensitive-Sample_Fingerprinting_of_Deep_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1194": {"title": "3d appearance super-resolution with deep learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_3D_Appearance_Super-Resolution_With_Deep_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1071": {"title": "deepfashion2: a versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ge_DeepFashion2_A_Versatile_Benchmark_for_Detection_Pose_Estimation_Segmentation_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "407": {"title": "translate-to-recognize networks for rgb-d scene recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Du_Translate-to-Recognize_Networks_for_RGB-D_Scene_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1015": {"title": "density map regression guided detection network for rgb-d crowd counting and localization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lian_Density_Map_Regression_Guided_Detection_Network_for_RGB-D_Crowd_Counting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1189": {"title": "recurrent mvsnet for high-resolution multi-view stereo depth inference", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yao_Recurrent_MVSNet_for_High-Resolution_Multi-View_Stereo_Depth_Inference_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "444": {"title": "regularizing activation distribution for training binarized deep networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Regularizing_Activation_Distribution_for_Training_Binarized_Deep_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "532": {"title": "bounding box regression with uncertainty for accurate object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Bounding_Box_Regression_With_Uncertainty_for_Accurate_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "526": {"title": "nesti-net: normal estimation for unstructured 3d point clouds using convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ben-Shabat_Nesti-Net_Normal_Estimation_for_Unstructured_3D_Point_Clouds_Using_Convolutional_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "655": {"title": "detailed human shape estimation from a single image by hierarchical mesh deformation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Detailed_Human_Shape_Estimation_From_a_Single_Image_by_Hierarchical_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "392": {"title": "embodied question answering in photorealistic environments with point cloud perception", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wijmans_Embodied_Question_Answering_in_Photorealistic_Environments_With_Point_Cloud_Perception_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1081": {"title": "neural illumination: lighting prediction for indoor environments", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Song_Neural_Illumination_Lighting_Prediction_for_Indoor_Environments_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "626": {"title": "deep dual relation modeling for egocentric interaction recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Deep_Dual_Relation_Modeling_for_Egocentric_Interaction_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "667": {"title": "graph attention convolution for point cloud semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "Standard convolution is inherently limited for semantic segmentation of point cloud due to its isotr\nopy about features. It neglects the structure of an object, results in poor object delineation and s\nmall spurious regions in the segmentation result. This paper proposes a novel graph attention convol\nution (GAC), whose kernels can be dynamically carved into specific shapes to adapt to the structure \nof an object. Specifically, by assigning proper attentional weights to different neighboring points,\n GAC is designed to selectively focus on the most relevant part of them according to their dynamical\nly learned features. The shape of the convolution kernel is then determined by the learned distribut\nion of the attentional weights. Though simple, GAC can capture the structured features of point clou\nds for fine-grained segmentation and avoid feature contamination between objects. Theoretically, we \nprovided a thorough analysis on the expressive capabilities of GAC to show how it can learn about th\ne features of point clouds. Empirically, we evaluated the proposed GAC on challenging indoor and out\ndoor datasets and achieved the state-of-the-art results in both scenarios.\r", "cite_num": 0}, "763": {"title": "local relationship learning with person-specific shape regularization for facial action unit detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Niu_Local_Relationship_Learning_With_Person-Specific_Shape_Regularization_for_Facial_Action_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "942": {"title": "capture, learning, and synthesis of 3d speaking styles", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cudeiro_Capture_Learning_and_Synthesis_of_3D_Speaking_Styles_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1224": {"title": "context and attribute grounded dense captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yin_Context_and_Attribute_Grounded_Dense_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "99": {"title": "look more than once: an accurate detector for text of arbitrary shapes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Look_More_Than_Once_An_Accurate_Detector_for_Text_of_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "438": {"title": "explicit bias discovery in visual question answering models", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Manjunatha_Explicit_Bias_Discovery_in_Visual_Question_Answering_Models_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1059": {"title": "ga-net: guided aggregation net for end-to-end stereo matching", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_GA-Net_Guided_Aggregation_Net_for_End-To-End_Stereo_Matching_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "192": {"title": "creative flow+ dataset", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shugrina_Creative_Flow_Dataset_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "851": {"title": "time-conditioned action anticipation in one shot", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Time-Conditioned_Action_Anticipation_in_One_Shot_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "662": {"title": "p2sgrad: refined gradients for optimizing deep face models", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_P2SGrad_Refined_Gradients_for_Optimizing_Deep_Face_Models_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1151": {"title": "a simple pooling-based design for real-time salient object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_A_Simple_Pooling-Based_Design_for_Real-Time_Salient_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "254": {"title": "ige-net: inverse graphics energy networks for human pose estimation and single-view reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jack_IGE-Net_Inverse_Graphics_Energy_Networks_for_Human_Pose_Estimation_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "924": {"title": "object counting and instance segmentation with image-level supervision", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cholakkal_Object_Counting_and_Instance_Segmentation_With_Image-Level_Supervision_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1284": {"title": "orthogonal decomposition network for pixel-wise binary classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Orthogonal_Decomposition_Network_for_Pixel-Wise_Binary_Classification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "293": {"title": "unsupervised part-based disentangling of object shape and appearance", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lorenz_Unsupervised_Part-Based_Disentangling_of_Object_Shape_and_Appearance_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "727": {"title": "adversarial semantic alignment for improved image captions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dognin_Adversarial_Semantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "950": {"title": "on the intrinsic dimensionality of image representations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gong_On_the_Intrinsic_Dimensionality_of_Image_Representations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "676": {"title": "holopose: holistic 3d human reconstruction in-the-wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guler_HoloPose_Holistic_3D_Human_Reconstruction_In-The-Wild_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "257": {"title": "face anti-spoofing: model matters, so does data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Face_Anti-Spoofing_Model_Matters_so_Does_Data_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1276": {"title": "competitive collaboration: joint unsupervised learning of depth, camera motion, optical flow and motion segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ranjan_Competitive_Collaboration_Joint_Unsupervised_Learning_of_Depth_Camera_Motion_Optical_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "391": {"title": "layout-graph reasoning for fashion landmark detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Layout-Graph_Reasoning_for_Fashion_Landmark_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "984": {"title": "enhancing triplegan for semi-supervised conditional instance synthesis and classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Enhancing_TripleGAN_for_Semi-Supervised_Conditional_Instance_Synthesis_and_Classification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "852": {"title": "towards vqa models that can read", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "224": {"title": "generalized intersection over union: a metric and a loss for bounding box regression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rezatofighi_Generalized_Intersection_Over_Union_A_Metric_and_a_Loss_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "787": {"title": "regularface: deep face recognition via exclusive regularization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_RegularFace_Deep_Face_Recognition_via_Exclusive_Regularization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "596": {"title": "learning spatio-temporal representation with local and global diffusion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qiu_Learning_Spatio-Temporal_Representation_With_Local_and_Global_Diffusion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "776": {"title": "pepsi : fast image inpainting with parallel decoding network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sagong_PEPSI__Fast_Image_Inpainting_With_Parallel_Decoding_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "394": {"title": "fast single image reflection suppression via convex optimization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Fast_Single_Image_Reflection_Suppression_via_Convex_Optimization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "215": {"title": "simulcap : single-view human performance capture with cloth simulation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_SimulCap__Single-View_Human_Performance_Capture_With_Cloth_Simulation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "974": {"title": "weakly supervised person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Meng_Weakly_Supervised_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1024": {"title": "dance with flow: two-in-one stream action detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Dance_With_Flow_Two-In-One_Stream_Action_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "335": {"title": "apdrawinggan: generating artistic portrait drawings from face photos with hierarchical gans", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yi_APDrawingGAN_Generating_Artistic_Portrait_Drawings_From_Face_Photos_With_Hierarchical_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "152": {"title": "learning cross-modal embeddings with adversarial networks for cooking recipes and food images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_Cross-Modal_Embeddings_With_Adversarial_Networks_for_Cooking_Recipes_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "991": {"title": "video magnification in the wild using fractional anisotropy in temporal distribution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Takeda_Video_Magnification_in_the_Wild_Using_Fractional_Anisotropy_in_Temporal_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "621": {"title": "meta-learning with differentiable convex optimization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_Meta-Learning_With_Differentiable_Convex_Optimization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "699": {"title": "combining 3d morphable models: a large scale face-and-head model", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ploumpis_Combining_3D_Morphable_Models_A_Large_Scale_Face-And-Head_Model_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "882": {"title": "f-vaegan-d2: a feature generating framework for any-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xian_F-VAEGAN-D2_A_Feature_Generating_Framework_for_Any-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "66": {"title": "domain-symmetric networks for adversarial domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Domain-Symmetric_Networks_for_Adversarial_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1192": {"title": "deep spherical quantization for image search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Eghbali_Deep_Spherical_Quantization_for_Image_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "2": {"title": "pay attention! - robustifying a deep visuomotor policy through task-focused visual attention", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Abolghasemi_Pay_Attention_-_Robustifying_a_Deep_Visuomotor_Policy_Through_Task-Focused_CVPR_2019_paper.html", "abstract": "Several recent studies have demonstrated the promise of deep visuomotor policies for robot manipulat\nor control. Despite impressive progress, these systems are known to be vulnerable to physical distur\nbances, such as accidental or adversarial bumps that make them drop the manipulated object. They als\no tend to be distracted by visual disturbances such as objects moving in the robot's field of view, \neven if the disturbance does not physically prevent the execution of the task. In this paper, we pro\npose an approach for augmenting a deep visuomotor policy trained through demonstrations with Task Fo\ncused visual Attention (TFA). The manipulation task is specified with a natural language text such a\ns \"move the red bowl to the left\". This allows the visual attention component to concentrate on the \ncurrent object that the robot needs to manipulate. We show that even in benign environments, the TFA\n allows the policy to consistently outperform a variant with no attention mechanism. More importantl\ny, the new policy is significantly more robust: it regularly recovers from severe physical disturban\nces (such as bumps causing it to drop the object) from which the baseline policy, i.e. with no visua\nl attention, almost never recovers. In addition, we show that the proposed policy performs correctly\n in the presence of a wide class of visual disturbances, exhibiting a behavior reminiscent of human \nselective visual attention experiments. \r", "cite_num": 0}, "571": {"title": "instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Neven_Instance_Segmentation_by_Jointly_Optimizing_Spatial_Embeddings_and_Clustering_Bandwidth_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "735": {"title": "knockoff nets: stealing functionality of black-box models", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Orekondy_Knockoff_Nets_Stealing_Functionality_of_Black-Box_Models_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "198": {"title": "geometry-aware symmetric domain adaptation for monocular depth estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Geometry-Aware_Symmetric_Domain_Adaptation_for_Monocular_Depth_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1001": {"title": "compact feature learning for multi-domain image classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Compact_Feature_Learning_for_Multi-Domain_Image_Classification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1290": {"title": "hplflownet: hierarchical permutohedral lattice flownet for scene flow estimation on large-scale point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gu_HPLFlowNet_Hierarchical_Permutohedral_Lattice_FlowNet_for_Scene_Flow_Estimation_on_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "40": {"title": "sr-lstm: state refinement for lstm towards pedestrian trajectory prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_SR-LSTM_State_Refinement_for_LSTM_Towards_Pedestrian_Trajectory_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "459": {"title": "progressive ensemble networks for zero-shot recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ye_Progressive_Ensemble_Networks_for_Zero-Shot_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "606": {"title": "cyclic guidance for weakly supervised joint detection and segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shen_Cyclic_Guidance_for_Weakly_Supervised_Joint_Detection_and_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "5": {"title": "from coarse to fine: robust hierarchical localization at large scale", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sarlin_From_Coarse_to_Fine_Robust_Hierarchical_Localization_at_Large_Scale_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1165": {"title": "on zero-shot recognition of generic objects", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hascoet_On_Zero-Shot_Recognition_of_Generic_Objects_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1205": {"title": "monocular total capture: posing face, body, and hands in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xiang_Monocular_Total_Capture_Posing_Face_Body_and_Hands_in_the_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1271": {"title": "multi-agent tensor fusion for contextual trajectory prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Multi-Agent_Tensor_Fusion_for_Contextual_Trajectory_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "145": {"title": "meshadv: adversarial meshes for visual recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xiao_MeshAdv_Adversarial_Meshes_for_Visual_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "756": {"title": "coin: a large-scale dataset for comprehensive instructional video analysis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_COIN_A_Large-Scale_Dataset_for_Comprehensive_Instructional_Video_Analysis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "645": {"title": "robust subspace clustering with independent and piecewise identically distributed noise modeling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Robust_Subspace_Clustering_With_Independent_and_Piecewise_Identically_Distributed_Noise_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "406": {"title": "efficient featurized image pyramid network for single shot detector", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pang_Efficient_Featurized_Image_Pyramid_Network_for_Single_Shot_Detector_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "206": {"title": "good news, everyone! context driven entity-aware captioning for news images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Biten_Good_News_Everyone_Context_Driven_Entity-Aware_Captioning_for_News_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "11": {"title": "towards social artificial intelligence: nonverbal social signal prediction in a triadic interaction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Joo_Towards_Social_Artificial_Intelligence_Nonverbal_Social_Signal_Prediction_in_a_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1022": {"title": "leveraging shape completion for 3d siamese tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Giancola_Leveraging_Shape_Completion_for_3D_Siamese_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1058": {"title": "point-to-pose voting based hand pose estimation using residual permutation equivariant layer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Point-To-Pose_Voting_Based_Hand_Pose_Estimation_Using_Residual_Permutation_Equivariant_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "250": {"title": "recurrent neural networks with intra-frame iterations for video deblurring", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nah_Recurrent_Neural_Networks_With_Intra-Frame_Iterations_for_Video_Deblurring_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1188": {"title": "dudonet: dual domain network for ct metal artifact reduction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_DuDoNet_Dual_Domain_Network_for_CT_Metal_Artifact_Reduction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "73": {"title": "object discovery in videos as foreground motion clustering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Object_Discovery_in_Videos_as_Foreground_Motion_Clustering_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "986": {"title": "devil is in the edges: learning semantic boundaries from noisy annotations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Acuna_Devil_Is_in_the_Edges_Learning_Semantic_Boundaries_From_Noisy_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "894": {"title": "robust facial landmark detection via occlusion-adaptive deep networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Robust_Facial_Landmark_Detection_via_Occlusion-Adaptive_Deep_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "232": {"title": "scan2mesh: from unstructured range scans to 3d meshes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dai_Scan2Mesh_From_Unstructured_Range_Scans_to_3D_Meshes_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "659": {"title": "deepflux for skeletons in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_DeepFlux_for_Skeletons_in_the_Wild_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "64": {"title": "collaborative global-local networks for memory-efficient segmentation of ultra-high resolution images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Collaborative_Global-Local_Networks_for_Memory-Efficient_Segmentation_of_Ultra-High_Resolution_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "806": {"title": "multispectral and hyperspectral image fusion by ms/hs fusion net", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Multispectral_and_Hyperspectral_Image_Fusion_by_MSHS_Fusion_Net_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "618": {"title": "variational prototyping-encoder: one-shot learning with prototypical images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Variational_Prototyping-Encoder_One-Shot_Learning_With_Prototypical_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "820": {"title": "capsal: leveraging captioning to boost semantics for salient object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_CapSal_Leveraging_Captioning_to_Boost_Semantics_for_Salient_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1184": {"title": "estimating 3d motion and forces of person-object interactions from monocular video", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Estimating_3D_Motion_and_Forces_of_Person-Object_Interactions_From_Monocular_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "650": {"title": "transgaga: geometry-aware unsupervised image-to-image translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_TransGaGa_Geometry-Aware_Unsupervised_Image-To-Image_Translation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1278": {"title": "generalized zero-shot recognition based on visually semantic embedding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Generalized_Zero-Shot_Recognition_Based_on_Visually_Semantic_Embedding_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "20": {"title": "single image reflection removal beyond linearity", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wen_Single_Image_Reflection_Removal_Beyond_Linearity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "159": {"title": "unifying heterogeneous classifiers with distillation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Vongkulbhisal_Unifying_Heterogeneous_Classifiers_With_Distillation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1137": {"title": "tafe-net: task-aware feature embeddings for low shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_TAFE-Net_Task-Aware_Feature_Embeddings_for_Low_Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "58": {"title": "posefix: model-agnostic general human pose refinement network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Moon_PoseFix_Model-Agnostic_General_Human_Pose_Refinement_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "341": {"title": "a sufficient condition for convergences of adam and rmsprop", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zou_A_Sufficient_Condition_for_Convergences_of_Adam_and_RMSProp_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "977": {"title": "3d motion decomposition for rgbd future dynamic scene synthesis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qi_3D_Motion_Decomposition_for_RGBD_Future_Dynamic_Scene_Synthesis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "251": {"title": "learning to calibrate straight lines for fisheye image rectification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xue_Learning_to_Calibrate_Straight_Lines_for_Fisheye_Image_Rectification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "622": {"title": "where's wally now? deep generative and discriminative embeddings for novelty detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Burlina_Wheres_Wally_Now_Deep_Generative_and_Discriminative_Embeddings_for_Novelty_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "306": {"title": "self-critical n-step training for image captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Self-Critical_N-Step_Training_for_Image_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "330": {"title": "learning semantic segmentation from synthetic data: a geometrically guided input-output adaptation approach", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Learning_Semantic_Segmentation_From_Synthetic_Data_A_Geometrically_Guided_Input-Output_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "87": {"title": "learning monocular depth estimation infusing traditional stereo knowledge", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tosi_Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "873": {"title": "multi-target embodied question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Multi-Target_Embodied_Question_Answering_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "531": {"title": "sea-thru: a method for removing water from underwater images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Akkaynak_Sea-Thru_A_Method_for_Removing_Water_From_Underwater_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "422": {"title": "from recognition to cognition: visual commonsense reasoning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zellers_From_Recognition_to_Cognition_Visual_Commonsense_Reasoning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "719": {"title": "jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pham_JSIS3D_Joint_Semantic-Instance_Segmentation_of_3D_Point_Clouds_With_Multi-Task_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "589": {"title": "sphere generative adversarial network based on geometric moment matching", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matching_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "665": {"title": "generating classification weights with gnn denoising autoencoders for few-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gidaris_Generating_Classification_Weights_With_GNN_Denoising_Autoencoders_for_Few-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "876": {"title": "revisiting perspective information for efficient crowd counting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Revisiting_Perspective_Information_for_Efficient_Crowd_Counting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "995": {"title": "gpsfm: global projective sfm using algebraic constraints on multi-view fundamental matrices", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kasten_GPSfM_Global_Projective_SFM_Using_Algebraic_Constraints_on_Multi-View_Fundamental_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "780": {"title": "mask-guided portrait editing with conditional gans", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gu_Mask-Guided_Portrait_Editing_With_Conditional_GANs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "535": {"title": "unsupervised multi-modal neural machine translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Su_Unsupervised_Multi-Modal_Neural_Machine_Translation_CVPR_2019_paper.html", "abstract": "Unsupervised neural machine translation (UNMT) has recently achieved remarkable results with only la\nrge monolingual corpora in each language. However, the uncertainty of associating target with source\n sentences makes UNMT theoretically an ill-posed problem. This work investigates the possibility of \nutilizing images for disambiguation to improve the performance of UNMT. Our assumption is intuitivel\ny based on the invariant property of image, i.e., the description of the same visual content by diff\nerent languages should be approximately similar. We propose an unsupervised multi-modal machine tran\nslation (UMNMT) framework based on the language translation cycle consistency loss conditional on th\ne image, targeting to learn the bidirectional multi-modal translation simultaneously. Through an alt\nernate training between multi-modal and uni-modal, our inference model can translate with or without\n the image. On the widely used Multi30K dataset, the experimental results of our approach are signif\nicantly better than those of the text-only UNMT on the 2016 test dataset.", "cite_num": 3}, "814": {"title": "few-shot learning with localization in realistic settings", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wertheimer_Few-Shot_Learning_With_Localization_in_Realistic_Settings_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "507": {"title": "unsupervised person re-identification by soft multilabel learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Unsupervised_Person_Re-Identification_by_Soft_Multilabel_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "761": {"title": "perturbation analysis of the 8-point algorithm: a case study for wide fov cameras", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/da_Silveira_Perturbation_Analysis_of_the_8-Point_Algorithm_A_Case_Study_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "238": {"title": "enhanced pix2pix dehazing network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qu_Enhanced_Pix2pix_Dehazing_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "117": {"title": "drivingstereo: a large-scale dataset for stereo matching in autonomous driving scenarios", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_DrivingStereo_A_Large-Scale_Dataset_for_Stereo_Matching_in_Autonomous_Driving_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1036": {"title": "why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hein_Why_ReLU_Networks_Yield_High-Confidence_Predictions_Far_Away_From_the_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "502": {"title": "large-scale weakly-supervised pre-training for video action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ghadiyaram_Large-Scale_Weakly-Supervised_Pre-Training_for_Video_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "271": {"title": "filter pruning via geometric median for deep convolutional neural networks acceleration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Filter_Pruning_via_Geometric_Median_for_Deep_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "541": {"title": "spectral reconstruction from dispersive blur: a novel light efficient spectral imager", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Spectral_Reconstruction_From_Dispersive_Blur_A_Novel_Light_Efficient_Spectral_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "195": {"title": "tacnet: transition-aware context network for spatio-temporal action detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Song_TACNet_Transition-Aware_Context_Network_for_Spatio-Temporal_Action_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "741": {"title": "when color constancy goes wrong: correcting improperly white-balanced images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Afifi_When_Color_Constancy_Goes_Wrong_Correcting_Improperly_White-Balanced_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "683": {"title": "improving few-shot user-specific gaze adaptation via gaze redirection synthesis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Improving_Few-Shot_User-Specific_Gaze_Adaptation_via_Gaze_Redirection_Synthesis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "657": {"title": "mutual learning of complementary networks via residual correction for improving semi-supervised classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Mutual_Learning_of_Complementary_Networks_via_Residual_Correction_for_Improving_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1135": {"title": "connecting touch and vision via cross-modal prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Connecting_Touch_and_Vision_via_Cross-Modal_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "337": {"title": "grounding human-to-vehicle advice for self-driving vehicles", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Grounding_Human-To-Vehicle_Advice_for_Self-Driving_Vehicles_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "410": {"title": "holistic and comprehensive annotation of clinically significant findings on diverse ct images: learning from radiology reports and label ontology", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_Holistic_and_Comprehensive_Annotation_of_Clinically_Significant_Findings_on_Diverse_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "488": {"title": "disentangled representation learning for 3d face shape", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jiang_Disentangled_Representation_Learning_for_3D_Face_Shape_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "401": {"title": "label propagation for deep semi-supervised learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Iscen_Label_Propagation_for_Deep_Semi-Supervised_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1074": {"title": "iterative residual refinement for joint optical flow and occlusion estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hur_Iterative_Residual_Refinement_for_Joint_Optical_Flow_and_Occlusion_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "561": {"title": "unsupervised learning of dense shape correspondence", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Halimi_Unsupervised_Learning_of_Dense_Shape_Correspondence_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "906": {"title": "semantic graph convolutional networks for 3d human pose regression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Semantic_Graph_Convolutional_Networks_for_3D_Human_Pose_Regression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "884": {"title": "slim densepose: thrifty learning from sparse annotations and motion cues", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Neverova_Slim_DensePose_Thrifty_Learning_From_Sparse_Annotations_and_Motion_Cues_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1009": {"title": "auto-encoding scene graphs for image captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "425": {"title": "the regretful agent: heuristic-aided navigation through progress estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ma_The_Regretful_Agent_Heuristic-Aided_Navigation_Through_Progress_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "506": {"title": "stochastic class-based hard example mining for deep metric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Suh_Stochastic_Class-Based_Hard_Example_Mining_for_Deep_Metric_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "259": {"title": "neighbourhood watch: referring expression comprehension via language-guided graph attention networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Neighbourhood_Watch_Referring_Expression_Comprehension_via_Language-Guided_Graph_Attention_Networks_CVPR_2019_paper.html", "abstract": "The task in referring expression comprehension is to localise the object instance in an image descri\nbed by a referring expression phrased in natural language. As a language-to-vision matching task, th\ne key to this problem is to learn a discriminative object feature that can adapt to the expression u\nsed. To avoid ambiguity, the expression normally tends to describe not only the properties of the re\nferent itself, but also its relationships to its neighbourhood. To capture and exploit this importan\nt information we propose a graph-based, language-guided attention mechanism. Being composed of node \nattention component and edge attention component, the proposed graph attention mechanism explicitly \nrepresents inter-object relationships, and properties with a flexibility and power impossible with c\nompeting approaches. Furthermore, the proposed graph attention mechanism enables the comprehension d\necision to be visualisable and explainable. Experiments on three referring expression comprehension \ndatasets show the advantage of the proposed approach.", "cite_num": 2}, "573": {"title": "efficient neural network compression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Efficient_Neural_Network_Compression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "368": {"title": "knowledge-embedded routing network for scene graph generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Knowledge-Embedded_Routing_Network_for_Scene_Graph_Generation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "34": {"title": "rules of the road: predicting driving behavior with a convolutional model of semantic interactions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hong_Rules_of_the_Road_Predicting_Driving_Behavior_With_a_Convolutional_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "578": {"title": "video summarization by learning from unpaired data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rochan_Video_Summarization_by_Learning_From_Unpaired_Data_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "217": {"title": "reasoning visual dialogs with structural and partial observations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Reasoning_Visual_Dialogs_With_Structural_and_Partial_Observations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1076": {"title": "spottune: transfer learning through adaptive fine-tuning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_SpotTune_Transfer_Learning_Through_Adaptive_Fine-Tuning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "594": {"title": "polarimetric camera calibration using an lcd monitor", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Polarimetric_Camera_Calibration_Using_an_LCD_Monitor_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "523": {"title": "building detail-sensitive semantic segmentation networks with polynomial pooling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wei_Building_Detail-Sensitive_Semantic_Segmentation_Networks_With_Polynomial_Pooling_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "505": {"title": "boosting local shape matching for dense 3d face correspondence", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Boosting_Local_Shape_Matching_for_Dense_3D_Face_Correspondence_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1068": {"title": "show, control and tell: a framework for generating controllable and grounded captions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cornia_Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1080": {"title": "task-free continual learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Aljundi_Task-Free_Continual_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "140": {"title": "metacleaner: learning to hallucinate clean representations for noisy-labeled visual recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_MetaCleaner_Learning_to_Hallucinate_Clean_Representations_for_Noisy-Labeled_Visual_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "182": {"title": "typography with decor: intelligent text style transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Typography_With_Decor_Intelligent_Text_Style_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1262": {"title": "spatial-aware graph relation network for large-scale object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Spatial-Aware_Graph_Relation_Network_for_Large-Scale_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1249": {"title": "deep rigid instance scene flow", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ma_Deep_Rigid_Instance_Scene_Flow_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "484": {"title": "densefusion: 6d object pose estimation by iterative dense fusion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_DenseFusion_6D_Object_Pose_Estimation_by_Iterative_Dense_Fusion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "679": {"title": "extreme relative pose estimation for rgb-d scans via scene completion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Extreme_Relative_Pose_Estimation_for_RGB-D_Scans_via_Scene_Completion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "383": {"title": "autoaugment: learning augmentation strategies from data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "920": {"title": "collaborative learning of semi-supervised segmentation and classification for medical images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_Collaborative_Learning_of_Semi-Supervised_Segmentation_and_Classification_for_Medical_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "773": {"title": "unsupervised visual domain adaptation: a deep max-margin gaussian process approach", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Unsupervised_Visual_Domain_Adaptation_A_Deep_Max-Margin_Gaussian_Process_Approach_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "261": {"title": "man: moment alignment network for natural language moment retrieval via iterative graph adjustment", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_MAN_Moment_Alignment_Network_for_Natural_Language_Moment_Retrieval_via_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "935": {"title": "adaptively connected neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Adaptively_Connected_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "853": {"title": "generating multiple hypotheses for 3d human pose estimation with mixture density network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Generating_Multiple_Hypotheses_for_3D_Human_Pose_Estimation_With_Mixture_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "815": {"title": "action4d: online action recognition in the crowd and clutter", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/You_Action4D_Online_Action_Recognition_in_the_Crowd_and_Clutter_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "949": {"title": "generalized zero- and few-shot learning via aligned variational autoencoders", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Schonfeld_Generalized_Zero-_and_Few-Shot_Learning_via_Aligned_Variational_Autoencoders_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1106": {"title": "perceive where to focus: learning visibility-aware part-level features for partial person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Perceive_Where_to_Focus_Learning_Visibility-Aware_Part-Level_Features_for_Partial_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "533": {"title": "an end-to-end network for panoptic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_An_End-To-End_Network_for_Panoptic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "941": {"title": "3d-sis: 3d semantic instance segmentation of rgb-d scans", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hou_3D-SIS_3D_Semantic_Instance_Segmentation_of_RGB-D_Scans_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1117": {"title": "assessment of faster r-cnn in man-machine collaborative search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Deza_Assessment_of_Faster_R-CNN_in_Man-Machine_Collaborative_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "943": {"title": "heterogeneous memory enhanced multimodal attention model for video question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Heterogeneous_Memory_Enhanced_Multimodal_Attention_Model_for_Video_Question_Answering_CVPR_2019_paper.html", "abstract": "In this paper, we propose a novel end-to-end trainable Video Question Answering (VideoQA) framework \nwith three major components: 1) a new heterogeneous memory which can effectively learn global contex\nt information from appearance and motion features; 2) a redesigned question memory which helps under\nstand the complex semantics of question and highlights queried subjects; and 3) a new multimodal fus\nion layer which performs multi-step reasoning by attending to relevant visual and textual hints with\n self-updated attention. Our VideoQA model firstly generates the global context-aware visual and tex\ntual features respectively by interacting current inputs with memory contents. After that, it makes \nthe attentional fusion of the multimodal visual and textual representations to infer the correct ans\nwer. Multiple cycles of reasoning can be made to iteratively refine attention weights of the multimo\ndal data and improve the final representation of the QA pair. Experimental results demonstrate our a\npproach achieves state-of-the-art performance on four VideoQA benchmark datasets.\r", "cite_num": 1}, "567": {"title": "learning transformation synchronization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_Learning_Transformation_Synchronization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1085": {"title": "finding task-relevant features for few-shot learning by category traversal", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Finding_Task-Relevant_Features_for_Few-Shot_Learning_by_Category_Traversal_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "795": {"title": "hierarchical discrete distribution decomposition for match density estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yin_Hierarchical_Discrete_Distribution_Decomposition_for_Match_Density_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "702": {"title": "deepvoxels: learning persistent 3d feature embeddings", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sitzmann_DeepVoxels_Learning_Persistent_3D_Feature_Embeddings_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "142": {"title": "trust region based adversarial attack on neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yao_Trust_Region_Based_Adversarial_Attack_on_Neural_Networks_CVPR_2019_paper.html", "abstract": "Deep Neural Networks are quite vulnerable to adversarial perturbations. Current state-of-the-art adv\nersarial attack methods typically require very time consuming hyper-parameter tuning, or require man\ny iterations to solve an optimization based adversarial attack. To address this problem, we present \na new family of trust region based adversarial attacks, with the goal of computing adversarial pertu\nrbations efficiently. We propose several attacks based on variants of the trust region optimization \nmethod. We test the proposed methods on Cifar-10 and ImageNet datasets using several different model\ns including AlexNet, ResNet-50, VGG-16, and DenseNet-121 models. Our methods achieve comparable resu\nlts with the Carlini-Wagner (CW) attack, but with significant speed up of up to $37\\times$, for the \nVGG-16 model on a Titan Xp GPU. For the case of ResNet-50 on ImageNet, we can bring down its classif\nication accuracy to less than 0.1\\% with at most $1.5\\%$ relative $L_\\infty$ (or $L_2$) perturbation\n requiring only $1.02$ seconds as compared to $27.04$ seconds for the CW attack. We have open source\nd our method which can be accessed at [1].", "cite_num": 1}, "1073": {"title": "understanding and visualizing deep visual saliency models", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Understanding_and_Visualizing_Deep_Visual_Saliency_Models_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "362": {"title": "lbs autoencoder: self-supervised fitting of articulated meshes to point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_LBS_Autoencoder_Self-Supervised_Fitting_of_Articulated_Meshes_to_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "555": {"title": "adversarial attacks beyond the image space", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_Adversarial_Attacks_Beyond_the_Image_Space_CVPR_2019_paper.html", "abstract": "Generating adversarial examples is an intriguing problem and an important way of understanding the w\norking mechanism of deep neural networks. Recently, it has attracted a lot of attention in the compu\nter vision community. Most existing approaches generated perturbations in image space, i.e., each pi\nxel can be modified independently. However, it remains unclear whether these adversarial examples ar\ne authentic, in the sense that they correspond to actual changes in physical properties. #R##N#This \npaper aims at exploring this topic in the contexts of object classification and visual question answ\nering. The baselines are set to be several state-of-the-art deep neural networks which receive 2D in\nput images. We augment these networks with a differentiable 3D rendering layer in front, so that a 3\nD scene (in physical space) is rendered into a 2D image (in image space), and then mapped to a predi\nction (in output space). There are two (direct or indirect) ways of attacking the physical parameter\ns. The former back-propagates the gradients of error signals from output space to physical space dir\nectly, while the latter first constructs an adversary in image space, and then attempts to find the \nbest solution in physical space that is rendered into this image. An important finding is that attac\nking physical space is much more difficult, as the direct method, compared with that used in image s\npace, produces a much lower success rate and requires heavier perturbations to be added. On the othe\nr hand, the indirect method does not work out, suggesting that adversaries generated in image space \nare inauthentic. By interpreting them in physical space, most of these adversaries can be filtered o\nut, showing promise in defending adversaries.", "cite_num": 17}, "557": {"title": "pseudo-lidar from visual depth estimation: bridging the gap in 3d object detection for autonomous driving", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Pseudo-LiDAR_From_Visual_Depth_Estimation_Bridging_the_Gap_in_3D_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "287": {"title": "shifting more attention to video salient object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Shifting_More_Attention_to_Video_Salient_Object_Detection_CVPR_2019_paper.html", "abstract": "The last decade has witnessed a growing interest in video salient object detection (VSOD). However, \nthe research community long-term lacked a well-established VSOD dataset representative of real dynam\nic scenes with high-quality annotations. To address this issue, we elaborately collected a visual-at\ntention-consistent Densely Annotated VSOD (DAVSOD) dataset, which contains 226 videos with 23,938 fr\names that cover diverse realistic-scenes, objects, instances and motions. With corresponding real hu\nman eye-fixation data, we obtain precise ground-truths. This is the first work that explicitly empha\nsizes the challenge of saliency shift, i.e., the video salient object(s) may dynamically change. To \nfurther contribute the community a complete benchmark, we systematically assess 17 representative VS\nOD algorithms over seven existing VSOD datasets and our DAVSOD with totally  84K frames (largest-sca\nle). Utilizing three famous metrics, we then present a comprehensive and insightful performance anal\nysis. Furthermore, we propose a baseline model. It is equipped with a saliency shift- aware convLSTM\n, which can efficiently capture video saliency dynamics through learning human attention-shift behav\nior. Extensive experiments open up promising future directions for model development and comparison.\n\r", "cite_num": 7}, "808": {"title": "efficient parameter-free clustering using first neighbor relations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sarfraz_Efficient_Parameter-Free_Clustering_Using_First_Neighbor_Relations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "705": {"title": "memory in memory: a predictive neural network for learning higher-order non-stationarity from spatiotemporal dynamics", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Memory_in_Memory_A_Predictive_Neural_Network_for_Learning_Higher-Order_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "710": {"title": "composing text and image for image retrieval - an empirical odyssey", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Vo_Composing_Text_and_Image_for_Image_Retrieval_-_an_Empirical_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1063": {"title": "mixed effects neural networks (menets) with applications to gaze estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xiong_Mixed_Effects_Neural_Networks_MeNets_With_Applications_to_Gaze_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "72": {"title": "revealing scenes by inverting structure from motion reconstructions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pittaluga_Revealing_Scenes_by_Inverting_Structure_From_Motion_Reconstructions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "545": {"title": "context-aware crowd counting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Context-Aware_Crowd_Counting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "163": {"title": "argoverse: 3d tracking and forecasting with rich maps", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "826": {"title": "context-reinforced semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_Context-Reinforced_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1246": {"title": "co-occurrent features in semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Co-Occurrent_Features_in_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "608": {"title": "metric learning for image registration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Niethammer_Metric_Learning_for_Image_Registration_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "101": {"title": "cross-modality personalization for retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Murrugarra-Llerena_Cross-Modality_Personalization_for_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1125": {"title": "depth-attentional features for single-image rain removal", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Depth-Attentional_Features_for_Single-Image_Rain_Removal_CVPR_2019_paper.html", "abstract": "Rain is a common weather phenomenon, where object visibility varies with depth from the camera and o\nbjects faraway are visually blocked more by fog than by rain streaks. Existing methods and datasets \nfor rain removal, however, ignore these physical properties, thereby limiting the rain removal effic\niency on real photos. In this work, we first analyze the visual effects of rain subject to scene dep\nth and formulate a rain imaging model collectively with rain streaks and fog; by then, we prepare a \nnew dataset called RainCityscapes with rain streaks and fog on real outdoor photos. Furthermore, we \ndesign an end-to-end deep neural network, where we train it to learn depth-attentional features via \na depth-guided attention mechanism, and regress a residual map to produce the rain-free image output\n. We performed various experiments to visually and quantitatively compare our method with several st\nate-of-the-art methods to demonstrate its superiority over the others.\r", "cite_num": 1}, "1259": {"title": "mirrorgan: learning text-to-image generation by redescription", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qiao_MirrorGAN_Learning_Text-To-Image_Generation_by_Redescription_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "119": {"title": "depth-aware video frame interpolation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bao_Depth-Aware_Video_Frame_Interpolation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "236": {"title": "divergence triangle for joint training of generator model, energy-based model, and inferential model", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Han_Divergence_Triangle_for_Joint_Training_of_Generator_Model_Energy-Based_Model_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1128": {"title": "sdrsac: semidefinite-based randomized approach for robust point cloud registration without correspondences", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Le_SDRSAC_Semidefinite-Based_Randomized_Approach_for_Robust_Point_Cloud_Registration_Without_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "771": {"title": "adcrowdnet: an attention-injective deformable convolutional network for crowd understanding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_ADCrowdNet_An_Attention-Injective_Deformable_Convolutional_Network_for_Crowd_Understanding_CVPR_2019_paper.html", "abstract": "We propose an attention-injective deformable convolutional network called ADCrowdNet for crowd under\nstanding that can address the accuracy degradation problem of highly congested noisy scenes. ADCrowd\nNet contains two concatenated networks. An attention-aware network called Attention Map Generator (A\nMG) first detects crowd regions in images and computes the congestion degree of these regions. Based\n on detected crowd regions and congestion priors, a multi-scale deformable network called Density Ma\np Estimator (DME) then generates high-quality density maps. With the attention-aware training scheme\n and multi-scale deformable convolutional scheme, the proposed ADCrowdNet achieves the capability of\n being more effective to capture the crowd features and more resistant to various noises. We have ev\naluated our method on four popular crowd counting datasets (ShanghaiTech, UCF_CC_50, WorldEXPO'10, a\nnd UCSD) and an extra vehicle counting dataset TRANCOS, and our approach beats existing state-of-the\n-art approaches on all of these datasets.", "cite_num": 3}, "758": {"title": "out-of-distribution detection for generalized zero-shot action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mandal_Out-Of-Distribution_Detection_for_Generalized_Zero-Shot_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "98": {"title": "iterative residual cnns for burst photography applications", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kokkinos_Iterative_Residual_CNNs_for_Burst_Photography_Applications_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "550": {"title": "face-focused cross-stream network for deception detection in videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Face-Focused_Cross-Stream_Network_for_Deception_Detection_in_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "90": {"title": "towards real scene super-resolution with raw images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Towards_Real_Scene_Super-Resolution_With_Raw_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "817": {"title": "learning individual styles of conversational gesture", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ginosar_Learning_Individual_Styles_of_Conversational_Gesture_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "951": {"title": "end-to-end interpretable neural motion planner", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_End-To-End_Interpretable_Neural_Motion_Planner_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "524": {"title": "a theory of fermat paths for non-line-of-sight shape reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xin_A_Theory_of_Fermat_Paths_for_Non-Line-Of-Sight_Shape_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "858": {"title": "deep rnn framework for visual sequential applications", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pang_Deep_RNN_Framework_for_Visual_Sequential_Applications_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1200": {"title": "cycle-consistency for robust visual question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shah_Cycle-Consistency_for_Robust_Visual_Question_Answering_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "74": {"title": "attribute-driven feature disentangling and temporal aggregation for video person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Attribute-Driven_Feature_Disentangling_and_Temporal_Aggregation_for_Video_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "762": {"title": "towards scene understanding: unsupervised monocular depth estimation with semantic-aware representation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Towards_Scene_Understanding_Unsupervised_Monocular_Depth_Estimation_With_Semantic-Aware_Representation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "598": {"title": "signal-to-noise ratio: a robust distance metric for deep metric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yuan_Signal-To-Noise_Ratio_A_Robust_Distance_Metric_for_Deep_Metric_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "404": {"title": "darnet: deep active ray network for building segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cheng_DARNet_Deep_Active_Ray_Network_for_Building_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "927": {"title": "unsupervised event-based learning of optical flow, depth, and egomotion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Unsupervised_Event-Based_Learning_of_Optical_Flow_Depth_and_Egomotion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "365": {"title": "deep modular co-attention networks for visual question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.html", "abstract": "Visual Question Answering (VQA) requires a fine-grained and simultaneous understanding of both the v\nisual content of images and the textual content of questions. Therefore, designing an effective `co-\nattention' model to associate key words in questions with key objects in images is central to VQA pe\nrformance. So far, most successful attempts at co-attention learning have been achieved by using sha\nllow models, and deep co-attention models show little improvement over their shallow counterparts. I\nn this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Atte\nntion (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and imag\nes, as well as the question-guided-attention of images jointly using a modular composition of two ba\nsic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 datas\net and conduct extensive ablation studies to explore the reasons behind MCAN's effectiveness. Experi\nmental results demonstrate that MCAN significantly outperforms the previous state-of-the-art. Our be\nst single model delivers 70.63% overall accuracy on the test-dev set.\r", "cite_num": 2}, "644": {"title": "bag of tricks for image classification with convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "811": {"title": "repmet: representative-based metric learning for classification and few-shot object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Karlinsky_RepMet_Representative-Based_Metric_Learning_for_Classification_and_Few-Shot_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "243": {"title": "ode-inspired network design for single image super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_ODE-Inspired_Network_Design_for_Single_Image_Super-Resolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "430": {"title": "elastic boundary projection for 3d medical image segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ni_Elastic_Boundary_Projection_for_3D_Medical_Image_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "867": {"title": "weakly supervised image classification through noise regularization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Weakly_Supervised_Image_Classification_Through_Noise_Regularization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1256": {"title": "rare event detection using disentangled representation learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hamaguchi_Rare_Event_Detection_Using_Disentangled_Representation_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "544": {"title": "non-local meets global: an integrated paradigm for hyperspectral denoising", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Non-Local_Meets_Global_An_Integrated_Paradigm_for_Hyperspectral_Denoising_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "639": {"title": "semantic component decomposition for face attribute manipulation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Semantic_Component_Decomposition_for_Face_Attribute_Manipulation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "514": {"title": "bringing alive blurred moments", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Purohit_Bringing_Alive_Blurred_Moments_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "979": {"title": "deepco3: deep instance co-segmentation by co-peak search and co-saliency detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hsu_DeepCO3_Deep_Instance_Co-Segmentation_by_Co-Peak_Search_and_Co-Saliency_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "717": {"title": "polynomial representation for persistence diagram", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Polynomial_Representation_for_Persistence_Diagram_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "126": {"title": "superquadrics revisited: learning 3d shape parsing beyond cuboids", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Paschalidou_Superquadrics_Revisited_Learning_3D_Shape_Parsing_Beyond_Cuboids_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "731": {"title": "pointpillars: fast encoders for object detection from point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lang_PointPillars_Fast_Encoders_for_Object_Detection_From_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "322": {"title": "end-to-end supervised product quantization for image search and retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Klein_End-To-End_Supervised_Product_Quantization_for_Image_Search_and_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "210": {"title": "networks for joint affine and non-parametric image registration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shen_Networks_for_Joint_Affine_and_Non-Parametric_Image_Registration_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "678": {"title": "snapshot distillation: teacher-student optimization in one generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Snapshot_Distillation_Teacher-Student_Optimization_in_One_Generation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "565": {"title": "hybrid scene compression for visual localization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Camposeco_Hybrid_Scene_Compression_for_Visual_Localization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1190": {"title": "domain generalization by solving jigsaw puzzles", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Carlucci_Domain_Generalization_by_Solving_Jigsaw_Puzzles_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "499": {"title": "quantization networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Quantization_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "462": {"title": "rob-gan: generator, discriminator, and adversarial attacker", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Rob-GAN_Generator_Discriminator_and_Adversarial_Attacker_CVPR_2019_paper.html", "abstract": "We study two important concepts in adversarial deep learning---adversarial training and generative a\ndversarial network (GAN).  Adversarial training is the technique used to improve the robustness of d\niscriminator by combining adversarial attacker and discriminator in the training phase.  GAN is comm\nonly used for image generation by jointly optimizing discriminator and generator. We show these two \nconcepts are indeed closely related and can be used to strengthen each other---adding a generator to\n the adversarial training procedure can improve the robustness of discriminators, and adding an adve\nrsarial attack to GAN training can improve the convergence speed and lead to better generators. Comb\nining these two insights, we develop a framework called Rob-GAN to jointly optimize generator and di\nscriminator in the presence of adversarial attacks---the generator generates fake images to fool dis\ncriminator; the adversarial attacker perturbs real images to fool discriminator, and the discriminat\nor wants to minimize loss under fake and adversarial images. Through this end-to-end training proced\nure, we are able to simultaneously improve the convergence speed of GAN training, the quality of syn\nthetic images, and the robustness of discriminator under strong adversarial attacks. Experimental re\nsults demonstrate that the obtained classifier is more robust than the state-of-the-art adversarial \ntraining approach (Madry 2017), and the generator outperforms SN-GAN on ImageNet-143.\r", "cite_num": 0}, "703": {"title": "target-aware deep tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Target-Aware_Deep_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "982": {"title": "striking the right balance with uncertainty", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Khan_Striking_the_Right_Balance_With_Uncertainty_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "269": {"title": "large-scale interactive object segmentation with human annotators", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Benenson_Large-Scale_Interactive_Object_Segmentation_With_Human_Annotators_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1038": {"title": "exploiting temporal context for 3d human pose estimation in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Arnab_Exploiting_Temporal_Context_for_3D_Human_Pose_Estimation_in_the_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "466": {"title": "amodal instance segmentation with kins dataset", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qi_Amodal_Instance_Segmentation_With_KINS_Dataset_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "623": {"title": "photometric mesh optimization for video-aligned 3d object reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_Photometric_Mesh_Optimization_for_Video-Aligned_3D_Object_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "433": {"title": "conditional single-view shape generation for multi-view stereo reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wei_Conditional_Single-View_Shape_Generation_for_Multi-View_Stereo_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "666": {"title": "cam-convs: camera-aware multi-scale convolutions for single-view depth", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Facil_CAM-Convs_Camera-Aware_Multi-Scale_Convolutions_for_Single-View_Depth_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "643": {"title": "interaction-and-aggregation network for person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hou_Interaction-And-Aggregation_Network_for_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "273": {"title": "neural rerendering in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Meshry_Neural_Rerendering_in_the_Wild_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1095": {"title": "dynamic scene deblurring with parameter selective sharing and nested skip connections", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Dynamic_Scene_Deblurring_With_Parameter_Selective_Sharing_and_Nested_Skip_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "100": {"title": "understanding the limitations of cnn-based absolute camera pose regression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sattler_Understanding_the_Limitations_of_CNN-Based_Absolute_Camera_Pose_Regression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "857": {"title": "rethinking the evaluation of video summaries", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Otani_Rethinking_the_Evaluation_of_Video_Summaries_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1108": {"title": "self-supervised representation learning by rotation feature decoupling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Feng_Self-Supervised_Representation_Learning_by_Rotation_Feature_Decoupling_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "552": {"title": "emotion-aware human attention prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cordel_Emotion-Aware_Human_Attention_Prediction_CVPR_2019_paper.html", "abstract": "Despite the recent success in face recognition and object classification, in the field of human gaze\n prediction, computer models are still struggling to accurately mimic human attention. One main reas\non is that visual attention is a complex human behavior influenced by multiple factors, ranging from\n low-level features (e.g., color, contrast) to high-level human perception (e.g., objects interactio\nns, object sentiment), making it difficult to model computationally. In this work, we investigate th\ne relation between object sentiment and human attention. We first introduce a new evaluation metric \n(AttI) for measuring human attention that focuses on human fixation consensus. A series of empirical\n data analyses with AttI indicate that emotion-evoking objects receive attention favor, especially w\nhen they co-occur with emotionally-neutral objects, and this favor varies with different image compl\nexity. Based on the empirical analyses, we design a deep neural network for human attention predicti\non which allows the attention bias on emotion-evoking objects to be encoded in its feature space. Ex\nperiments on two benchmark datasets demonstrate its superior performance, especially on metrics that\n evaluate relative importance of salient regions. This research provides the clearest picture to dat\ne on how object sentiments influence human attention, and it makes one of the first attempts to mode\nl this phenomenon computationally.\r", "cite_num": 0}, "177": {"title": "marginalized latent semantic encoder for zero-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Marginalized_Latent_Semantic_Encoder_for_Zero-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1236": {"title": "accel: a corrective fusion network for efficient semantic segmentation on video", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jain_Accel_A_Corrective_Fusion_Network_for_Efficient_Semantic_Segmentation_on_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1130": {"title": "stereo r-cnn based 3d object detection for autonomous driving", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Stereo_R-CNN_Based_3D_Object_Detection_for_Autonomous_Driving_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "796": {"title": "fbnet: hardware-aware efficient convnet design via differentiable neural architecture search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_FBNet_Hardware-Aware_Efficient_ConvNet_Design_via_Differentiable_Neural_Architecture_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "414": {"title": "generalizing eye tracking with bayesian adversarial learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Generalizing_Eye_Tracking_With_Bayesian_Adversarial_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "292": {"title": "3d local features for direct pairwise registration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Deng_3D_Local_Features_for_Direct_Pairwise_Registration_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1160": {"title": "adaptive confidence smoothing for generalized zero-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Atzmon_Adaptive_Confidence_Smoothing_for_Generalized_Zero-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "904": {"title": "hierarchical deep stereo matching on high-resolution images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Hierarchical_Deep_Stereo_Matching_on_High-Resolution_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "700": {"title": "long-term feature banks for detailed video understanding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Long-Term_Feature_Banks_for_Detailed_Video_Understanding_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "836": {"title": "precise detection in densely packed scenes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Goldman_Precise_Detection_in_Densely_Packed_Scenes_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "207": {"title": "moving object detection under discontinuous change in illumination using tensor low-rank and invariant sparse decomposition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shakeri_Moving_Object_Detection_Under_Discontinuous_Change_in_Illumination_Using_Tensor_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1212": {"title": "learning single-image depth from videos using quality assessment networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Learning_Single-Image_Depth_From_Videos_Using_Quality_Assessment_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "402": {"title": "hierarchy denoising recursive autoencoders for 3d scene layout prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Hierarchy_Denoising_Recursive_Autoencoders_for_3D_Scene_Layout_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "467": {"title": "greedy structure learning of hierarchical compositional models", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kortylewski_Greedy_Structure_Learning_of_Hierarchical_Compositional_Models_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "471": {"title": "multi-source weak supervision for saliency detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_Multi-Source_Weak_Supervision_for_Saliency_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "612": {"title": "assessing personally perceived image quality via image features and collaborative filtering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Korhonen_Assessing_Personally_Perceived_Image_Quality_via_Image_Features_and_Collaborative_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "770": {"title": "unsupervised open domain recognition by semantic discrepancy minimization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhuo_Unsupervised_Open_Domain_Recognition_by_Semantic_Discrepancy_Minimization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1118": {"title": "variational convolutional neural network pruning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Variational_Convolutional_Neural_Network_Pruning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1172": {"title": "retrieval-augmented convolutional neural networks against adversarial examples", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Retrieval-Augmented_Convolutional_Neural_Networks_Against_Adversarial_Examples_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "921": {"title": "putting humans in a scene: learning affordance in 3d indoor environments", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Putting_Humans_in_a_Scene_Learning_Affordance_in_3D_Indoor_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1016": {"title": "partnet: a recursive part decomposition network for fine-grained and hierarchical shape segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_PartNet_A_Recursive_Part_Decomposition_Network_for_Fine-Grained_and_Hierarchical_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "376": {"title": "maxpoolnms: getting rid of nms bottlenecks in two-stage object detectors", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cai_MaxpoolNMS_Getting_Rid_of_NMS_Bottlenecks_in_Two-Stage_Object_Detectors_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1268": {"title": "frame-consistent recurrent video deraining with dual-level flow", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Frame-Consistent_Recurrent_Video_Deraining_With_Dual-Level_Flow_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1144": {"title": "probabilistic end-to-end noise correction for learning with noisy labels", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yi_Probabilistic_End-To-End_Noise_Correction_for_Learning_With_Noisy_Labels_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "156": {"title": "deep robust subjective visual property prediction in crowdsourcing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Deep_Robust_Subjective_Visual_Property_Prediction_in_Crowdsourcing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "646": {"title": "multi-step prediction of occupancy grid maps with recurrent neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mohajerin_Multi-Step_Prediction_of_Occupancy_Grid_Maps_With_Recurrent_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "938": {"title": "acoustic non-line-of-sight imaging", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lindell_Acoustic_Non-Line-Of-Sight_Imaging_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "186": {"title": "a style-based generator architecture for generative adversarial networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1269": {"title": "a relation-augmented fully convolutional network for semantic segmentation in aerial scenes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mou_A_Relation-Augmented_Fully_Convolutional_Network_for_Semantic_Segmentation_in_Aerial_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "601": {"title": "interactive full image segmentation by considering all regions jointly", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Agustsson_Interactive_Full_Image_Segmentation_by_Considering_All_Regions_Jointly_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "919": {"title": "towards optimal structured cnn pruning via generative adversarial learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_Towards_Optimal_Structured_CNN_Pruning_via_Generative_Adversarial_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "303": {"title": "detect-to-retrieve: efficient regional aggregation for image search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Teichmann_Detect-To-Retrieve_Efficient_Regional_Aggregation_for_Image_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1075": {"title": "unsupervised moving object detection via contextual information separation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Unsupervised_Moving_Object_Detection_via_Contextual_Information_Separation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "367": {"title": "diversify and match: a domain adaptive representation learning paradigm for object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Diversify_and_Match_A_Domain_Adaptive_Representation_Learning_Paradigm_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1107": {"title": "ddlstm: dual-domain lstm for cross-dataset action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Perrett_DDLSTM_Dual-Domain_LSTM_for_Cross-Dataset_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "863": {"title": "example-guided style-consistent image synthesis from semantic labeling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Example-Guided_Style-Consistent_Image_Synthesis_From_Semantic_Labeling_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "580": {"title": "spherephd: applying cnns on a spherical polyhedron representation of 360deg images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_SpherePHD_Applying_CNNs_on_a_Spherical_PolyHeDron_Representation_of_360deg_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "227": {"title": "inserting videos into videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_Inserting_Videos_Into_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "14": {"title": "mmface: a multi-metric regression network for unconstrained face reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yi_MMFace_A_Multi-Metric_Regression_Network_for_Unconstrained_Face_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "47": {"title": "learning from synthetic data for crowd counting in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_From_Synthetic_Data_for_Crowd_Counting_in_the_Wild_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "497": {"title": "the alignment of the spheres: globally-optimal spherical mixture alignment for camera pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Campbell_The_Alignment_of_the_Spheres_Globally-Optimal_Spherical_Mixture_Alignment_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1121": {"title": "veritatem dies aperit - temporally consistent depth prediction enabled by a multi-task geometric and semantic scene understanding approach", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Atapour-Abarghouei_Veritatem_Dies_Aperit_-_Temporally_Consistent_Depth_Prediction_Enabled_by_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "439": {"title": "classification-reconstruction learning for open-set recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yoshihashi_Classification-Reconstruction_Learning_for_Open-Set_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "540": {"title": "blind super-resolution with iterative kernel correction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gu_Blind_Super-Resolution_With_Iterative_Kernel_Correction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1067": {"title": "dmc-net: generating discriminative motion cues for fast compressed video action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shou_DMC-Net_Generating_Discriminative_Motion_Cues_for_Fast_Compressed_Video_Action_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1270": {"title": "predicting future frames using retrospective cycle gan", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kwon_Predicting_Future_Frames_Using_Retrospective_Cycle_GAN_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "129": {"title": "deep charuco: dark charuco marker pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Deep_ChArUco_Dark_ChArUco_Marker_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "120": {"title": "learning to cluster faces on an affinity graph", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Learning_to_Cluster_Faces_on_an_Affinity_Graph_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "560": {"title": "temporal transformer networks: joint learning of invariant and discriminative time warping", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lohit_Temporal_Transformer_Networks_Joint_Learning_of_Invariant_and_Discriminative_Time_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "436": {"title": "unsupervised deep epipolar flow for stationary or dynamic scenes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhong_Unsupervised_Deep_Epipolar_Flow_for_Stationary_or_Dynamic_Scenes_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1247": {"title": "natural and realistic single image super-resolution with explicit natural manifold discrimination", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Soh_Natural_and_Realistic_Single_Image_Super-Resolution_With_Explicit_Natural_Manifold_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "19": {"title": "mind your neighbours: image annotation with metadata neighbourhood graph co-attention networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Mind_Your_Neighbours_Image_Annotation_With_Metadata_Neighbourhood_Graph_Co-Attention_CVPR_2019_paper.html", "abstract": "As the visual reflections of our daily lives, images are frequently shared on the social network, wh\nich generates the abundant 'metadata' that records user interactions with images. Due to the diverse\n contents and complex styles, some images can be challenging to recognise when neglecting the contex\nt. Images with the similar metadata, such as 'relevant topics and textual descriptions', 'common fri\nends of users' and 'nearby locations', form a neighbourhood for each image, which can be used to ass\nist the annotation. In this paper, we propose a Metadata Neighbourhood Graph Co-Attention Network (M\nangoNet) to model the correlations between each target image and its neighbours. To accurately captu\nre the visual clues from the neighbourhood, a co-attention mechanism is introduced to embed the targ\net image and its neighbours as graph nodes, while the graph edges capture the node pair correlations\n. By reasoning on the neighbourhood graph, we obtain the graph representation to help annotate the t\narget image. Experimental results on three benchmark datasets indicate that our proposed model achie\nves the best performance compared to the state-of-the-art methods.\r", "cite_num": 1}, "965": {"title": "learning roi transformer for oriented object detection in aerial images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Learning_RoI_Transformer_for_Oriented_Object_Detection_in_Aerial_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "299": {"title": "taking a closer look at domain shift: category-level adversaries for semantics consistent domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Luo_Taking_a_Closer_Look_at_Domain_Shift_Category-Level_Adversaries_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "802": {"title": "toward convolutional blind denoising of real photographs", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_Toward_Convolutional_Blind_Denoising_of_Real_Photographs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "844": {"title": "fully automatic video colorization with self-regularization and diversity", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lei_Fully_Automatic_Video_Colorization_With_Self-Regularization_and_Diversity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1251": {"title": "balanced self-paced learning for generative adversarial clustering network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ghasedi_Balanced_Self-Paced_Learning_for_Generative_Adversarial_Clustering_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "154": {"title": "ensemble deep manifold similarity learning using hard proxies", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Aziere_Ensemble_Deep_Manifold_Similarity_Learning_Using_Hard_Proxies_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "432": {"title": "intention oriented image captions with guiding objects", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Intention_Oriented_Image_Captions_With_Guiding_Objects_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1272": {"title": "practical coding function design for time-of-flight imaging", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gutierrez-Barragan_Practical_Coding_Function_Design_for_Time-Of-Flight_Imaging_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "647": {"title": "overcoming limitations of mixture density networks: a sampling and fitting framework for multimodal future prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Makansi_Overcoming_Limitations_of_Mixture_Density_Networks_A_Sampling_and_Fitting_CVPR_2019_paper.html", "abstract": "Future prediction is a fundamental principle of intelligence that helps plan actions and avoid possi\nble dangers. As the future is uncertain to a large extent, modeling the uncertainty and multimodalit\ny of the future states is of great relevance. Existing approaches are rather limited in this regard \nand mostly yield a single hypothesis of the future or, at the best, strongly constrained mixture com\nponents that suffer from instabilities in training and mode collapse. In this work, we present an ap\nproach that involves the prediction of several samples of the future with a winner-takes-all loss an\nd iterative grouping of samples to multiple modes. Moreover, we discuss how to evaluate predicted mu\nltimodal distributions, including the common real scenario, where only a single sample from the grou\nnd-truth distribution is available for evaluation. We show on synthetic and real data that the propo\nsed approach triggers good estimates of multimodal distributions and avoids mode collapse. \r", "cite_num": 1}, "469": {"title": "transferrable prototypical networks for unsupervised domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pan_Transferrable_Prototypical_Networks_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "962": {"title": "weakly supervised open-set domain adaptation by dual-domain collaboration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tan_Weakly_Supervised_Open-Set_Domain_Adaptation_by_Dual-Domain_Collaboration_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "910": {"title": "fa-rpn: floating region proposals for face detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Najibi_FA-RPN_Floating_Region_Proposals_for_Face_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "456": {"title": "modulating image restoration with continual levels via adaptive feature modification layers", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Modulating_Image_Restoration_With_Continual_Levels_via_Adaptive_Feature_Modification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "197": {"title": "the domain transform solver", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bapat_The_Domain_Transform_Solver_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "476": {"title": "siamese cascaded region proposal networks for real-time visual tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Siamese_Cascaded_Region_Proposal_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "963": {"title": "haq: hardware-aware automated quantization with mixed precision", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "658": {"title": "the visual centrifuge: model-free layered video representations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Alayrac_The_Visual_Centrifuge_Model-Free_Layered_Video_Representations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "755": {"title": "strike (with) a pose: neural networks are easily fooled by strange poses of familiar objects", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Alcorn_Strike_With_a_Pose_Neural_Networks_Are_Easily_Fooled_by_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1134": {"title": "monocular 3d object detection leveraging accurate proposals and shape reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ku_Monocular_3D_Object_Detection_Leveraging_Accurate_Proposals_and_Shape_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1243": {"title": "student becoming the master: knowledge amalgamation for joint scene parsing, depth estimation, and more", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ye_Student_Becoming_the_Master_Knowledge_Amalgamation_for_Joint_Scene_Parsing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "256": {"title": "rvos: end-to-end recurrent network for video object segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ventura_RVOS_End-To-End_Recurrent_Network_for_Video_Object_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "218": {"title": "social relation recognition from videos via multi-scale spatial-temporal reasoning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Social_Relation_Recognition_From_Videos_via_Multi-Scale_Spatial-Temporal_Reasoning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "78": {"title": "self-supervised convolutional subspace clustering network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Self-Supervised_Convolutional_Subspace_Clustering_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "732": {"title": "sodeep: a sorting deep net to learn ranking loss surrogates", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Engilberge_SoDeep_A_Sorting_Deep_Net_to_Learn_Ranking_Loss_Surrogates_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "59": {"title": "sosnet: second order similarity regularization for local descriptor learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tian_SOSNet_Second_Order_Similarity_Regularization_for_Local_Descriptor_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1030": {"title": "rl-gan-net: a reinforcement learning agent controlled gan network for real-time point cloud shape completion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sarmad_RL-GAN-Net_A_Reinforcement_Learning_Agent_Controlled_GAN_Network_for_Real-Time_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "116": {"title": "invariance matters: exemplar memory for domain adaptive person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhong_Invariance_Matters_Exemplar_Memory_for_Domain_Adaptive_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1102": {"title": "spatially variant linear representation models for joint filtering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pan_Spatially_Variant_Linear_Representation_Models_for_Joint_Filtering_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "136": {"title": "you look twice: gaternet for dynamic filter selection in cnns", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_You_Look_Twice_GaterNet_for_Dynamic_Filter_Selection_in_CNNs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "861": {"title": "touchdown: natural language navigation and spatial reasoning in visual street environments", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_TOUCHDOWN_Natural_Language_Navigation_and_Spatial_Reasoning_in_Visual_Street_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "686": {"title": "sampling techniques for large-scale object detection from sparsely annotated objects", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Niitani_Sampling_Techniques_for_Large-Scale_Object_Detection_From_Sparsely_Annotated_Objects_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "385": {"title": "inverse discriminative networks for handwritten signature verification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wei_Inverse_Discriminative_Networks_for_Handwritten_Signature_Verification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "548": {"title": "douglas-rachford networks: learning both the image prior and data fidelity terms for blind image deconvolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Aljadaany_Douglas-Rachford_Networks_Learning_Both_the_Image_Prior_and_Data_Fidelity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1043": {"title": "explore-exploit graph traversal for image retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Explore-Exploit_Graph_Traversal_for_Image_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "714": {"title": "content authentication for neural imaging pipelines: end-to-end optimization of photo provenance in complex distribution channels", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Korus_Content_Authentication_for_Neural_Imaging_Pipelines_End-To-End_Optimization_of_Photo_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1283": {"title": "on the structural sensitivity of deep convolutional networks to the directions of fourier basis functions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tsuzuku_On_the_Structural_Sensitivity_of_Deep_Convolutional_Networks_to_the_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "809": {"title": "defense against adversarial images using web-scale nearest-neighbor search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dubey_Defense_Against_Adversarial_Images_Using_Web-Scale_Nearest-Neighbor_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "231": {"title": "discovering visual patterns in art collections with spatially-consistent feature learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shen_Discovering_Visual_Patterns_in_Art_Collections_With_Spatially-Consistent_Feature_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "285": {"title": "selective sensor fusion for neural visual-inertial odometry", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Selective_Sensor_Fusion_for_Neural_Visual-Inertial_Odometry_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "583": {"title": "beyond tracking: selecting memory and refining poses for deep visual odometry", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xue_Beyond_Tracking_Selecting_Memory_and_Refining_Poses_for_Deep_Visual_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1060": {"title": "stereodrnet: dilated residual stereonet", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chabra_StereoDRNet_Dilated_Residual_StereoNet_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "457": {"title": "graph-based global reasoning networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Graph-Based_Global_Reasoning_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "193": {"title": "knowledge distillation via instance relationship graph", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "704": {"title": "distillhash: unsupervised deep hashing by distilling data pairs", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_DistillHash_Unsupervised_Deep_Hashing_by_Distilling_Data_Pairs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "739": {"title": "learning to synthesize motion blur", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Brooks_Learning_to_Synthesize_Motion_Blur_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "166": {"title": "abc: a big cad model dataset for geometric deep learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Koch_ABC_A_Big_CAD_Model_Dataset_for_Geometric_Deep_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "262": {"title": "supervised fitting of geometric primitives to 3d point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Supervised_Fitting_of_Geometric_Primitives_to_3D_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "263": {"title": "feature distillation: dnn-oriented jpeg compression against adversarial examples", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Feature_Distillation_DNN-Oriented_JPEG_Compression_Against_Adversarial_Examples_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "244": {"title": "part-regularized near-duplicate vehicle re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Part-Regularized_Near-Duplicate_Vehicle_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "812": {"title": "learning not to learn: training deep neural networks with biased data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Learning_Not_to_Learn_Training_Deep_Neural_Networks_With_Biased_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "36": {"title": "not using the car to see the sidewalk -- quantifying and controlling the effects of context in classification and segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shetty_Not_Using_the_Car_to_See_the_Sidewalk_--_Quantifying_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "768": {"title": "cross-atlas convolution for parameterization invariant learning on textured mesh surface", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Cross-Atlas_Convolution_for_Parameterization_Invariant_Learning_on_Textured_Mesh_Surface_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "648": {"title": "edge-labeling graph neural network for few-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Edge-Labeling_Graph_Neural_Network_for_Few-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "288": {"title": "d-sne: domain adaptation using stochastic neighborhood embedding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_d-SNE_Domain_Adaptation_Using_Stochastic_Neighborhood_Embedding_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "400": {"title": "ppgnet: learning point-pair graph for line segment detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_PPGNet_Learning_Point-Pair_Graph_for_Line_Segment_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "102": {"title": "geometry-aware distillation for indoor semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jiao_Geometry-Aware_Distillation_for_Indoor_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "339": {"title": "attention based glaucoma detection: a large-scale database and cnn model", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Attention_Based_Glaucoma_Detection_A_Large-Scale_Database_and_CNN_Model_CVPR_2019_paper.html", "abstract": "Recently, the attention mechanism has been successfully applied in convolutional neural networks (CN\nNs), significantly boosting the performance of many computer vision tasks. Unfortunately, few medica\nl image recognition approaches incorporate the attention mechanism in the CNNs. In particular, there\n exists high redundancy in fundus images for glaucoma detection, such that the attention mechanism h\nas potential in improving the performance of CNN-based glaucoma detection. This paper proposes an at\ntention-based CNN for glaucoma detection (AG-CNN). Specifically, we first establish a large-scale at\ntention based glaucoma (LAG) database, which includes 5,824 fundus images labeled with either positi\nve glaucoma (2,392) or negative glaucoma (3,432). The attention maps of the ophthalmologists are als\no collected in LAG database through a simulated eye-tracking experiment. Then, a new structure of AG\n-CNN is designed, including an attention prediction subnet, a pathological area localization subnet \nand a glaucoma classification subnet. Different from other attention-based CNN methods, the features\n are also visualized as the localized pathological area, which can advance the performance of glauco\nma detection. Finally, the experiment results show that the proposed AG-CNN approach significantly a\ndvances state-of-the-art glaucoma detection.", "cite_num": 1}, "737": {"title": "fast online object tracking and segmentation: a unifying approach", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "800": {"title": "scops: self-supervised co-part segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hung_SCOPS_Self-Supervised_Co-Part_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "569": {"title": "video action transformer network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Girdhar_Video_Action_Transformer_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "752": {"title": "deep surface normal estimation with hierarchical rgb-d fusion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_Deep_Surface_Normal_Estimation_With_Hierarchical_RGB-D_Fusion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "764": {"title": "pies: pose invariant embeddings", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ho_PIEs_Pose_Invariant_Embeddings_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "988": {"title": "interactive image segmentation via backpropagating refinement scheme", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jang_Interactive_Image_Segmentation_via_Backpropagating_Refinement_Scheme_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1156": {"title": "scene parsing via integrated classification model and variance-based regularization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Scene_Parsing_via_Integrated_Classification_Model_and_Variance-Based_Regularization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "103": {"title": "video generation from single semantic label map", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pan_Video_Generation_From_Single_Semantic_Label_Map_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "821": {"title": "mapping, localization and path planning for image-based navigation using visual features and map", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Thoma_Mapping_Localization_and_Path_Planning_for_Image-Based_Navigation_Using_Visual_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1223": {"title": "structured pruning of neural networks with budget-aware regularization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lemaire_Structured_Pruning_of_Neural_Networks_With_Budget-Aware_Regularization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "386": {"title": "towards universal object detection by domain attention", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Towards_Universal_Object_Detection_by_Domain_Attention_CVPR_2019_paper.html", "abstract": "Despite increasing efforts on universal representations for visual recognition, few have addressed o\nbject detection. In this paper, we develop an effective and efficient universal object detection sys\ntem that is capable of working on various image domains, from human faces and traffic signs to medic\nal CT images. Unlike multi-domain models, this universal model does not require prior knowledge of t\nhe domain of interest. This is achieved by the introduction of a new family of adaptation layers, ba\nsed on the principles of squeeze and excitation, and a new domain-attention mechanism. In the propos\ned universal detector, all parameters and computations are shared across domains, and a single netwo\nrk processes all domains all the time. Experiments, on a newly established universal object detectio\nn benchmark of 11 diverse datasets, show that the proposed detector outperforms a bank of individual\n detectors, a multi-domain detector, and a baseline universal detector, with a 1.3x parameter increa\nse over a single-domain baseline detector. The code and benchmark will be released at this http URL.\n", "cite_num": 3}, "357": {"title": "fsa-net: learning fine-grained structure aggregation for head pose estimation from a single image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_FSA-Net_Learning_Fine-Grained_Structure_Aggregation_for_Head_Pose_Estimation_From_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1257": {"title": "all you need is a few shifts: designing efficient convolutional neural networks for image classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_All_You_Need_Is_a_Few_Shifts_Designing_Efficient_Convolutional_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "688": {"title": "co-saliency detection via mask-guided fully convolutional networks with multi-scale label smoothing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Co-Saliency_Detection_via_Mask-Guided_Fully_Convolutional_Networks_With_Multi-Scale_Label_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "804": {"title": "relation-shape convolutional neural network for point cloud analysis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Relation-Shape_Convolutional_Neural_Network_for_Point_Cloud_Analysis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1193": {"title": "3dn: 3d deformation network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_3DN_3D_Deformation_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "356": {"title": "towards accurate one-stage object detection with ap-loss", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Towards_Accurate_One-Stage_Object_Detection_With_AP-Loss_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "109": {"title": "biologically-constrained graphs for global connectomics reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Matejek_Biologically-Constrained_Graphs_for_Global_Connectomics_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "71": {"title": "decorrelated adversarial learning for age-invariant face recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Decorrelated_Adversarial_Learning_for_Age-Invariant_Face_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1004": {"title": "multimodal explanations by predicting counterfactuality in videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kanehira_Multimodal_Explanations_by_Predicting_Counterfactuality_in_Videos_CVPR_2019_paper.html", "abstract": "This study addresses generating counterfactual explanations with multimodal information. Our goal is\n not only to classify a video into a specific category, but also to provide explanations on why it i\ns not categorized to a specific class with combinations of visual-linguistic information. Requiremen\nts that the expected output should satisfy are referred to as counterfactuality in this paper: (1) C\nompatibility of visual-linguistic explanations, and (2) Positiveness/negativeness for the specific p\nositive/negative class. Exploiting a spatio-temporal region (tube) and an attribute as visual and li\nnguistic explanations respectively, the explanation model is trained to predict the counterfactualit\ny for possible combinations of multimodal information in a post-hoc manner. The optimization problem\n, which appears during training/inference, can be efficiently solved by inserting a novel neural net\nwork layer, namely the maximum subpath layer. We demonstrated the effectiveness of this method by co\nmparison with a baseline of the action recognition datasets extended for this task. Moreover, we pro\nvide information-theoretical insight into the proposed method.\r", "cite_num": 1}, "181": {"title": "d3tw: discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_D3TW_Discriminative_Differentiable_Dynamic_Time_Warping_for_Weakly_Supervised_Action_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "562": {"title": "pa3d: pose-action 3d machine for video recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "160": {"title": "towards rich feature discovery with class activation maps augmentation for person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Towards_Rich_Feature_Discovery_With_Class_Activation_Maps_Augmentation_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "875": {"title": "c-mil: continuation multiple instance learning for weakly supervised object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wan_C-MIL_Continuation_Multiple_Instance_Learning_for_Weakly_Supervised_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "127": {"title": "hierarchical disentanglement of discriminative latent features for zero-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tong_Hierarchical_Disentanglement_of_Discriminative_Latent_Features_for_Zero-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "225": {"title": "triangulation learning network: from monocular to stereo 3d object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qin_Triangulation_Learning_Network_From_Monocular_to_Stereo_3D_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "480": {"title": "not all frames are equal: weakly-supervised video grounding with contextual similarity and visual clustering losses", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Not_All_Frames_Are_Equal_Weakly-Supervised_Video_Grounding_With_Contextual_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "584": {"title": "learning loss for active learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yoo_Learning_Loss_for_Active_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "881": {"title": "dichromatic model based temporal color constancy for ac light sources", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yoo_Dichromatic_Model_Based_Temporal_Color_Constancy_for_AC_Light_Sources_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "670": {"title": "learning pyramid-context encoder network for high-quality image inpainting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_Learning_Pyramid-Context_Encoder_Network_for_High-Quality_Image_Inpainting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "219": {"title": "nettailor: tuning the architecture, not just the weights", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Morgado_NetTailor_Tuning_the_Architecture_Not_Just_the_Weights_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1010": {"title": "canet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_CANet_Class-Agnostic_Segmentation_Networks_With_Iterative_Refinement_and_Attentive_Few-Shot_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1061": {"title": "exact adversarial attack to image captioning via structured output learning with latent variables", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Exact_Adversarial_Attack_to_Image_Captioning_via_Structured_Output_Learning_CVPR_2019_paper.html", "abstract": "In this work, we study the robustness of a CNN+RNN based image captioning system being subjected to \nadversarial noises. We propose to fool an image captioning system to generate some targeted partial \ncaptions for an image polluted by adversarial noises, even the targeted captions are totally irrelev\nant to the image content. A partial caption indicates that the words at some locations in this capti\non are observed, while words at other locations are not restricted. It is the first work to study ex\nact adversarial attacks of targeted partial captions. Due to the sequential dependencies among words\n in a caption, we formulate the generation of adversarial noises for targeted partial captions as a \nstructured output learning problem with latent variables. Both the generalized expectation maximizat\nion algorithm and structural SVMs with latent variables are then adopted to optimize the problem. Th\ne proposed methods generate very successful attacks to three popular CNN+RNN based image captioning \nmodels. Furthermore, the proposed attack methods are used to understand the inner mechanism of image\n captioning systems, providing the guidance to further improve automatic image captioning systems to\nwards human captioning.\r", "cite_num": 0}, "1267": {"title": "deep plug-and-play super-resolution for arbitrary blur kernels", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deep_Plug-And-Play_Super-Resolution_for_Arbitrary_Blur_Kernels_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "121": {"title": "learning to regress 3d face shape and expression from an image without 3d supervision", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sanyal_Learning_to_Regress_3D_Face_Shape_and_Expression_From_an_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "590": {"title": "adaptive transfer network for cross-domain person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Adaptive_Transfer_Network_for_Cross-Domain_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "165": {"title": "attentive single-tasking of multiple tasks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Maninis_Attentive_Single-Tasking_of_Multiple_Tasks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1220": {"title": "lvis: a dataset for large vocabulary instance segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gupta_LVIS_A_Dataset_for_Large_Vocabulary_Instance_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "494": {"title": "feedback adversarial learning: spatial feedback for improving generative adversarial networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huh_Feedback_Adversarial_Learning_Spatial_Feedback_for_Improving_Generative_Adversarial_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1023": {"title": "unsupervised deep tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Unsupervised_Deep_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "975": {"title": "k-nearest neighbors hashing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_K-Nearest_Neighbors_Hashing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "825": {"title": "aggregation cross-entropy for sequence recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Aggregation_Cross-Entropy_for_Sequence_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "742": {"title": "constrained generative adversarial networks for interactive image generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Heim_Constrained_Generative_Adversarial_Networks_for_Interactive_Image_Generation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "701": {"title": "refine and distill: exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pilzer_Refine_and_Distill_Exploiting_Cycle-Inconsistency_and_Knowledge_Distillation_for_Unsupervised_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "150": {"title": "parsing r-cnn for instance-level human analysis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Parsing_R-CNN_for_Instance-Level_Human_Analysis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "794": {"title": "inverse cooking: recipe generation from food images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Salvador_Inverse_Cooking_Recipe_Generation_From_Food_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "158": {"title": "on learning density aware embeddings", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ghosh_On_Learning_Density_Aware_Embeddings_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "290": {"title": "end-to-end time-lapse video synthesis from a single outdoor image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nam_End-To-End_Time-Lapse_Video_Synthesis_From_a_Single_Outdoor_Image_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "79": {"title": "repr: improved training of convolutional filters", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Prakash_RePr_Improved_Training_of_Convolutional_Filters_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "846": {"title": "siclope: silhouette-based clothed people", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Natsume_SiCloPe_Silhouette-Based_Clothed_People_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "864": {"title": "binary ensemble neural network: more bits per network or more networks per bit?", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Binary_Ensemble_Neural_Network_More_Bits_per_Network_or_More_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "245": {"title": "large-scale, metric structure from motion for unordered light fields", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nousias_Large-Scale_Metric_Structure_From_Motion_for_Unordered_Light_Fields_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "575": {"title": "convolutional neural networks can be deceived by visual illusions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gomez-Villa_Convolutional_Neural_Networks_Can_Be_Deceived_by_Visual_Illusions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "512": {"title": "learning to detect human-object interactions with knowledge", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Learning_to_Detect_Human-Object_Interactions_With_Knowledge_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "913": {"title": "sparsefool: a few pixels make a big difference", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Modas_SparseFool_A_Few_Pixels_Make_a_Big_Difference_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1219": {"title": "model-blind video denoising via frame-to-frame training", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ehret_Model-Blind_Video_Denoising_via_Frame-To-Frame_Training_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "692": {"title": "learning-based sampling for natural image matting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_Learning-Based_Sampling_for_Natural_Image_Matting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "925": {"title": "neuro-inspired eye tracking with eye movement dynamics", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Neuro-Inspired_Eye_Tracking_With_Eye_Movement_Dynamics_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "527": {"title": "bridging stereo matching and optical flow via spatiotemporal correspondence", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lai_Bridging_Stereo_Matching_and_Optical_Flow_via_Spatiotemporal_Correspondence_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "656": {"title": "storygan: a sequential conditional gan for story visualization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_StoryGAN_A_Sequential_Conditional_GAN_for_Story_Visualization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1182": {"title": "laso: label-set operations networks for multi-label few-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Alfassy_LaSO_Label-Set_Operations_Networks_for_Multi-Label_Few-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "9": {"title": "esir: end-to-end scene text recognition via iterative image rectification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhan_ESIR_End-To-End_Scene_Text_Recognition_via_Iterative_Image_Rectification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1254": {"title": "adversarial inference for multi-sentence video description", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Adversarial_Inference_for_Multi-Sentence_Video_Description_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "697": {"title": "a content transformation block for image style transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kotovenko_A_Content_Transformation_Block_for_Image_Style_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1053": {"title": "pvnet: pixel-wise voting network for 6dof pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Peng_PVNet_Pixel-Wise_Voting_Network_for_6DoF_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "85": {"title": "learning joint gait representation via quintuplet loss minimization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Learning_Joint_Gait_Representation_via_Quintuplet_Loss_Minimization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "397": {"title": "coordinate-based texture inpainting for pose-guided human image generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Grigorev_Coordinate-Based_Texture_Inpainting_for_Pose-Guided_Human_Image_Generation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "537": {"title": "single image reflection removal exploiting misaligned training data and network enhancements", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wei_Single_Image_Reflection_Removal_Exploiting_Misaligned_Training_Data_and_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "690": {"title": "an alternative deep feature approach to line level keyword spotting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Retsinas_An_Alternative_Deep_Feature_Approach_to_Line_Level_Keyword_Spotting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1099": {"title": "actor-critic instance segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Araslanov_Actor-Critic_Instance_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1032": {"title": "privacy protection in street-view panoramas using depth and multi-view imagery", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Uittenbogaard_Privacy_Protection_in_Street-View_Panoramas_Using_Depth_and_Multi-View_Imagery_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "508": {"title": "object-centric auto-encoders and dummy anomalies for abnormal event detection in video", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ionescu_Object-Centric_Auto-Encoders_and_Dummy_Anomalies_for_Abnormal_Event_Detection_in_CVPR_2019_paper.html", "abstract": "Abnormal event detection in video is a challenging vision problem. Most existing approaches formulat\ne abnormal event detection as an outlier detection task, due to the scarcity of anomalous data durin\ng training. Because of the lack of prior information regarding abnormal events, these methods are no\nt fully-equipped to differentiate between normal and abnormal events. In this work, we formalize abn\normal event detection as a one-versus-rest binary classification problem. Our contribution is two-fo\nld. First, we introduce an unsupervised feature learning framework based on object-centric convoluti\nonal auto-encoders to encode both motion and appearance information. Second, we propose a supervised\n classification approach based on clustering the training samples into normality clusters. A one-ver\nsus-rest abnormal event classifier is then employed to separate each normality cluster from the rest\n. For the purpose of training the classifier, the other clusters act as dummy anomalies. During infe\nrence, an object is labeled as abnormal if the highest classification score assigned by the one-vers\nus-rest classifiers is negative. Comprehensive experiments are performed on four benchmarks: Avenue,\n ShanghaiTech, UCSD and UMN. Our approach provides superior results on all four data sets. On the la\nrge-scale ShanghaiTech data set, our method provides an absolute gain of 8.4% in terms of frame-leve\nl AUC compared to the state-of-the-art method.\r", "cite_num": 1}, "880": {"title": "doodle to search: practical zero-shot sketch-based image retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dey_Doodle_to_Search_Practical_Zero-Shot_Sketch-Based_Image_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "29": {"title": "espnetv2: a light-weight, power efficient, and general purpose convolutional neural network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mehta_ESPNetv2_A_Light-Weight_Power_Efficient_and_General_Purpose_Convolutional_Neural_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "23": {"title": "repair: removing representation bias by dataset resampling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_REPAIR_Removing_Representation_Bias_by_Dataset_Resampling_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "94": {"title": "seamless scene segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Porzi_Seamless_Scene_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1289": {"title": "dsfd: dual shot face detector", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_DSFD_Dual_Shot_Face_Detector_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "992": {"title": "circulant binary convolutional networks: enhancing the performance of 1-bit dcnns with circulant back propagation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Circulant_Binary_Convolutional_Networks_Enhancing_the_Performance_of_1-Bit_DCNNs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "91": {"title": "ficklenet: weakly and semi-supervised semantic image segmentation using stochastic inference", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_FickleNet_Weakly_and_Semi-Supervised_Semantic_Image_Segmentation_Using_Stochastic_Inference_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "775": {"title": "crossinfonet: multi-task information sharing based hand pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Du_CrossInfoNet_Multi-Task_Information_Sharing_Based_Hand_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "326": {"title": "mnasnet: platform-aware neural architecture search for mobile", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tan_MnasNet_Platform-Aware_Neural_Architecture_Search_for_Mobile_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "745": {"title": "transfer learning via unsupervised task discovery for visual question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Noh_Transfer_Learning_via_Unsupervised_Task_Discovery_for_Visual_Question_Answering_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "148": {"title": "progressive teacher-student learning for early action prediction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Progressive_Teacher-Student_Learning_for_Early_Action_Prediction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "33": {"title": "embedding complementary deep networks for image classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Embedding_Complementary_Deep_Networks_for_Image_Classification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "185": {"title": "high-level semantic feature detection: a new perspective for pedestrian detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "722": {"title": "topology reconstruction of tree-like structure in images via structural similarity measure and dominant set clustering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Topology_Reconstruction_of_Tree-Like_Structure_in_Images_via_Structural_Similarity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "321": {"title": "c2ae: class conditioned auto-encoder for open-set recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Oza_C2AE_Class_Conditioned_Auto-Encoder_for_Open-Set_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "746": {"title": "learning to minify photometric stereo", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_to_Minify_Photometric_Stereo_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1286": {"title": "pointflownet: learning representations for rigid motion estimation from point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Behl_PointFlowNet_Learning_Representations_for_Rigid_Motion_Estimation_From_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "577": {"title": "double nuclear norm based low rank representation on grassmann manifolds for clustering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Piao_Double_Nuclear_Norm_Based_Low_Rank_Representation_on_Grassmann_Manifolds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "239": {"title": "scene categorization from contours: medial axis based salience measures", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rezanejad_Scene_Categorization_From_Contours_Medial_Axis_Based_Salience_Measures_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1185": {"title": "semi-supervised transfer learning for image rain removal", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wei_Semi-Supervised_Transfer_Learning_for_Image_Rain_Removal_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1273": {"title": "polysemous visual-semantic embedding for cross-modal retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Song_Polysemous_Visual-Semantic_Embedding_for_Cross-Modal_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1174": {"title": "semi-supervised learning with graph learning-convolutional networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jiang_Semi-Supervised_Learning_With_Graph_Learning-Convolutional_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "715": {"title": "a bayesian perspective on the deep image prior", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cheng_A_Bayesian_Perspective_on_the_Deep_Image_Prior_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "593": {"title": "effective aesthetics prediction with multi-level spatially pooled features", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hosu_Effective_Aesthetics_Prediction_With_Multi-Level_Spatially_Pooled_Features_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "347": {"title": "looking for the devil in the details: learning trilinear attention sampling network for fine-grained image recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Looking_for_the_Devil_in_the_Details_Learning_Trilinear_Attention_CVPR_2019_paper.html", "abstract": "Learning subtle yet discriminative features (e.g., beak and eyes for a bird) plays a significant rol\ne in fine-grained image recognition. Existing attention-based approaches localize and amplify signif\nicant parts to learn fine-grained details, which often suffer from a limited number of parts and hea\nvy computational cost. In this paper, we propose to learn such fine-grained features from hundreds o\nf part proposals by Trilinear Attention Sampling Network (TASN) in an efficient teacher-student mann\ner. Specifically, TASN consists of 1) a trilinear attention module, which generates attention maps b\ny modeling the inter-channel relationships, 2) an attention-based sampler which highlights attended \nparts with high resolution, and 3) a feature distiller, which distills part features into an object-\nlevel feature by weight sharing and feature preserving strategies. Extensive experiments verify that\n TASN yields the best performance under the same settings with the most competitive approaches, in i\nNaturalist-2017, CUB-Bird, and Stanford-Cars datasets.\r", "cite_num": 0}, "959": {"title": "patch-based discriminative feature learning for unsupervised person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Patch-Based_Discriminative_Feature_Learning_for_Unsupervised_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "32": {"title": "coloring with limited data: few-shot colorization via memory augmented networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yoo_Coloring_With_Limited_Data_Few-Shot_Colorization_via_Memory_Augmented_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1222": {"title": "a structured model for action detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_A_Structured_Model_for_Action_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "279": {"title": "adaframe: adaptive frame selection for fast video recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_AdaFrame_Adaptive_Frame_Selection_for_Fast_Video_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "234": {"title": "complete the look: scene-based complementary product recommendation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kang_Complete_the_Look_Scene-Based_Complementary_Product_Recommendation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "390": {"title": "contextdesc: local descriptor augmentation with cross-modality context", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Luo_ContextDesc_Local_Descriptor_Augmentation_With_Cross-Modality_Context_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "973": {"title": "lending orientation to neural networks for cross-view geo-localization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Lending_Orientation_to_Neural_Networks_for_Cross-View_Geo-Localization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1005": {"title": "traphic: trajectory prediction in dense and heterogeneous traffic using weighted interactions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chandra_TraPHic_Trajectory_Prediction_in_Dense_and_Heterogeneous_Traffic_Using_Weighted_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "640": {"title": "in the wild human pose estimation using explicit 2d features and intermediate 3d representations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Habibie_In_the_Wild_Human_Pose_Estimation_Using_Explicit_2D_Features_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "458": {"title": "rgbd based dimensional decomposition residual network for 3d semantic scene completion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_RGBD_Based_Dimensional_Decomposition_Residual_Network_for_3D_Semantic_Scene_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "325": {"title": "facial emotion distribution learning by exploiting low-rank label correlations locally", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jia_Facial_Emotion_Distribution_Learning_by_Exploiting_Low-Rank_Label_Correlations_Locally_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1198": {"title": "blind image deblurring with local maximum gradient prior", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Blind_Image_Deblurring_With_Local_Maximum_Gradient_Prior_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "682": {"title": "expressive body capture: 3d hands, face, and body from a single image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pavlakos_Expressive_Body_Capture_3D_Hands_Face_and_Body_From_a_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "138": {"title": "cityflow: a city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_CityFlow_A_City-Scale_Benchmark_for_Multi-Target_Multi-Camera_Vehicle_Tracking_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "994": {"title": "residual regression with semantic prior for crowd counting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wan_Residual_Regression_With_Semantic_Prior_for_Crowd_Counting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "51": {"title": "cascaded partial decoder for fast and accurate salient object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Cascaded_Partial_Decoder_for_Fast_and_Accurate_Salient_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "918": {"title": "learning to adapt for stereo", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tonioni_Learning_to_Adapt_for_Stereo_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "42": {"title": "progressive attention memory network for movie story question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Progressive_Attention_Memory_Network_for_Movie_Story_Question_Answering_CVPR_2019_paper.html", "abstract": "This paper proposes the progressive attention memory network (PAMN) for movie story question answeri\nng (QA). Movie story QA is challenging compared to VQA in two aspects: (1) pinpointing the temporal \nparts relevant to answer the question is difficult as the movies are typically longer than an hour, \n(2) it has both video and subtitle where different questions require different modality to infer the\n answer. To overcome these challenges, PAMN involves three main features: (1) progressive attention \nmechanism that utilizes cues from both question and answer to progressively prune out irrelevant tem\nporal parts in memory, (2) dynamic modality fusion that adaptively determines the contribution of ea\nch modality for answering the current question, and (3) belief correction answering scheme that succ\nessively corrects the prediction score on each candidate answer. Experiments on publicly available b\nenchmark datasets, MovieQA and TVQA, demonstrate that each feature contributes to our movie story QA\n architecture, PAMN, and improves performance to achieve the state-of-the-art result. Qualitative an\nalysis by visualizing the inference mechanism of PAMN is also provided.", "cite_num": 1}, "1105": {"title": "weakly supervised deep image hashing through tag embeddings", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gattupalli_Weakly_Supervised_Deep_Image_Hashing_Through_Tag_Embeddings_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "627": {"title": "latent space autoregression for novelty detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Abati_Latent_Space_Autoregression_for_Novelty_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "833": {"title": "meta-learning convolutional neural architectures for multi-target concrete defect classification with the concrete defect bridge image dataset", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mundt_Meta-Learning_Convolutional_Neural_Architectures_for_Multi-Target_Concrete_Defect_Classification_With_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "998": {"title": "semantic correlation promoted shape-variant context for segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Semantic_Correlation_Promoted_Shape-Variant_Context_for_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "134": {"title": "distraction-aware shadow detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Distraction-Aware_Shadow_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "733": {"title": "robust video stabilization by optimization in cnn weight space", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Robust_Video_Stabilization_by_Optimization_in_CNN_Weight_Space_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1202": {"title": "transferable automl by model sharing over grouped datasets", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xue_Transferable_AutoML_by_Model_Sharing_Over_Grouped_Datasets_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "296": {"title": "3d point capsule networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_3D_Point_Capsule_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "168": {"title": "self-supervised adaptation of high-fidelity face models for monocular performance tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yoon_Self-Supervised_Adaptation_of_High-Fidelity_Face_Models_for_Monocular_Performance_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1133": {"title": "sliced wasserstein generative models", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1231": {"title": "photon-flooded single-photon 3d cameras", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gupta_Photon-Flooded_Single-Photon_3D_Cameras_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "559": {"title": "robust histopathology image analysis: to label or to synthesize?", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hou_Robust_Histopathology_Image_Analysis_To_Label_or_to_Synthesize_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "25": {"title": "comdefend: an efficient image compression model to defend adversarial examples", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jia_ComDefend_An_Efficient_Image_Compression_Model_to_Defend_Adversarial_Examples_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "377": {"title": "learning a unified classifier incrementally via rebalancing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hou_Learning_a_Unified_Classifier_Incrementally_via_Rebalancing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "570": {"title": "atom: accurate tracking by overlap maximization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Danelljan_ATOM_Accurate_Tracking_by_Overlap_Maximization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "338": {"title": "learning the depths of moving people by watching frozen people", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_People_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "613": {"title": "salient object detection with pyramid attention and salient edges", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Salient_Object_Detection_With_Pyramid_Attention_and_Salient_Edges_CVPR_2019_paper.html", "abstract": "This paper presents a new method for detecting salient objects in images using convolutional neural \nnetworks (CNNs). The proposed network, named PAGE-Net, offers two key contributions. The first is th\ne exploitation of an essential pyramid attention structure for salient object detection. This enable\ns the network to concentrate more on salient regions while considering multi-scale saliency informat\nion. Such a stacked attention design provides a powerful tool to efficiently improve the representat\nion ability of the corresponding network layer with an enlarged receptive field. The second contribu\ntion lies in the emphasis on the importance of salient edges. Salient edge information offers a stro\nng cue to better segment salient objects and refine object boundaries. To this end, our model is equ\nipped with a salient edge detection module, which is learned for precise salient boundary estimation\n. This encourages better edge-preserving salient object segmentation. Exhaustive experiments confirm\n that the proposed pyramid attention and salient edges are effective for salient object detection. W\ne show that our deep saliency model outperforms state-of-the-art approaches for several benchmarks w\nith a fast processing speed (25fps on one GPU).\r", "cite_num": 2}, "691": {"title": "unequal-training for deep face recognition with long-tailed noisy data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhong_Unequal-Training_for_Deep_Face_Recognition_With_Long-Tailed_Noisy_Data_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1217": {"title": "listen to the image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Listen_to_the_Image_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "172": {"title": "sim-to-real via sim-to-sim: data-efficient robotic grasping via randomized-to-canonical adaptation networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "228": {"title": "unprocessing images for learned raw denoising", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Brooks_Unprocessing_Images_for_Learned_Raw_Denoising_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "10": {"title": "what does it mean to learn in deep networks? and, how does one detect adversarial attacks?", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Corneanu_What_Does_It_Mean_to_Learn_in_Deep_Networks_And_CVPR_2019_paper.html", "abstract": "The flexibility and high-accuracy of Deep Neural Networks (DNNs) has transformed computer vision. Bu\nt, the fact that we do not know when a specific DNN will work and when it will fail has resulted in \na lack of trust. A clear example is self-driving cars; people are uncomfortable sitting in a car dri\nven by algorithms that may fail under some unknown, unpredictable conditions. Interpretability and e\nxplainability approaches attempt to address this by uncovering what a DNN models, i.e., what each no\nde (cell) in the network represents and what images are most likely to activate it. This can be used\n to generate, for example, adversarial attacks. But these approaches do not generally allow us to de\ntermine where a DNN will succeed or fail and   why . i.e., does this learned representation   genera\nlize  to unseen samples? Here, we derive a novel approach to define what it means to learn in deep n\networks, and how to use this knowledge to detect adversarial attacks. We show how this defines the a\nbility of a network to generalize to unseen testing samples and, most importantly,   why  this is th\ne case.\r", "cite_num": 0}, "1162": {"title": "im-net for high resolution video frame interpolation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Peleg_IM-Net_for_High_Resolution_Video_Frame_Interpolation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1197": {"title": "feelvos: fast end-to-end embedding learning for video object segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Voigtlaender_FEELVOS_Fast_End-To-End_Embedding_Learning_for_Video_Object_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "6": {"title": "learning spatial common sense with geometry-aware recurrent networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tung_Learning_Spatial_Common_Sense_With_Geometry-Aware_Recurrent_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "831": {"title": "ae2-nets: autoencoder in autoencoder networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_AE2-Nets_Autoencoder_in_Autoencoder_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "61": {"title": "decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Rony_Decoupling_Direction_and_Norm_for_Efficient_Gradient-Based_L2_Adversarial_Attacks_CVPR_2019_paper.html", "abstract": "Research on adversarial examples in computer vision tasks has shown that small, often imperceptible \nchanges to an image can induce misclassification, which has security implications for a wide range o\nf image processing systems. Considering $L_2$ norm distortions, the Carlini and Wagner attack is pre\nsently the most effective white-box attack in the literature. However, this method is slow since it \nperforms a line-search for one of the optimization terms, and often requires thousands of iterations\n. In this paper, an efficient approach is proposed to generate gradient-based attacks that induce mi\nsclassifications with low $L_2$ norm, by decoupling the direction and the norm of the adversarial pe\nrturbation that is added to the image. Experiments conducted on the MNIST, CIFAR-10 and ImageNet dat\nasets indicate that our attack achieves comparable results to the state-of-the-art (in terms of $L_2\n$ norm) with considerably fewer iterations (as few as 100 iterations), which opens the possibility o\nf using these attacks for adversarial training. Models trained with our attack achieve state-of-the-\nart robustness against white-box gradient-based $L_2$ attacks on the MNIST and CIFAR-10 datasets, ou\ntperforming the Madry defense when the attacks are limited to a maximum norm.", "cite_num": 6}, "797": {"title": "scan2cad: learning cad model alignment in rgb-d scans", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Avetisyan_Scan2CAD_Learning_CAD_Model_Alignment_in_RGB-D_Scans_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "22": {"title": "auto-deeplab: hierarchical neural architecture search for semantic image segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Auto-DeepLab_Hierarchical_Neural_Architecture_Search_for_Semantic_Image_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "872": {"title": "deeplidar: deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qiu_DeepLiDAR_Deep_Surface_Normal_Guided_Depth_Prediction_for_Outdoor_Scene_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "478": {"title": "mscap: multi-style image captioning with unpaired stylized text", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_MSCap_Multi-Style_Image_Captioning_With_Unpaired_Stylized_Text_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1065": {"title": "unsupervised primitive discovery for improved 3d generative modeling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Khan_Unsupervised_Primitive_Discovery_for_Improved_3D_Generative_Modeling_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "295": {"title": "learning to explain with complemental examples", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kanehira_Learning_to_Explain_With_Complemental_Examples_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "381": {"title": "grid r-cnn", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lu_Grid_R-CNN_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "92": {"title": "fast user-guided video object segmentation by interaction-and-propagation networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Oh_Fast_User-Guided_Video_Object_Segmentation_by_Interaction-And-Propagation_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "473": {"title": "deep exemplar-based video colorization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deep_Exemplar-Based_Video_Colorization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "179": {"title": "gif2video: color dequantization and temporal interpolation of gif images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_GIF2Video_Color_Dequantization_and_Temporal_Interpolation_of_GIF_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "968": {"title": "davanet: stereo deblurring with view aggregation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_DAVANet_Stereo_Deblurring_With_View_Aggregation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "614": {"title": "dynamics are important for the recognition of equine pain in video", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Broome_Dynamics_Are_Important_for_the_Recognition_of_Equine_Pain_in_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "167": {"title": "unsupervised image matching and object discovery as optimization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Vo_Unsupervised_Image_Matching_and_Object_Discovery_as_Optimization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1066": {"title": "pluralistic image completion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Pluralistic_Image_Completion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1203": {"title": "graphical contrastive losses for scene graph parsing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Graphical_Contrastive_Losses_for_Scene_Graph_Parsing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "80": {"title": "liff: light field features in scale and depth", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dansereau_LiFF_Light_Field_Features_in_Scale_and_Depth_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "44": {"title": "arbitrary shape scene text detection with adaptive text region representation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Arbitrary_Shape_Scene_Text_Detection_With_Adaptive_Text_Region_Representation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "173": {"title": "disentangling latent hands for image synthesis and pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Disentangling_Latent_Hands_for_Image_Synthesis_and_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "15": {"title": "camera lens super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Camera_Lens_Super-Resolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "170": {"title": "hyperspectral image reconstruction using a deep spatial-spectral prior", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Hyperspectral_Image_Reconstruction_Using_a_Deep_Spatial-Spectral_Prior_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1166": {"title": "convolutional recurrent network for road boundary extraction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liang_Convolutional_Recurrent_Network_for_Road_Boundary_Extraction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1195": {"title": "sequence-to-sequence domain adaptation network for robust text image recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Sequence-To-Sequence_Domain_Adaptation_Network_for_Robust_Text_Image_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "813": {"title": "low-rank laplacian-uniform mixed model for robust face recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Low-Rank_Laplacian-Uniform_Mixed_Model_for_Robust_Face_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1141": {"title": "unsupervised person image generation with semantic parsing transformation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Song_Unsupervised_Person_Image_Generation_With_Semantic_Parsing_Transformation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "382": {"title": "non-adversarial image synthesis with generative latent nearest neighbors", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hoshen_Non-Adversarial_Image_Synthesis_With_Generative_Latent_Nearest_Neighbors_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "725": {"title": "label-noise robust generative adversarial networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kaneko_Label-Noise_Robust_Generative_Adversarial_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "118": {"title": "learning to quantize deep networks by optimizing quantization intervals with task loss", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jung_Learning_to_Quantize_Deep_Networks_by_Optimizing_Quantization_Intervals_With_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "272": {"title": "coordinate-free carlsson-weinshall duality and relative multi-view geometry", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Trager_Coordinate-Free_Carlsson-Weinshall_Duality_and_Relative_Multi-View_Geometry_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "348": {"title": "deep sky modeling for single image outdoor lighting estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hold-Geoffroy_Deep_Sky_Modeling_for_Single_Image_Outdoor_Lighting_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "30": {"title": "simultaneously optimizing weight and quantizer of ternary neural network using truncated gaussian approximation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Simultaneously_Optimizing_Weight_and_Quantizer_of_Ternary_Neural_Network_Using_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "54": {"title": "crowd counting and density estimation by trellis encoder-decoder networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jiang_Crowd_Counting_and_Density_Estimation_by_Trellis_Encoder-Decoder_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "868": {"title": "map inference via block-coordinate frank-wolfe algorithm", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Swoboda_MAP_Inference_via_Block-Coordinate_Frank-Wolfe_Algorithm_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "989": {"title": "elastic: improving cnns with dynamic scaling policies", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_ELASTIC_Improving_CNNs_With_Dynamic_Scaling_Policies_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "930": {"title": "separate to adapt: open set domain adaptation via progressive separation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Separate_to_Adapt_Open_Set_Domain_Adaptation_via_Progressive_Separation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1017": {"title": "dense 3d face decoding over 2500fps: joint texture & shape convolutional mesh decoders", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_Dense_3D_Face_Decoding_Over_2500FPS_Joint_Texture__Shape_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "542": {"title": "stgan: a unified selective transfer network for arbitrary image attribute editing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_STGAN_A_Unified_Selective_Transfer_Network_for_Arbitrary_Image_Attribute_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "27": {"title": "gs3d: an efficient 3d object detection framework for autonomous driving", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_GS3D_An_Efficient_3D_Object_Detection_Framework_for_Autonomous_Driving_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1126": {"title": "customizable architecture search for semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Customizable_Architecture_Search_for_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1168": {"title": "end-to-end projector photometric compensation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_End-To-End_Projector_Photometric_Compensation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "151": {"title": "unsupervised disentangling of appearance and geometry by deformable generator network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xing_Unsupervised_Disentangling_of_Appearance_and_Geometry_by_Deformable_Generator_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "743": {"title": "learning binary code for personalized fashion recommendation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lu_Learning_Binary_Code_for_Personalized_Fashion_Recommendation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1181": {"title": "3d hand shape and pose from images in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Boukhayma_3D_Hand_Shape_and_Pose_From_Images_in_the_Wild_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1077": {"title": "tightness-aware evaluation protocol for scene text detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Tightness-Aware_Evaluation_Protocol_for_Scene_Text_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1104": {"title": "a neurobiological evaluation metric for neural network model search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Blanchard_A_Neurobiological_Evaluation_Metric_for_Neural_Network_Model_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "997": {"title": "visual tracking via adaptive spatially-regularized correlation filters", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dai_Visual_Tracking_via_Adaptive_Spatially-Regularized_Correlation_Filters_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1149": {"title": "compressing unknown images with product quantizer for efficient zero-shot classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Compressing_Unknown_Images_With_Product_Quantizer_for_Efficient_Zero-Shot_Classification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "582": {"title": "vitamin-e: visual tracking and mapping with extremely dense feature points", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yokozuka_VITAMIN-E_VIsual_Tracking_and_MappINg_With_Extremely_Dense_Feature_Points_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "978": {"title": "image deformation meta-networks for one-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Image_Deformation_Meta-Networks_for_One-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "371": {"title": "co-occurrence neural network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shevlev_Co-Occurrence_Neural_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "684": {"title": "convolutional mesh regression for single-image human shape reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kolotouros_Convolutional_Mesh_Regression_for_Single-Image_Human_Shape_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "16": {"title": "finegan: unsupervised hierarchical disentanglement for fine-grained object generation and discovery", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Singh_FineGAN_Unsupervised_Hierarchical_Disentanglement_for_Fine-Grained_Object_Generation_and_Discovery_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "115": {"title": "beyond gradient descent for regularized segmentation losses", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Marin_Beyond_Gradient_Descent_for_Regularized_Segmentation_Losses_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "885": {"title": "gframes: gradient-based local reference frame for 3d shape matching", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Melzi_GFrames_Gradient-Based_Local_Reference_Frame_for_3D_Shape_Matching_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "520": {"title": "large scale high-resolution land cover mapping with multi-resolution data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Robinson_Large_Scale_High-Resolution_Land_Cover_Mapping_With_Multi-Resolution_Data_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "696": {"title": "local to global learning: gradually adding classes for training deep neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cheng_Local_to_Global_Learning_Gradually_Adding_Classes_for_Training_Deep_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "187": {"title": "learning personalized modular network guided by structured knowledge", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liang_Learning_Personalized_Modular_Network_Guided_by_Structured_Knowledge_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1238": {"title": "meta-transfer learning for few-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "641": {"title": "learning implicit fields for generative shape modeling", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Learning_Implicit_Fields_for_Generative_Shape_Modeling_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "492": {"title": "sketchgan: joint sketch completion and recognition with generative adversarial network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_SketchGAN_Joint_Sketch_Completion_and_Recognition_With_Generative_Adversarial_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "554": {"title": "evading defenses to transferable adversarial examples by translation-invariant attacks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Evading_Defenses_to_Transferable_Adversarial_Examples_by_Translation-Invariant_Attacks_CVPR_2019_paper.html", "abstract": "Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding\n imperceptible perturbations. An intriguing property of adversarial examples is their good transfera\nbility, making black-box attacks feasible in real-world applications. Due to the threat of adversari\nal attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defe\nnses are shown to be robust against transferable adversarial examples. In this paper, we propose a t\nranslation-invariant attack method to generate more transferable adversarial examples against the de\nfense models. By optimizing a perturbation over an ensemble of translated images, the generated adve\nrsarial example is less sensitive to the white-box model being attacked and has better transferabili\nty. To improve the efficiency of attacks, we further show that our method can be implemented by conv\nolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally app\nlicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate \nthe effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at a\nn 82% success rate on average based only on the transferability, demonstrating the insecurity of the\n current defense techniques.\r", "cite_num": 0}, "1044": {"title": "pose2seg: detection free human instance segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Pose2Seg_Detection_Free_Human_Instance_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "490": {"title": "a cross-season correspondence dataset for robust semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Larsson_A_Cross-Season_Correspondence_Dataset_for_Robust_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "564": {"title": "phase-only image based kernel estimation for single image blind deblurring", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pan_Phase-Only_Image_Based_Kernel_Estimation_for_Single_Image_Blind_Deblurring_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1282": {"title": "inverse path tracing for joint material and lighting estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Azinovic_Inverse_Path_Tracing_for_Joint_Material_and_Lighting_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "69": {"title": "gqa: a new dataset for real-world visual reasoning and compositional question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "855": {"title": "learning image and video compression through spatial-temporal energy compaction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cheng_Learning_Image_and_Video_Compression_Through_Spatial-Temporal_Energy_Compaction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1281": {"title": "deep transfer learning for multiple class novelty detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Perera_Deep_Transfer_Learning_for_Multiple_Class_Novelty_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "434": {"title": "rethinking knowledge graph propagation for zero-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kampffmeyer_Rethinking_Knowledge_Graph_Propagation_for_Zero-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "632": {"title": "sliced wasserstein discrepancy for unsupervised domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_Sliced_Wasserstein_Discrepancy_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "75": {"title": "object-driven text-to-image synthesis via adversarial training", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Object-Driven_Text-To-Image_Synthesis_via_Adversarial_Training_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "897": {"title": "laeo-net: revisiting people looking at each other in videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Marin-Jimenez_LAEO-Net_Revisiting_People_Looking_at_Each_Other_in_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "740": {"title": "semantic projection network for zero- and few-label semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xian_Semantic_Projection_Network_for_Zero-_and_Few-Label_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1154": {"title": "adapting object detectors via selective cross-domain alignment", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Adapting_Object_Detectors_via_Selective_Cross-Domain_Alignment_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "135": {"title": "divergence prior and vessel-tree reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Divergence_Prior_and_Vessel-Tree_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1230": {"title": "a neural network based on spd manifold learning for skeleton-based hand gesture recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nguyen_A_Neural_Network_Based_on_SPD_Manifold_Learning_for_Skeleton-Based_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1084": {"title": "geonet: deep geodesic networks for point cloud analysis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_GeoNet_Deep_Geodesic_Networks_for_Point_Cloud_Analysis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "784": {"title": "unsupervised domain adaptation using feature-whitening and consensus loss", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Roy_Unsupervised_Domain_Adaptation_Using_Feature-Whitening_and_Consensus_Loss_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "26": {"title": "recursive visual attention in visual dialog", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Niu_Recursive_Visual_Attention_in_Visual_Dialog_CVPR_2019_paper.html", "abstract": "Visual dialog is a challenging vision-language task, which requires the agent to answer multi-round \nquestions about an image. It typically needs to address two major problems: (1) How to answer visual\nly-grounded questions, which is the core challenge in visual question answering (VQA); (2) How to in\nfer the co-reference between questions and the dialog history. An example of visual co-reference is:\n pronouns (e.g., `they') in the question (e.g., `Are they on or off?') are linked with nouns (e.g., \n`lamps') appearing in the dialog history (e.g., `How many lamps are there?') and the object grounded\n in the image. In this work, to resolve the visual co-reference for visual dialog, we propose a nove\nl attention mechanism called Recursive Visual Attention (RvA). Specifically, our dialog agent browse\ns the dialog history until the agent has sufficient confidence in the visual co-reference resolution\n, and refines the visual attention recursively. The quantitative and qualitative experimental result\ns on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the proposed RvA not only outpe\nrforms the state-of-the-art methods, but also achieves reasonable recursion and interpretable attent\nion maps without additional annotations.", "cite_num": 4}, "574": {"title": "hetconv: heterogeneous kernel-based convolutions for deep cnns", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Singh_HetConv_Heterogeneous_Kernel-Based_Convolutions_for_Deep_CNNs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "62": {"title": "visual query answering by entity-attribute graph matching and reasoning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xiong_Visual_Query_Answering_by_Entity-Attribute_Graph_Matching_and_Reasoning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1221": {"title": "iterative projection and matching: finding structure-preserving representatives and its application to computer vision", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zaeemzadeh_Iterative_Projection_and_Matching_Finding_Structure-Preserving_Representatives_and_Its_Application_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "744": {"title": "arbitrary style transfer with style-attentional networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Arbitrary_Style_Transfer_With_Style-Attentional_Networks_CVPR_2019_paper.html", "abstract": "Arbitrary style transfer aims to synthesize a content image with the style of an image to create a t\nhird image that has never been seen before. Recent arbitrary style transfer algorithms find it chall\nenging to balance the content structure and the style patterns. Moreover, simultaneously maintaining\n the global and local style patterns is difficult due to the patch-based mechanism. In this paper, w\ne introduce a novel style-attentional network (SANet) that efficiently and flexibly integrates the l\nocal style patterns according to the semantic spatial distribution of the content image. A new ident\nity loss function and multi-level feature embeddings enable our SANet and decoder to preserve the co\nntent structure as much as possible while enriching the style patterns. Experimental results demonst\nrate that our algorithm synthesizes stylized images in real-time that are higher in quality than tho\nse produced by the state-of-the-art algorithms.", "cite_num": 1}, "1275": {"title": "causes and corrections for bimodal multi-path scanning with structured light", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Causes_and_Corrections_for_Bimodal_Multi-Path_Scanning_With_Structured_Light_CVPR_2019_paper.html", "abstract": "Structured light illumination is an active 3D scanning technique based on projecting/capturing a set\n of striped patterns and measuring the warping of the patterns as they reflect off a target object's\n surface. As designed, each pixel in the camera sees exactly one pixel from the projector; however, \nthere are multi-path situations when the scanned surface has a complicated geometry with step edges \nand other discontinuities in depth or where the target surface has specularities that reflect light \naway from the camera. These situations are generally referred to multi-path where a camera pixel see\ns light from multiple projector positions.  In the case of bimodal multi-path, the camera pixel rece\nives light from exactly two positions which occurs along a step edge where the edge slices through a\n pixel so that the pixel sees both a foreground and background surface. In this paper, we present a \ngeneral mathematical model to address the bimodal multi-path issue in a phase-measuring-profilometry\n scanner to measure the constructive and destructive interference between the two light paths, and b\ny taking advantage of this interesting cue, separate the paths and make two decoupled phase measurem\nents. We validate our algorithm with  a number of challenging real-world scenarios, outperforming th\ne state-of-the-art method.\r", "cite_num": 0}, "525": {"title": "unsupervised face normalization with extreme pose and expression in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qian_Unsupervised_Face_Normalization_With_Extreme_Pose_and_Expression_in_the_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1253": {"title": "predicting visible image differences under varying display brightness and viewing distance", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ye_Predicting_Visible_Image_Differences_Under_Varying_Display_Brightness_and_Viewing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "213": {"title": "learning without memorizing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dhar_Learning_Without_Memorizing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "602": {"title": "dense classification and implanting for few-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lifchitz_Dense_Classification_and_Implanting_for_Few-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "435": {"title": "light field messaging with deep photographic steganography", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wengrowski_Light_Field_Messaging_With_Deep_Photographic_Steganography_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1152": {"title": "text2scene: generating compositional scenes from textual descriptions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tan_Text2Scene_Generating_Compositional_Scenes_From_Textual_Descriptions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "265": {"title": "neural rejuvenation: improving deep network training by enhancing computational resource utilization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qiao_Neural_Rejuvenation_Improving_Deep_Network_Training_by_Enhancing_Computational_Resource_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "866": {"title": "grounded video description", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_Grounded_Video_Description_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1180": {"title": "what object should i use? - task driven object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sawatzky_What_Object_Should_I_Use_-_Task_Driven_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "720": {"title": "ke-gan: knowledge embedded generative adversarial networks for semi-supervised scene parsing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qi_KE-GAN_Knowledge_Embedded_Generative_Adversarial_Networks_for_Semi-Supervised_Scene_Parsing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1169": {"title": "high-quality face capture using anatomical muscles", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bao_High-Quality_Face_Capture_Using_Anatomical_Muscles_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1051": {"title": "universal domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/You_Universal_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "247": {"title": "revisiting self-supervised visual representation learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "180": {"title": "reflection removal using a dual-pixel sensor", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Punnappurath_Reflection_Removal_Using_a_Dual-Pixel_Sensor_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "183": {"title": "explainable and explicit visual reasoning over scene graphs", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Explainable_and_Explicit_Visual_Reasoning_Over_Scene_Graphs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "936": {"title": "eventnet: asynchronous recursive event processing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sekikawa_EventNet_Asynchronous_Recursive_Event_Processing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "343": {"title": "min-max statistical alignment for transfer learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Herath_Min-Max_Statistical_Alignment_for_Transfer_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1229": {"title": "text guided person image synthesis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_Text_Guided_Person_Image_Synthesis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "801": {"title": "not all areas are equal: transfer learning for semantic segmentation via hierarchical region selection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Not_All_Areas_Are_Equal_Transfer_Learning_for_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "489": {"title": "raven: a dataset for relational and analogical visual reasoning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_RAVEN_A_Dataset_for_Relational_and_Analogical_Visual_REasoNing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "759": {"title": "a perceptual prediction framework for self supervised event segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Aakur_A_Perceptual_Prediction_Framework_for_Self_Supervised_Event_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1124": {"title": "led3d: a lightweight and efficient deep approach to recognizing low-quality 3d faces", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mu_Led3D_A_Lightweight_and_Efficient_Deep_Approach_to_Recognizing_Low-Quality_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "642": {"title": "all-weather deep outdoor lighting estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_All-Weather_Deep_Outdoor_Lighting_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "674": {"title": "neural rgb(r)d sensing: depth and uncertainty from a video camera", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Neural_RGBrD_Sensing_Depth_and_Uncertainty_From_a_Video_Camera_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "960": {"title": "automatic face aging in videos via deep reinforcement learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Duong_Automatic_Face_Aging_in_Videos_via_Deep_Reinforcement_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1164": {"title": "pointing novel objects in image captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Pointing_Novel_Objects_in_Image_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "214": {"title": "siamrpn++: evolution of siamese visual tracking with very deep networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_SiamRPN_Evolution_of_Siamese_Visual_Tracking_With_Very_Deep_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "37": {"title": "reflective and fluorescent separation under narrow-band illumination", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Koyamatsu_Reflective_and_Fluorescent_Separation_Under_Narrow-Band_Illumination_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "184": {"title": "fully quantized network for object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Fully_Quantized_Network_for_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "384": {"title": "variational information distillation for knowledge transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ahn_Variational_Information_Distillation_for_Knowledge_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1265": {"title": "defusionnet: defocus blur detection via recurrently fusing and refining multi-scale deep features", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_DeFusionNET_Defocus_Blur_Detection_via_Recurrently_Fusing_and_Refining_Multi-Scale_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "222": {"title": "fast human pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "765": {"title": "multi-person pose estimation with enhanced channel-wise and spatial information", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Su_Multi-Person_Pose_Estimation_With_Enhanced_Channel-Wise_and_Spatial_Information_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1279": {"title": "improving transferability of adversarial examples with input diversity", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Improving_Transferability_of_Adversarial_Examples_With_Input_Diversity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "563": {"title": "learning with batch-wise optimal transport loss for 3d shape recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Learning_With_Batch-Wise_Optimal_Transport_Loss_for_3D_Shape_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "782": {"title": "bottom-up object detection by grouping extreme and center points", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_Bottom-Up_Object_Detection_by_Grouping_Extreme_and_Center_Points_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1127": {"title": "s4net: single stage salient-instance segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_S4Net_Single_Stage_Salient-Instance_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1094": {"title": "hyperspectral image super-resolution with optimized rgb guidance", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Hyperspectral_Image_Super-Resolution_With_Optimized_RGB_Guidance_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1069": {"title": "deep network interpolation for continuous imagery effect transition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Deep_Network_Interpolation_for_Continuous_Imagery_Effect_Transition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1211": {"title": "look back and predict forward in image captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qin_Look_Back_and_Predict_Forward_in_Image_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "981": {"title": "texturenet: consistent local parametrizations for learning from high-resolution signals on meshes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_TextureNet_Consistent_Local_Parametrizations_for_Learning_From_High-Resolution_Signals_on_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1003": {"title": "spot and learn: a maximum-entropy patch sampler for few-shot image classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chu_Spot_and_Learn_A_Maximum-Entropy_Patch_Sampler_for_Few-Shot_Image_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1179": {"title": "contrastive adaptation network for unsupervised domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kang_Contrastive_Adaptation_Network_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "464": {"title": "renas: reinforced evolutionary neural architecture search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_RENAS_Reinforced_Evolutionary_Neural_Architecture_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "161": {"title": "adaptiveface: adaptive margin and sampling for face recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_AdaptiveFace_Adaptive_Margin_and_Sampling_for_Face_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "354": {"title": "learning to separate multiple illuminants in a single image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hui_Learning_to_Separate_Multiple_Illuminants_in_a_Single_Image_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "49": {"title": "reversible gans for memory-efficient image-to-image translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/van_der_Ouderaa_Reversible_GANs_for_Memory-Efficient_Image-To-Image_Translation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "428": {"title": "weakly-supervised discovery of geometry-aware representation for 3d human pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Weakly-Supervised_Discovery_of_Geometry-Aware_Representation_for_3D_Human_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "65": {"title": "adversarial defense by stratified convolutional sparse coding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Adversarial_Defense_by_Stratified_Convolutional_Sparse_Coding_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "52": {"title": "unsupervised learning of consensus maximization for 3d vision problems", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Probst_Unsupervised_Learning_of_Consensus_Maximization_for_3D_Vision_Problems_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "553": {"title": "qatm: quality-aware template matching for deep learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cheng_QATM_Quality-Aware_Template_Matching_for_Deep_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1206": {"title": "neural sequential phrase grounding (seqground)", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dogan_Neural_Sequential_Phrase_Grounding_SeqGROUND_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "266": {"title": "point in, box out: beyond counting persons in crowds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Point_in_Box_Out_Beyond_Counting_Persons_in_Crowds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "291": {"title": "spatio-temporal video re-localization by warp lstm", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Feng_Spatio-Temporal_Video_Re-Localization_by_Warp_LSTM_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1145": {"title": "d2-net: a trainable cnn for joint description and detection of local features", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dusmanu_D2-Net_A_Trainable_CNN_for_Joint_Description_and_Detection_of_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "902": {"title": "on implicit filter level sparsity in convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mehta_On_Implicit_Filter_Level_Sparsity_in_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "891": {"title": "learning active contour models for medical image segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Learning_Active_Contour_Models_for_Medical_Image_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "468": {"title": "less is more: learning highlight detection from video duration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xiong_Less_Is_More_Learning_Highlight_Detection_From_Video_Duration_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "958": {"title": "what correspondences reveal about unknown camera and motion models?", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Probst_What_Correspondences_Reveal_About_Unknown_Camera_and_Motion_Models_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "522": {"title": "deep asymmetric metric learning via rich relationship mining", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Deep_Asymmetric_Metric_Learning_via_Rich_Relationship_Mining_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "767": {"title": "deep blind video decaptioning by temporal aggregation and recurrence", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Deep_Blind_Video_Decaptioning_by_Temporal_Aggregation_and_Recurrence_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1027": {"title": "deep multimodal clustering for unsupervised audiovisual learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Deep_Multimodal_Clustering_for_Unsupervised_Audiovisual_Learning_CVPR_2019_paper.html", "abstract": "The seen birds twitter, the running cars accompany with noise, etc. These naturally audiovisual corr\nespondences provide the possibilities to explore and understand the outside world. However, the mixe\nd multiple objects and sounds make it intractable to perform efficient matching in the unconstrained\n environment. To settle this problem, we propose to adequately excavate audio and visual components \nand perform elaborate correspondence learning among them. Concretely, a novel unsupervised audiovisu\nal learning model is proposed, named as \\Deep Multimodal Clustering (DMC), that synchronously perfor\nms sets of clustering with multimodal vectors of convolutional maps in different shared spaces for c\napturing multiple audiovisual correspondences. And such integrated multimodal clustering network can\n be effectively trained with max-margin loss in the end-to-end fashion. Amounts of experiments in fe\nature evaluation and audiovisual tasks are performed. The results demonstrate that DMC can learn eff\nective unimodal representation, with which the classifier can even outperform human performance. Fur\nther, DMC shows noticeable performance in sound localization, multisource detection, and audiovisual\n understanding.", "cite_num": 0}, "1072": {"title": "deep stacked hierarchical multi-patch network for image deblurring", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deep_Stacked_Hierarchical_Multi-Patch_Network_for_Image_Deblurring_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1237": {"title": "feedback network for image super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Feedback_Network_for_Image_Super-Resolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1120": {"title": "scale-adaptive neural dense features: learning via hierarchical context aggregation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Spencer_Scale-Adaptive_Neural_Dense_Features_Learning_via_Hierarchical_Context_Aggregation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "511": {"title": "dula-net: a dual-projection network for estimating room layouts from a single rgb panorama", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_DuLa-Net_A_Dual-Projection_Network_for_Estimating_Room_Layouts_From_a_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "495": {"title": "depth from a polarisation + rgb stereo pair", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Depth_From_a_Polarisation__RGB_Stereo_Pair_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "736": {"title": "a generative appearance model for end-to-end video object segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Johnander_A_Generative_Appearance_Model_for_End-To-End_Video_Object_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "677": {"title": "viewport proposal cnn for 360deg video quality assessment", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Viewport_Proposal_CNN_for_360deg_Video_Quality_Assessment_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "328": {"title": "unos: unified unsupervised optical-flow and stereo-depth estimation by watching videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_UnOS_Unified_Unsupervised_Optical-Flow_and_Stereo-Depth_Estimation_by_Watching_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "242": {"title": "a parametric top-view representation of complex road scenes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_A_Parametric_Top-View_Representation_of_Complex_Road_Scenes_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "980": {"title": "learning to compose dynamic tree structures for visual contexts", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_Learning_to_Compose_Dynamic_Tree_Structures_for_Visual_Contexts_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "408": {"title": "learning to extract flawless slow motion from blurry videos", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jin_Learning_to_Extract_Flawless_Slow_Motion_From_Blurry_Videos_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "915": {"title": "ganfit: generative adversarial network fitting for high fidelity 3d face reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gecer_GANFIT_Generative_Adversarial_Network_Fitting_for_High_Fidelity_3D_Face_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "358": {"title": "re-ranking via metric fusion for object retrieval and person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bai_Re-Ranking_via_Metric_Fusion_for_Object_Retrieval_and_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "205": {"title": "selective kernel networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Selective_Kernel_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1011": {"title": "graph convolutional tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Graph_Convolutional_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "396": {"title": "adaptive nms: refining pedestrian detection in a crowd", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Adaptive_NMS_Refining_Pedestrian_Detection_in_a_Crowd_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1050": {"title": "topnet: structural point cloud decoder", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tchapmi_TopNet_Structural_Point_Cloud_Decoder_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "751": {"title": "single-image piece-wise planar 3d reconstruction via associative embedding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Single-Image_Piece-Wise_Planar_3D_Reconstruction_via_Associative_Embedding_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1049": {"title": "segmentation-driven 6d object pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Segmentation-Driven_6D_Object_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "479": {"title": "re-identification supervised texture generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Re-Identification_Supervised_Texture_Generation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "45": {"title": "completeness modeling and context separation for weakly supervised temporal action localization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Completeness_Modeling_and_Context_Separation_for_Weakly_Supervised_Temporal_Action_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "204": {"title": "which way are you going? imitative decision learning for path forecasting in dynamic scenes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Which_Way_Are_You_Going_Imitative_Decision_Learning_for_Path_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "772": {"title": "feature denoising for improving adversarial robustness", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Feature_Denoising_for_Improving_Adversarial_Robustness_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "233": {"title": "image generation from layout", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Image_Generation_From_Layout_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "588": {"title": "atlas of digital pathology: a generalized hierarchical histological tissue type-annotated database for deep learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hosseini_Atlas_of_Digital_Pathology_A_Generalized_Hierarchical_Histological_Tissue_Type-Annotated_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1208": {"title": "answer them all! toward universal visual question answering models", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shrestha_Answer_Them_All_Toward_Universal_Visual_Question_Answering_Models_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "625": {"title": "laf-net: locally adaptive fusion networks for stereo confidence estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_LAF-Net_Locally_Adaptive_Fusion_Networks_for_Stereo_Confidence_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "220": {"title": "divide and conquer the embedding space for metric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1087": {"title": "seernet: predicting convolutional neural network feature-map sparsity through low-bit quantization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cao_SeerNet_Predicting_Convolutional_Neural_Network_Feature-Map_Sparsity_Through_Low-Bit_Quantization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1241": {"title": "deep single image camera calibration with radial distortion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lopez_Deep_Single_Image_Camera_Calibration_With_Radial_Distortion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "461": {"title": "sdc - stacked dilated convolution: a unified descriptor network for dense matching tasks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Schuster_SDC_-_Stacked_Dilated_Convolution_A_Unified_Descriptor_Network_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "132": {"title": "visual attention consistency under image transforms for multi-label image classification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_Visual_Attention_Consistency_Under_Image_Transforms_for_Multi-Label_Image_Classification_CVPR_2019_paper.html", "abstract": "Human visual perception shows good consistency for many multi-label image classification tasks under\n certain spatial transforms, such as scaling, rotation, flipping and translation. This has motivated\n the data augmentation strategy widely used in CNN classifier training -- transformed images are inc\nluded for training by assuming the same class labels as their original images. In this paper, we fur\nther propose the assumption of perceptual consistency of visual attention regions for classification\n under such transforms, i.e., the attention region for a classification follows the same transform i\nf the input image is spatially transformed. While the attention regions of CNN classifiers can be de\nrived as an attention heatmap in middle layers of the network, we find that their consistency under \nmany transforms are not preserved.  To address this problem, we propose a two-branch network with an\n original image and its transformed image as inputs and introduce a new attention consistency loss t\nhat measures the attention heatmap consistency between two branches. This new loss is then combined \nwith multi-label image classification loss for network training. Experiments on three datasets verif\ny the superiority of the proposed network by achieving new state-of-the-art classification performan\nce.\r", "cite_num": 1}, "990": {"title": "vision-based navigation with language-based assistance via imitation learning with indirect intervention", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Nguyen_Vision-Based_Navigation_With_Language-Based_Assistance_via_Imitation_Learning_With_Indirect_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "199": {"title": "on exploring undetermined relationships for visual relationship detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhan_On_Exploring_Undetermined_Relationships_for_Visual_Relationship_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1031": {"title": "deepmapping: unsupervised map estimation from multiple point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_DeepMapping_Unsupervised_Map_Estimation_From_Multiple_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "862": {"title": "variational bayesian dropout with a hierarchical prior", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Variational_Bayesian_Dropout_With_a_Hierarchical_Prior_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "896": {"title": "underexposed photo enhancement using deep illumination estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Underexposed_Photo_Enhancement_Using_Deep_Illumination_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "417": {"title": "residual networks for light field image super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Residual_Networks_for_Light_Field_Image_Super-Resolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "859": {"title": "unsupervised domain-specific deblurring via disentangled representations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lu_Unsupervised_Domain-Specific_Deblurring_via_Disentangled_Representations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "779": {"title": "skeleton-based action recognition with directed graph neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "680": {"title": "flownet3d: learning scene flow in 3d point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_FlowNet3D_Learning_Scene_Flow_in_3D_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1266": {"title": "end-to-end multi-task learning with attention", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_End-To-End_Multi-Task_Learning_With_Attention_CVPR_2019_paper.html", "abstract": "In this paper, we propose a novel multi-task learning architecture, which incorporates recent advanc\nes in attention mechanisms. Our approach, the Multi-Task Attention Network (MTAN), consists of a sin\ngle shared network containing a global feature pool, together with task-specific soft-attention modu\nles, which are trainable in an end-to-end manner. These attention modules allow for learning of task\n-specific features from the global pool, whilst simultaneously allowing for features to be shared ac\nross different tasks. The architecture can be built upon any feed-forward neural network, is simple \nto implement, and is parameter efficient. Experiments on the CityScapes dataset show that our method\n outperforms several baselines in both single-task and multi-task learning, and is also more robust \nto the various weighting schemes in the multi-task loss function. We further explore the effectivene\nss of our method through experiments over a range of task complexities, and show how our method scal\nes well with task complexity compared to baselines.", "cite_num": 7}, "849": {"title": "joint representation and estimator learning for facial action unit intensity estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Joint_Representation_and_Estimator_Learning_for_Facial_Action_Unit_Intensity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "155": {"title": "focnet: a fractional optimal control network for image denoising", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jia_FOCNet_A_Fractional_Optimal_Control_Network_for_Image_Denoising_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1171": {"title": "a generative adversarial density estimator", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Abbasnejad_A_Generative_Adversarial_Density_Estimator_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "426": {"title": "pushing the envelope for rgb-based dense 3d hand pose estimation via neural rendering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Baek_Pushing_the_Envelope_for_RGB-Based_Dense_3D_Hand_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "57": {"title": "task agnostic meta-learning for few-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jamal_Task_Agnostic_Meta-Learning_for_Few-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "653": {"title": "feature selective anchor-free module for single-shot object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Feature_Selective_Anchor-Free_Module_for_Single-Shot_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "440": {"title": "bidirectional learning for domain adaptation of semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Bidirectional_Learning_for_Domain_Adaptation_of_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "660": {"title": "weakly supervised video moment retrieval from text queries", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mithun_Weakly_Supervised_Video_Moment_Retrieval_From_Text_Queries_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "248": {"title": "efficient video classification using fewer frames", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bhardwaj_Efficient_Video_Classification_Using_Fewer_Frames_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1079": {"title": "robustness of 3d deep learning in an adversarial setting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wicker_Robustness_of_3D_Deep_Learning_in_an_Adversarial_Setting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "268": {"title": "sail-vos: semantic amodal instance level video object segmentation - a synthetic dataset and baselines", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_SAIL-VOS_Semantic_Amodal_Instance_Level_Video_Object_Segmentation_-_A_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "543": {"title": "incremental object learning from contiguous views", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Stojanov_Incremental_Object_Learning_From_Contiguous_Views_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1026": {"title": "feature-level frankenstein: eliminating variations for discriminative recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Feature-Level_Frankenstein_Eliminating_Variations_for_Discriminative_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "636": {"title": "learning channel-wise interactions for binary convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_Channel-Wise_Interactions_for_Binary_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "340": {"title": "iterative normalization: beyond standardization towards efficient whitening", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Huang_Iterative_Normalization_Beyond_Standardization_Towards_Efficient_Whitening_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1139": {"title": "deepview: view synthesis with learned gradient descent", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Flynn_DeepView_View_Synthesis_With_Learned_Gradient_Descent_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "957": {"title": "oicsr: out-in-channel sparsity regularization for compact deep neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_OICSR_Out-In-Channel_Sparsity_Regularization_for_Compact_Deep_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1018": {"title": "language-driven temporal activity localization: a semantic matching reinforcement learning model", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Language-Driven_Temporal_Activity_Localization_A_Semantic_Matching_Reinforcement_Learning_Model_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "893": {"title": "occlusion-net: 2d/3d occluded keypoint localization using graph networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Reddy_Occlusion-Net_2D3D_Occluded_Keypoint_Localization_Using_Graph_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1232": {"title": "wide-context semantic image extrapolation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Wide-Context_Semantic_Image_Extrapolation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "13": {"title": "what's to know? uncertainty as a guide to asking goal-oriented questions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Abbasnejad_Whats_to_Know_Uncertainty_as_a_Guide_to_Asking_Goal-Oriented_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1239": {"title": "leveraging heterogeneous auxiliary tasks to assist crowd counting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Leveraging_Heterogeneous_Auxiliary_Tasks_to_Assist_Crowd_Counting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "566": {"title": "unsupervised image captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Feng_Unsupervised_Image_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "277": {"title": "dense depth posterior (ddp) from single image and sparse range", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_Dense_Depth_Posterior_DDP_From_Single_Image_and_Sparse_Range_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "834": {"title": "large scale incremental learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Large_Scale_Incremental_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "342": {"title": "deep sketch-shape hashing with segmented 3d stochastic viewing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Deep_Sketch-Shape_Hashing_With_Segmented_3D_Stochastic_Viewing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "934": {"title": "character region awareness for text detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Baek_Character_Region_Awareness_for_Text_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "805": {"title": "pifpaf: composite fields for human pose estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kreiss_PifPaf_Composite_Fields_for_Human_Pose_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "724": {"title": "tactical rewind: self-correction via backtracking in vision-and-language navigation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1136": {"title": "veri-wild: a large dataset and a new method for vehicle re-identification in the wild", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lou_VERI-Wild_A_Large_Dataset_and_a_New_Method_for_Vehicle_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1150": {"title": "understanding the disharmony between dropout and batch normalization by variance shift", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "788": {"title": "sophie: an attentive gan for predicting paths compliant to social and physical constraints", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sadeghian_SoPhie_An_Attentive_GAN_for_Predicting_Paths_Compliant_to_Social_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "500": {"title": "vizwiz-priv: a dataset for recognizing the presence and purpose of private visual information in images taken by blind people", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gurari_VizWiz-Priv_A_Dataset_for_Recognizing_the_Presence_and_Purpose_of_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "83": {"title": "hardness-aware deep metric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Hardness-Aware_Deep_Metric_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "940": {"title": "efficient online multi-person 2d pose tracking with recurrent spatio-temporal affinity fields", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Raaj_Efficient_Online_Multi-Person_2D_Pose_Tracking_With_Recurrent_Spatio-Temporal_Affinity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "993": {"title": "curls & whey: boosting black-box adversarial attacks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Curls__Whey_Boosting_Black-Box_Adversarial_Attacks_CVPR_2019_paper.html", "abstract": "Image classifiers based on deep neural networks suffer from harassment caused by adversarial example\ns. Two defects exist in black-box iterative attacks that generate adversarial examples by incrementa\nlly adjusting the noise-adding direction for each step. On the one hand, existing iterative attacks \nadd noises monotonically along the direction of gradient ascent, resulting in a lack of diversity an\nd adaptability of the generated iterative trajectories. On the other hand, it is trivial to perform \nadversarial attack by adding excessive noises, but currently there is no refinement mechanism to squ\neeze redundant noises. In this work, we propose Curls & Whey black-box attack to fix the above two d\nefects. During Curls iteration, by combining gradient ascent and descent, we `curl' up iterative tra\njectories to integrate more diversity and transferability into adversarial examples. Curls iteration\n also alleviates the diminishing marginal effect in existing iterative attacks. The Whey optimizatio\nn further squeezes the `whey' of noises by exploiting the robustness of adversarial perturbation. Ex\ntensive experiments on Imagenet and Tiny-Imagenet demonstrate that our approach achieves impressive \ndecrease on noise magnitude in l2 norm. Curls & Whey attack also shows promising transferability aga\ninst ensemble models as well as adversarially trained models. In addition, we extend our attack to t\nhe targeted misclassification, effectively reducing the difficulty of targeted attacks under black-b\nox condition.", "cite_num": 0}, "455": {"title": "joint manifold diffusion for combining predictions on decoupled observations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Joint_Manifold_Diffusion_for_Combining_Predictions_on_Decoupled_Observations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "672": {"title": "adagraph: unifying predictive and continuous domain adaptation through graphs", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mancini_AdaGraph_Unifying_Predictive_and_Continuous_Domain_Adaptation_Through_Graphs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "12": {"title": "nas-fpn: learning scalable feature pyramid architecture for object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ghiasi_NAS-FPN_Learning_Scalable_Feature_Pyramid_Architecture_for_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "294": {"title": "multi-adversarial discriminative deep domain generalization for face presentation attack detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shao_Multi-Adversarial_Discriminative_Deep_Domain_Generalization_for_Face_Presentation_Attack_Detection_CVPR_2019_paper.html", "abstract": "Face presentation attacks have become an increasingly critical issue in the face recognition communi\nty. Many face anti-spoofing methods have been proposed, but they cannot generalize well on \"unseen\" \nattacks. This work focuses on improving the generalization ability of face anti-spoofing methods fro\nm the perspective of the domain generalization. We propose to learn a generalized feature space via \na novel multi-adversarial discriminative deep domain generalization framework. In this framework, a \nmulti-adversarial deep domain generalization is performed under a dual-force triplet-mining constrai\nnt. This ensures that the learned feature space is discriminative and shared by multiple source doma\nins, and thus is more generalized to new face presentation attacks. An auxiliary face depth supervis\nion is incorporated to further enhance the generalization ability. Extensive experiments on four pub\nlic datasets validate the effectiveness of the proposed method. \r", "cite_num": 0}, "633": {"title": "pushing the boundaries of view extrapolation with multiplane images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Srinivasan_Pushing_the_Boundaries_of_View_Extrapolation_With_Multiplane_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "604": {"title": "event cameras, contrast maximization and reward functions: an analysis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Stoffregen_Event_Cameras_Contrast_Maximization_and_Reward_Functions_An_Analysis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1041": {"title": "minimal solvers for mini-loop closures in 3d multi-scan alignment", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Miraldo_Minimal_Solvers_for_Mini-Loop_Closures_in_3D_Multi-Scan_Alignment_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1186": {"title": "mfas: multimodal fusion architecture search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Perez-Rua_MFAS_Multimodal_Fusion_Architecture_Search_CVPR_2019_paper.html", "abstract": "We tackle the problem of finding good architectures for multimodal classification problems. We propo\nse a novel and generic search space that spans a large number of possible fusion architectures. In o\nrder to find an optimal architecture for a given dataset in the proposed search space, we leverage a\nn efficient sequential model-based exploration approach that is tailored for the problem. We demonst\nrate the value of posing multimodal fusion as a neural architecture search problem by extensive expe\nrimentation on a toy dataset and two other real multimodal datasets. We discover fusion architecture\ns that exhibit state-of-the-art performance for problems with different domain and dataset size, inc\nluding the NTU RGB+D dataset, the largest multi-modal action recognition dataset available.", "cite_num": 0}, "996": {"title": "large-scale few-shot learning: knowledge transfer with class hierarchy", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Large-Scale_Few-Shot_Learning_Knowledge_Transfer_With_Class_Hierarchy_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "661": {"title": "deformable convnets v2: more deformable, better results", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Deformable_ConvNets_V2_More_Deformable_Better_Results_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1047": {"title": "zigzagnet: fusing top-down and bottom-up context for object segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_ZigZagNet_Fusing_Top-Down_and_Bottom-Up_Context_for_Object_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "634": {"title": "travelgan: image-to-image translation by transformation vector learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Amodio_TraVeLGAN_Image-To-Image_Translation_by_Transformation_Vector_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "255": {"title": "object instance annotation with deep extreme level set evolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Object_Instance_Annotation_With_Deep_Extreme_Level_Set_Evolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "297": {"title": "beautyglow: on-demand makeup transfer framework with reversible generative network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_BeautyGlow_On-Demand_Makeup_Transfer_Framework_With_Reversible_Generative_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "323": {"title": "learning 3d human dynamics from video", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kanazawa_Learning_3D_Human_Dynamics_From_Video_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "465": {"title": "blind visual motif removal from a single image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hertz_Blind_Visual_Motif_Removal_From_a_Single_Image_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "518": {"title": "cross domain model compression by structurally weight sharing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1014": {"title": "multi-person articulated tracking with spatial and temporal embeddings", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jin_Multi-Person_Articulated_Tracking_With_Spatial_and_Temporal_Embeddings_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1055": {"title": "unsupervised domain adaptation for tof data denoising with adversarial learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Agresti_Unsupervised_Domain_Adaptation_for_ToF_Data_Denoising_With_Adversarial_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "93": {"title": "attentive relational networks for mapping images to scene graphs", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qi_Attentive_Relational_Networks_for_Mapping_Images_to_Scene_Graphs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1177": {"title": "self-calibrating deep photometric stereo networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Self-Calibrating_Deep_Photometric_Stereo_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "300": {"title": "pointnetlk: robust & efficient point cloud registration using pointnet", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Aoki_PointNetLK_Robust__Efficient_Point_Cloud_Registration_Using_PointNet_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "810": {"title": "timeception for complex action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hussein_Timeception_for_Complex_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "944": {"title": "discovering fair representations in the data domain", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Quadrianto_Discovering_Fair_Representations_in_the_Data_Domain_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "310": {"title": "convolutional relational machine for group activity recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Azar_Convolutional_Relational_Machine_for_Group_Activity_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "824": {"title": "interpretable and fine-grained visual explanations for convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wagner_Interpretable_and_Fine-Grained_Visual_Explanations_for_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1082": {"title": "normalized object coordinate space for category-level 6d object pose and size estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "360": {"title": "dual encoding for zero-example video retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Dual_Encoding_for_Zero-Example_Video_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "730": {"title": "fastdraw: addressing the long tail of lane detection by adapting a sequential prediction network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Philion_FastDraw_Addressing_the_Long_Tail_of_Lane_Detection_by_Adapting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "619": {"title": "arcface: additive angular margin loss for deep face recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "3": {"title": "attention-guided network for ghost-free high dynamic range imaging", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_Attention-Guided_Network_for_Ghost-Free_High_Dynamic_Range_Imaging_CVPR_2019_paper.html", "abstract": "Ghosting artifacts caused by moving objects or misalignments is a key challenge in high dynamic rang\ne (HDR) imaging for dynamic scenes. Previous methods first register the input low dynamic range (LDR\n) images using optical flow before merging them, which are error-prone and cause ghosts in results. \nA very recent work tries to bypass optical flows via a deep network with skip-connections, however, \nwhich still suffers from ghosting artifacts for severe movement. To avoid the ghosting from the sour\nce, we propose a novel attention-guided end-to-end deep neural network (AHDRNet) to produce high-qua\nlity ghost-free HDR images. Unlike previous methods directly stacking the LDR images or features for\n merging, we use attention modules to guide the merging according to the reference image. The attent\nion modules automatically suppress undesired components caused by misalignments and saturation and e\nnhance desirable fine details in the non-reference images. In addition to the attention model, we us\ne dilated residual dense block (DRDB) to make full use of the hierarchical features and increase the\n receptive field for hallucinating the missing details. The proposed AHDRNet is a non-flow-based met\nhod, which can also avoid the artifacts generated by optical-flow estimation error. Experiments on d\nifferent datasets show that the proposed AHDRNet can achieve state-of-the-art quantitative and quali\ntative results.", "cite_num": 0}, "865": {"title": "a local block coordinate descent algorithm for the csc model", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zisselman_A_Local_Block_Coordinate_Descent_Algorithm_for_the_CSC_Model_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "832": {"title": "weakly supervised learning of instance segmentation with inter-pixel relations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ahn_Weakly_Supervised_Learning_of_Instance_Segmentation_With_Inter-Pixel_Relations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "987": {"title": "densely semantically aligned person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Densely_Semantically_Aligned_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "460": {"title": "recurrent attentive zooming for joint crowd counting and precise localization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Recurrent_Attentive_Zooming_for_Joint_Crowd_Counting_and_Precise_Localization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "617": {"title": "meta-sr: a magnification-arbitrary network for super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Meta-SR_A_Magnification-Arbitrary_Network_for_Super-Resolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "708": {"title": "murel: multimodal relational reasoning for visual question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cadene_MUREL_Multimodal_Relational_Reasoning_for_Visual_Question_Answering_CVPR_2019_paper.html", "abstract": "Multimodal attentional networks are currently state-of-the-art models for Visual Question Answering \n(VQA) tasks involving real images. Although attention allows to focus on the visual content relevant\n to the question, this simple mechanism is arguably insufficient to model complex reasoning features\n required for VQA or other high-level tasks. In this paper, we propose MuRel, a multimodal relationa\nl network which is learned end-to-end to reason over real images. Our first contribution is the intr\noduction of the MuRel cell, an atomic reasoning primitive representing interactions between question\n and image regions by a rich vectorial representation, and modeling region relations with pairwise c\nombinations. Secondly, we incorporate the cell into a full MuRel network, which progressively refine\ns visual and question interactions, and can be leveraged to define visualization schemes finer than \nmere attention maps. We validate the relevance of our approach with various ablation studies, and sh\now its superiority to attention-based methods on three datasets: VQA 2.0, VQA-CP v2 and TDIUC. Our f\ninal MuRel network is competitive to or outperforms state-of-the-art results in this challenging con\ntext. Our code is available: https://github.com/Cadene/murel.bootstrap.pytorch", "cite_num": 6}, "713": {"title": "explainability methods for graph convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pope_Explainability_Methods_for_Graph_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "599": {"title": "multi-label image recognition with graph convolutional networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Multi-Label_Image_Recognition_With_Graph_Convolutional_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "286": {"title": "improving action localization by progressive cross-stream cooperation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Su_Improving_Action_Localization_by_Progressive_Cross-Stream_Cooperation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1092": {"title": "fitting multiple heterogeneous models by multi-class cascaded t-linkage", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Magri_Fitting_Multiple_Heterogeneous_Models_by_Multi-Class_Cascaded_T-Linkage_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1111": {"title": "fast and flexible indoor scene synthesis via deep convolutional generative models", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ritchie_Fast_and_Flexible_Indoor_Scene_Synthesis_via_Deep_Convolutional_Generative_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1244": {"title": "diverse generation for multi-agent sports games", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yeh_Diverse_Generation_for_Multi-Agent_Sports_Games_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1054": {"title": "deep reinforcement learning of volume-guided progressive view inpainting for 3d point scene completion from a single depth image", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Han_Deep_Reinforcement_Learning_of_Volume-Guided_Progressive_View_Inpainting_for_3D_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "50": {"title": "chamnet: towards efficient network design through platform-aware model adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dai_ChamNet_Towards_Efficient_Network_Design_Through_Platform-Aware_Model_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "237": {"title": "a theoretically sound upper bound on the triplet loss for improving the efficiency of deep distance metric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Do_A_Theoretically_Sound_Upper_Bound_on_the_Triplet_Loss_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "597": {"title": "structure-preserving stereoscopic view synthesis with multi-scale adversarial correlation matching", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Structure-Preserving_Stereoscopic_View_Synthesis_With_Multi-Scale_Adversarial_Correlation_Matching_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1228": {"title": "spm-tracker: series-parallel matching for real-time visual object tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_SPM-Tracker_Series-Parallel_Matching_for_Real-Time_Visual_Object_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "334": {"title": "max-sliced wasserstein distance and its use for gans", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Deshpande_Max-Sliced_Wasserstein_Distance_and_Its_Use_for_GANs_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "63": {"title": "spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Aafaq_Spatio-Temporal_Dynamics_and_Semantic_Attribute_Enriched_Visual_Encoding_for_Video_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "976": {"title": "unified visual-semantic embeddings: bridging vision and language with structured meaning representations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Unified_Visual-Semantic_Embeddings_Bridging_Vision_and_Language_With_Structured_Meaning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "886": {"title": "group sampling for scale invariant face detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ming_Group_Sampling_for_Scale_Invariant_Face_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "143": {"title": "step: spatio-temporal progressive learning for video action detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yang_STEP_Spatio-Temporal_Progressive_Learning_for_Video_Action_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "753": {"title": "learning multi-class segmentations from single-class datasets", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dmitriev_Learning_Multi-Class_Segmentations_From_Single-Class_Datasets_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "412": {"title": "representation similarity analysis for efficient task taxonomy & transfer learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dwivedi_Representation_Similarity_Analysis_for_Efficient_Task_Taxonomy__Transfer_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1093": {"title": "reducing uncertainty in undersampled mri reconstruction with active acquisition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Reducing_Uncertainty_in_Undersampled_MRI_Reconstruction_With_Active_Acquisition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "558": {"title": "bayesian hierarchical dynamic model for human action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "208": {"title": "exploiting edge features for graph neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gong_Exploiting_Edge_Features_for_Graph_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "734": {"title": "what do single-view 3d reconstruction networks learn?", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tatarchenko_What_Do_Single-View_3D_Reconstruction_Networks_Learn_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "850": {"title": "dlow: domain flow for adaptation and generalization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gong_DLOW_Domain_Flow_for_Adaptation_and_Generalization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "937": {"title": "assisted excitation of activations: a learning technique to improve object detectors", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Derakhshani_Assisted_Excitation_of_Activations_A_Learning_Technique_to_Improve_Object_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "516": {"title": "aird: adversarial learning framework for image repurposing detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jaiswal_AIRD_Adversarial_Learning_Framework_for_Image_Repurposing_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "649": {"title": "propagation mechanism for deep and wide neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Propagation_Mechanism_for_Deep_and_Wide_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "513": {"title": "taking a deeper look at the inverse compositional algorithm", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lv_Taking_a_Deeper_Look_at_the_Inverse_Compositional_Algorithm_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "551": {"title": "learning to transfer examples for partial domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cao_Learning_to_Transfer_Examples_for_Partial_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "364": {"title": "memory-attended recurrent network for video captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pei_Memory-Attended_Recurrent_Network_for_Video_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1012": {"title": "enhancing diversity of defocus blur detectors via cross-ensemble network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Enhancing_Diversity_of_Defocus_Blur_Detectors_via_Cross-Ensemble_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "948": {"title": "audio visual scene-aware dialog", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Alamri_Audio_Visual_Scene-Aware_Dialog_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "900": {"title": "distilling object detectors with fine-grained feature imitation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Distilling_Object_Detectors_With_Fine-Grained_Feature_Imitation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "895": {"title": "attention-based adaptive selection of operations for image restoration in the presence of unknown combined distortions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Suganuma_Attention-Based_Adaptive_Selection_of_Operations_for_Image_Restoration_in_the_CVPR_2019_paper.html", "abstract": "Many studies have been conducted so far on image restoration, the problem of restoring a clean image\n from its distorted version. There are many different types of distortion affecting image quality. P\nrevious studies have focused on single types of distortion, proposing methods for removing them. How\never, image quality degrades due to multiple factors in the real world. Thus, depending on applicati\nons, e.g., vision for autonomous cars or surveillance cameras, we need to be able to deal with multi\nple combined distortions with unknown mixture ratios. For this purpose, we propose a simple yet effe\nctive layer architecture of neural networks. It performs multiple operations in parallel, which are \nweighted by an attention mechanism to enable selection of proper operations depending on the input. \nThe layer can be stacked to form a deep network, which is differentiable and thus can be trained in \nan end-to-end fashion by gradient descent. The experimental results show that the proposed method wo\nrks better than previous methods by a good margin on tasks of restoring images with multiple combine\nd distortions.\r", "cite_num": 1}, "345": {"title": "cascaded projection: end-to-end network compression and acceleration", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Minnehan_Cascaded_Projection_End-To-End_Network_Compression_and_Acceleration_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "190": {"title": "collagan: collaborative gan for missing image data imputation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lee_CollaGAN_Collaborative_GAN_for_Missing_Image_Data_Imputation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "600": {"title": "warpgan: automatic caricature generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_WarpGAN_Automatic_Caricature_Generation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "890": {"title": "contactdb: analyzing and predicting grasp contact via thermal imaging", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Brahmbhatt_ContactDB_Analyzing_and_Predicting_Grasp_Contact_via_Thermal_Imaging_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "654": {"title": "versatile multiple choice learning and its application to vision computing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tian_Versatile_Multiple_Choice_Learning_and_Its_Application_to_Vision_Computing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1089": {"title": "a kernelized manifold mapping to diminish the effect of adversarial perturbations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Taghanaki_A_Kernelized_Manifold_Mapping_to_Diminish_the_Effect_of_Adversarial_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1109": {"title": "mode seeking generative adversarial networks for diverse image synthesis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mao_Mode_Seeking_Generative_Adversarial_Networks_for_Diverse_Image_Synthesis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "651": {"title": "pointrcnn: 3d object proposal generation and detection from point cloud", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_PointRCNN_3D_Object_Proposal_Generation_and_Detection_From_Point_Cloud_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "946": {"title": "inverserendernet: learning single image inverse rendering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_InverseRenderNet_Learning_Single_Image_Inverse_Rendering_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "615": {"title": "face parsing with roi tanh-warping", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_Face_Parsing_With_RoI_Tanh-Warping_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "107": {"title": "irlas: inverse reinforcement learning for architecture search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_IRLAS_Inverse_Reinforcement_Learning_for_Architecture_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "302": {"title": "direct object recognition without line-of-sight using optical coherence", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lei_Direct_Object_Recognition_Without_Line-Of-Sight_Using_Optical_Coherence_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1196": {"title": "dense intrinsic appearance flow for human pose transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Dense_Intrinsic_Appearance_Flow_for_Human_Pose_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "171": {"title": "shieldnets: defending against adversarial attacks using probabilistic adversarial robustness", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Theagarajan_ShieldNets_Defending_Against_Adversarial_Attacks_Using_Probabilistic_Adversarial_Robustness_CVPR_2019_paper.html", "abstract": "Defending adversarial attack is a critical step towards reliable deployment of deep learning empower\ned solutions for industrial applications. Probabilistic adversarial robustness (PAR), as a theoretic\nal framework, is introduced to neutralize adversarial attacks by concentrating sample probability to\n adversarial-free zones. Distinct to most of the existing defense mechanisms that require modifying \nthe architecture/training of the target classifier which is not feasible in the real-world scenario,\n e.g., when a model has already been deployed, PAR is designed in the first place to provide proacti\nve protection to an existing fixed model. ShieldNet is implemented as a demonstration of PAR in this\n work by using PixelCNN. Experimental results show that this approach is generalizable, robust again\nst adversarial transferability and resistant to a wide variety of attacks on the Fashion-MNIST and C\nIFAR10 datasets, respectively.\r", "cite_num": 0}, "999": {"title": "toothnet: automatic tooth instance segmentation and identification from cone beam ct images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cui_ToothNet_Automatic_Tooth_Instance_Segmentation_and_Identification_From_Cone_Beam_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1258": {"title": "learning parallax attention for stereo image super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_Parallax_Attention_for_Stereo_Image_Super-Resolution_CVPR_2019_paper.html", "abstract": "Stereo image pairs can be used to improve the performance of super-resolution (SR) since additional \ninformation is provided from a second viewpoint. However, it is challenging to incorporate this info\nrmation for SR since disparities between stereo images vary significantly. In this paper, we propose\n a parallax-attention stereo superresolution network (PASSRnet) to integrate the information from a \nstereo image pair for SR. Specifically, we introduce a parallax-attention mechanism with a global re\nceptive field along the epipolar line to handle different stereo images with large disparity variati\nons. We also propose a new and the largest dataset for stereo image SR (namely, Flickr1024). Extensi\nve experiments demonstrate that the parallax-attention mechanism can capture correspondence between \nstereo images to improve SR performance with a small computational and memory cost. Comparative resu\nlts show that our PASSRnet achieves the state-of-the-art performance on the Middlebury, KITTI 2012 a\nnd KITTI 2015 datasets.", "cite_num": 1}, "333": {"title": "shapes and context: in-the-wild image synthesis & manipulation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Bansal_Shapes_and_Context_In-The-Wild_Image_Synthesis__Manipulation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "308": {"title": "robust point cloud based reconstruction of large-scale outdoor scenes", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lan_Robust_Point_Cloud_Based_Reconstruction_of_Large-Scale_Outdoor_Scenes_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "510": {"title": "practical full resolution learned lossless image compression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mentzer_Practical_Full_Resolution_Learned_Lossless_Image_Compression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1157": {"title": "learning non-volumetric depth fusion using successive reprojections", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Donne_Learning_Non-Volumetric_Depth_Fusion_Using_Successive_Reprojections_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "932": {"title": "dynamic fusion with intra- and inter-modality attention flow for visual question answering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Dynamic_Fusion_With_Intra-_and_Inter-Modality_Attention_Flow_for_Visual_CVPR_2019_paper.html", "abstract": "Learning effective fusion of multi-modality features is at the heart of visual question answering. W\ne propose a novel method of dynamically fusing multi-modal features with intra- and inter-modality i\nnformation flow, which alternatively pass dynamic information between and across the visual and lang\nuage modalities. It can robustly capture the high-level interactions between language and vision dom\nains, thus significantly improves the performance of visual question answering. We also show that th\ne proposed dynamic intra-modality attention flow conditioned on the other modality can dynamically m\nodulate the intra-modality attention of the target modality, which is vital for multimodality featur\ne fusion. Experimental evaluations on the VQA 2.0 dataset show that the proposed method achieves sta\nte-of-the-art VQA performance. Extensive ablation studies are carried out for the comprehensive anal\nysis of the proposed method.", "cite_num": 3}, "695": {"title": "dfanet: deep feature aggregation for real-time semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_DFANet_Deep_Feature_Aggregation_for_Real-Time_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "827": {"title": "a dataset and benchmark for large-scale multi-modal face anti-spoofing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_A_Dataset_and_Benchmark_for_Large-Scale_Multi-Modal_Face_Anti-Spoofing_CVPR_2019_paper.html", "abstract": "Face anti-spoofing is essential to prevent face recognition systems from a security breach. Much of \nthe progresses have been made by the availability of face anti-spoofing benchmark datasets in recent\n years. However, existing face anti-spoofing benchmarks have limited number of subjects ($\\le\\negmed\nspace170$) and modalities ($\\leq\\negmedspace2$), which hinder the further development of the academi\nc community. To facilitate face anti-spoofing research, we introduce a large-scale multi-modal datas\net, namely CASIA-SURF, which is the largest publicly available dataset for face anti-spoofing in ter\nms of both subjects and visual modalities. Specifically, it consists of $1,000$ subjects with $21,00\n0$ videos and each sample has $3$ modalities (i.e., RGB, Depth and IR). We also provide a measuremen\nt set, evaluation protocol and training/validation/testing subsets, developing a new benchmark for f\nace anti-spoofing. Moreover, we present a new multi-modal fusion method as baseline, which performs \nfeature re-weighting to select the more informative channel features while suppressing the less usef\nul ones for each modal. Extensive experiments have been conducted on the proposed dataset to verify \nits significance and generalization capability. The dataset is available at this https URL", "cite_num": 3}, "909": {"title": "attentive feedback network for boundary-aware salient object detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Feng_Attentive_Feedback_Network_for_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "411": {"title": "detecting overfitting of deep generative networks via latent recovery", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Webster_Detecting_Overfitting_of_Deep_Generative_Networks_via_Latent_Recovery_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1263": {"title": "data representation and learning with graph diffusion-embedding networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Jiang_Data_Representation_and_Learning_With_Graph_Diffusion-Embedding_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1119": {"title": "hyperspectral imaging with random printed mask", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Hyperspectral_Imaging_With_Random_Printed_Mask_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "579": {"title": "3d guided fine-grained face manipulation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Geng_3D_Guided_Fine-Grained_Face_Manipulation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "307": {"title": "fast spatially-varying indoor lighting estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Garon_Fast_Spatially-Varying_Indoor_Lighting_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "174": {"title": "a poisson-gaussian denoising dataset with real fluorescence microscopy images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_A_Poisson-Gaussian_Denoising_Dataset_With_Real_Fluorescence_Microscopy_Images_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "366": {"title": "scenecode: monocular dense semantic reconstruction using learned encoded scene representations", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhi_SceneCode_Monocular_Dense_Semantic_Reconstruction_Using_Learned_Encoded_Scene_Representations_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "164": {"title": "progressive image deraining networks: a better and simpler baseline", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ren_Progressive_Image_Deraining_Networks_A_Better_and_Simpler_Baseline_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "89": {"title": "engaging image captioning via personality", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shuster_Engaging_Image_Captioning_via_Personality_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1213": {"title": "attention-aware multi-stroke style transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yao_Attention-Aware_Multi-Stroke_Style_Transfer_CVPR_2019_paper.html", "abstract": "Neural style transfer has drawn considerable attention from both academic and industrial field. Alth\nough visual effect and efficiency have been significantly improved, existing methods are unable to c\noordinate spatial distribution of visual attention between the content image and stylized image, or \nrender diverse level of detail via different brush strokes. In this paper, we tackle these limitatio\nns by developing an attention-aware multi-stroke style transfer model. We first propose to assemble \nself-attention mechanism into a style-agnostic reconstruction autoencoder framework, from which the \nattention map of a content image can be derived. By performing multi-scale style swap on content fea\ntures and style features, we produce multiple feature maps reflecting different stroke patterns. A f\nlexible fusion strategy is further presented to incorporate the salient characteristics from the att\nention map, which allows integrating multiple stroke patterns into different spatial regions of the \noutput image harmoniously. We demonstrate the effectiveness of our method, as well as generate compa\nrable stylized images with multiple stroke patterns against the state-of-the-art methods.", "cite_num": 0}, "77": {"title": "multi-scale geometric consistency guided multi-view stereo", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xu_Multi-Scale_Geometric_Consistency_Guided_Multi-View_Stereo_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "760": {"title": "blending-target domain adaptation by adversarial meta-adaptation networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Blending-Target_Domain_Adaptation_by_Adversarial_Meta-Adaptation_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "153": {"title": "destruction and construction learning for fine-grained image recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Destruction_and_Construction_Learning_for_Fine-Grained_Image_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "953": {"title": "social-iq: a question answering benchmark for artificial social intelligence", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "620": {"title": "volumetric capture of humans with a single rgbd camera via semi-parametric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pandey_Volumetric_Capture_of_Humans_With_a_Single_RGBD_Camera_via_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "848": {"title": "deep metric learning to rank", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cakir_Deep_Metric_Learning_to_Rank_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1002": {"title": "ok-vqa: a visual question answering benchmark requiring external knowledge", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1250": {"title": "res-pca: a scalable approach to recovering low-rank matrices", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Peng_RES-PCA_A_Scalable_Approach_to_Recovering_Low-Rank_Matrices_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1078": {"title": "learning to reduce dual-level discrepancy for infrared-visible person re-identification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_to_Reduce_Dual-Level_Discrepancy_for_Infrared-Visible_Person_Re-Identification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "923": {"title": "turn a silicon camera into an ingaas camera", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lv_Turn_a_Silicon_Camera_Into_an_InGaAs_Camera_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "972": {"title": "mots: multi-object tracking and segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Voigtlaender_MOTS_Multi-Object_Tracking_and_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "374": {"title": "spatial fusion gan for image synthesis", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhan_Spatial_Fusion_GAN_for_Image_Synthesis_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1209": {"title": "a mutual learning method for salient object detection with intertwined multi-supervision", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_A_Mutual_Learning_Method_for_Salient_Object_Detection_With_Intertwined_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1112": {"title": "attentive region embedding network for zero-shot learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Attentive_Region_Embedding_Network_for_Zero-Shot_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "496": {"title": "second-order attention network for single image super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.html", "abstract": "Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-\nresolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR \nmethods mainly focus on wider or deeper architecture design, neglecting to explore the feature corre\nlations of intermediate layers, hence hindering the representational power of CNNs. To address this \nissue, in this paper, we propose a second-order attention network (SAN) for more powerful feature ex\npression and feature correlation learning. Specifically, a novel train- able second-order channel at\ntention (SOCA) module is developed to adaptively rescale the channel-wise features by using second-o\nrder feature statistics for more discriminative representations. Furthermore, we present a non-local\nly enhanced residual group (NLRG) structure, which not only incorporates non-local operations to cap\nture long-distance spatial contextual information, but also contains repeated local-source residual \nattention groups (LSRAG) to learn increasingly abstract feature representations. Experimental result\ns demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both\n quantitative metrics and visual quality.\r", "cite_num": 2}, "664": {"title": "photo wake-up: 3d character animation from a single photo", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Weng_Photo_Wake-Up_3D_Character_Animation_From_a_Single_Photo_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "546": {"title": "zero-shot task transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pal_Zero-Shot_Task_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1148": {"title": "path-invariant map networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Path-Invariant_Map_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "229": {"title": "deep video inpainting", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Deep_Video_Inpainting_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "791": {"title": "connecting the dots: learning representations for active monocular depth estimation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Riegler_Connecting_the_Dots_Learning_Representations_for_Active_Monocular_Depth_Estimation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "416": {"title": "pyramid feature attention network for saliency detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Pyramid_Feature_Attention_Network_for_Saliency_Detection_CVPR_2019_paper.html", "abstract": "Saliency detection is one of the basic challenges in computer vision. How to extract effective featu\nres is a critical point for saliency detection. Recent methods mainly adopt integrating multi-scale \nconvolutional features indiscriminately. However, not all features are useful for saliency detection\n and some even cause interferences. To solve this problem, we propose Pyramid Feature Attention netw\nork to focus on effective high-level context features and low-level spatial structural features. Fir\nst, we design Context-aware Pyramid Feature Extraction (CPFE) module for multi-scale high-level feat\nure maps to capture rich context features. Second, we adopt channel-wise attention (CA) after CPFE f\neature maps and spatial attention (SA) after low-level feature maps, then fuse outputs of CA & SA to\ngether. Finally, we propose an edge preservation loss to guide network to learn more detailed inform\nation in boundary localization. Extensive evaluations on five benchmark datasets demonstrate that th\ne proposed method outperforms the state-of-the-art approaches under different evaluation metrics.", "cite_num": 2}, "188": {"title": "pointweb: enhancing local neighborhood features for point cloud processing", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_PointWeb_Enhancing_Local_Neighborhood_Features_for_Point_Cloud_Processing_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "908": {"title": "r2gan: cross-modal recipe retrieval with generative adversarial network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_R2GAN_Cross-Modal_Recipe_Retrieval_With_Generative_Adversarial_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "637": {"title": "learning unsupervised video object segmentation through visual attention", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_Unsupervised_Video_Object_Segmentation_Through_Visual_Attention_CVPR_2019_paper.html", "abstract": "This paper conducts a systematic study on the role of visual attention in Unsupervised Video Object \nSegmentation (UVOS) tasks. By elaborately annotating three popular video segmentation datasets (DAVI\nS, Youtube-Objects and SegTrack V2) with dynamic eye-tracking data in the UVOS setting, for the firs\nt time, we quantitatively verified the high consistency of visual attention behavior among human obs\nervers, and found strong correlation between human attention and explicit primary object judgements \nduring dynamic, task-driven viewing. Such novel observations provide an in-depth insight into the un\nderlying rationale behind UVOS. Inspired by these findings, we decouple UVOS into two sub-tasks: UVO\nS-driven Dynamic Visual Attention Prediction (DVAP) in spatiotemporal domain, and Attention-Guided O\nbject Segmentation (AGOS) in spatial domain. Our UVOS solution enjoys three major merits: 1) modular\n training without using expensive video segmentation annotations, instead, using more affordable dyn\namic fixation data to train the initial video attention module and using existing fixation-segmentat\nion paired static/image data to train the subsequent segmentation module; 2) comprehensive foregroun\nd understanding through multi-source learning; and 3) additional interpretability from the biologica\nlly-inspired and assessable attention. Experiments on popular benchmarks show that, even without usi\nng expensive video object mask annotations, our model achieves compelling performance in comparison \nwith state-of-the-arts. \r", "cite_num": 2}, "1163": {"title": "graph convolutional label noise cleaner: train a plug-and-play action classifier for anomaly detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhong_Graph_Convolutional_Label_Noise_Cleaner_Train_a_Plug-And-Play_Action_Classifier_CVPR_2019_paper.html", "abstract": "Video anomaly detection under weak labels is formulated as a typical multiple-instance learning prob\nlem in previous works. In this paper, we provide a new perspective, i.e., a supervised learning task\n under noisy labels. In such a viewpoint, as long as cleaning away label noise, we can directly appl\ny fully supervised action classifiers to weakly supervised anomaly detection, and take maximum advan\ntage of these well-developed classifiers. For this purpose, we devise a graph convolutional network \nto correct noisy labels. Based upon feature similarity and temporal consistency, our network propaga\ntes supervisory signals from high-confidence snippets to low-confidence ones. In this manner, the ne\ntwork is capable of providing cleaned supervision for action classifiers. During the test phase, we \nonly need to obtain snippet-wise predictions from the action classifier without any extra post-proce\nssing. Extensive experiments on 3 datasets at different scales with 2 types of action classifiers de\nmonstrate the efficacy of our method. Remarkably, we obtain the frame-level AUC score of 82.12% on U\nCF-Crime.\r", "cite_num": 0}, "137": {"title": "upsnet: a unified panoptic segmentation network", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Xiong_UPSNet_A_Unified_Panoptic_Segmentation_Network_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "970": {"title": "deeply-supervised knowledge synergy", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Deeply-Supervised_Knowledge_Synergy_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "24": {"title": "advent: adversarial entropy minimization for domain adaptation in semantic segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "587": {"title": "unsupervised learning of action classes with continuous temporal embedding", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kukleva_Unsupervised_Learning_of_Action_Classes_With_Continuous_Temporal_Embedding_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "818": {"title": "multi-granularity generator for temporal action proposal", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Multi-Granularity_Generator_for_Temporal_Action_Proposal_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "246": {"title": "query-guided end-to-end person search", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Munjal_Query-Guided_End-To-End_Person_Search_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "485": {"title": "cascaded generative and discriminative learning for microcalcification detection in breast mammograms", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Cascaded_Generative_and_Discriminative_Learning_for_Microcalcification_Detection_in_Breast_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "331": {"title": "online high rank matrix completion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_Online_High_Rank_Matrix_Completion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "81": {"title": "progressive pose attention transfer for person image generation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Progressive_Pose_Attention_Transfer_for_Person_Image_Generation_CVPR_2019_paper.html", "abstract": "This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the p\nose of a given person to a target pose. The generator of the network comprises a sequence of Pose-At\ntentional Transfer Blocks that each transfers certain regions it attends to, generating the person i\nmage progressively. Compared with those in previous works, our generated person images possess bette\nr appearance consistency and shape consistency with the input images, thus significantly more realis\ntic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively an\nd quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate\n training images for person re-identification, alleviating data insufficiency. Codes and models are \navailable at: this https URL.", "cite_num": 1}, "1114": {"title": "the perfect match: 3d point cloud matching with smoothed densities", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gojcic_The_Perfect_Match_3D_Point_Cloud_Matching_With_Smoothed_Densities_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1123": {"title": "dissecting person re-identification from the viewpoint of viewpoint", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Dissecting_Person_Re-Identification_From_the_Viewpoint_of_Viewpoint_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "482": {"title": "structural relational reasoning of point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Duan_Structural_Relational_Reasoning_of_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "452": {"title": "pixel-adaptive convolutional neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Su_Pixel-Adaptive_Convolutional_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1285": {"title": "recurrent back-projection network for video super-resolution", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Haris_Recurrent_Back-Projection_Network_for_Video_Super-Resolution_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "125": {"title": "knowing when to stop: evaluation and verification of conformity to output-size specifications", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Knowing_When_to_Stop_Evaluation_and_Verification_of_Conformity_to_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "898": {"title": "defending against adversarial attacks by randomized diversification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Taran_Defending_Against_Adversarial_Attacks_by_Randomized_Diversification_CVPR_2019_paper.html", "abstract": "The vulnerability of machine learning systems to adversarial attacks questions their usage in many a\npplications. In this paper, we propose a randomized diversification as a defense strategy. We introd\nuce a multi-channel architecture in a gray-box scenario, which assumes that the architecture of the \nclassifier and the training data set are known to the attacker. The attacker does not only have acce\nss to a secret key and to the internal states of the system at the test time. The defender processes\n an input in multiple channels. Each channel introduces its own randomization in a special transform\n domain based on a secret key shared between the training and testing stages. Such a transform based\n randomization with a shared key preserves the gradients in key-defined sub-spaces for the defender \nbut it prevents gradient back propagation and the creation of various bypass systems for the attacke\nr. An additional benefit of multi-channel randomization is the aggregation that fuses soft-outputs f\nrom all channels, thus increasing the reliability of the final score. The sharing of a secret key cr\neates an information advantage to the defender. Experimental evaluation demonstrates an increased ro\nbustness of the proposed method to a number of known state-of-the-art attacks.\r", "cite_num": 0}, "253": {"title": "large-scale long-tailed recognition in an open world", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Large-Scale_Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "874": {"title": "guided stereo matching", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Poggi_Guided_Stereo_Matching_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "823": {"title": "context-aware spatio-recurrent curvilinear structure segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Context-Aware_Spatio-Recurrent_Curvilinear_Structure_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "353": {"title": "deep geometric prior for surface reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Williams_Deep_Geometric_Prior_for_Surface_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "448": {"title": "reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Reinforced_Cross-Modal_Matching_and_Self-Supervised_Imitation_Learning_for_Vision-Language_Navigation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "503": {"title": "gcan: graph convolutional adversarial network for unsupervised domain adaptation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ma_GCAN_Graph_Convolutional_Adversarial_Network_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "964": {"title": "learning shape-aware embedding for scene text detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tian_Learning_Shape-Aware_Embedding_for_Scene_Text_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "395": {"title": "jumping manifolds: geometry aware dense non-rigid structure from motion", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kumar_Jumping_Manifolds_Geometry_Aware_Dense_Non-Rigid_Structure_From_Motion_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "929": {"title": "mvf-net: multi-view 3d face morphable model regression", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_MVF-Net_Multi-View_3D_Face_Morphable_Model_Regression_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "318": {"title": "synthesizing 3d shapes from silhouette image collections using multi-projection generative adversarial networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Synthesizing_3D_Shapes_From_Silhouette_Image_Collections_Using_Multi-Projection_Generative_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "871": {"title": "towards visual feature translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Towards_Visual_Feature_Translation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1288": {"title": "catastrophic child's play: easy to perform, hard to defend adversarial attacks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ho_Catastrophic_Childs_Play_Easy_to_Perform_Hard_to_Defend_Adversarial_CVPR_2019_paper.html", "abstract": "The problem of adversarial CNN attacks is considered, with an emphasis on attacks that are trivial t\no perform but difficult to defend. A framework for the study of such attacks is proposed, using real\n world object manipulations. Unlike most works in the past, this framework supports the design of at\ntacks based on both small and large image perturbations, implemented by camera shake and pose variat\nion. A setup is proposed for the collection of such perturbations and determination of their percept\nibility. It is argued that perceptibility depends on context, and a distinction is made between impe\nrceptible and semantically imperceptible perturbations. While the former survives image comparisons,\n the latter are perceptible but have no impact on human object recognition. A procedure is proposed \nto determine the perceptibility of perturbations using Turk experiments, and a dataset of both pertu\nrbation classes which enables replicable studies of object manipulation attacks, is assembled. Exper\niments using defenses based on many datasets, CNN models, and algorithms from the literature elucida\nte the difficulty of defending these attacks -- in fact, none of the existing defenses is found effe\nctive against them. Better results are achieved with real world data augmentation, but even this is \nnot foolproof. These results confirm the hypothesis that current CNNs are vulnerable to attacks impl\nementable even by a child, and that such attacks may prove difficult to defend.\r", "cite_num": 0}, "689": {"title": "streamlined dense video captioning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mun_Streamlined_Dense_Video_Captioning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "128": {"title": "dyntypo: example-based dynamic text effects transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Men_DynTypo_Example-Based_Dynamic_Text_Effects_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "363": {"title": "mbs: macroblock scaling for cnn model reduction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_MBS_Macroblock_Scaling_for_CNN_Model_Reduction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "486": {"title": "attention branch network: learning of attention mechanism for visual explanation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019_paper.html", "abstract": "Visual explanation enables human to understand the decision making of Deep Convolutional Neural Netw\nork (CNN), but it is insufficient to contribute the performance improvement. In this paper, we focus\n on the attention map for visual explanation, which represents high response value as the important \nregion in image recognition. This region significantly improves the performance of CNN by introducin\ng an attention mechanism that focuses on a specific region in an image. In this work, we propose Att\nention Branch Network (ABN), which extends the top-down visual explanation model by introducing a br\nanch structure with an attention mechanism. ABN can be applicable to several image recognition tasks\n by introducing a branch for attention mechanism and is trainable for the visual explanation and ima\nge recognition in end-to-end manner. We evaluate ABN on several image recognition tasks such as imag\ne classification, fine-grained recognition, and multiple facial attributes recognition. Experimental\n results show that ABN can outperform the accuracy of baseline models on these image recognition tas\nks while generating an attention map for visual explanation. Our code is available at this https URL\n.", "cite_num": 3}, "157": {"title": "ranked list loss for deep metric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Ranked_List_Loss_for_Deep_Metric_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "694": {"title": "filterreg: robust and efficient probabilistic point-set registration using gaussian filter and twist parameterization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_FilterReg_Robust_and_Efficient_Probabilistic_Point-Set_Registration_Using_Gaussian_Filter_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "415": {"title": "in defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Orsic_In_Defense_of_Pre-Trained_ImageNet_Architectures_for_Real-Time_Semantic_Segmentation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "738": {"title": "lifting vectorial variational problems: a natural formulation based on geometric measure theory and discrete exterior calculus", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Mollenhoff_Lifting_Vectorial_Variational_Problems_A_Natural_Formulation_Based_on_Geometric_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "370": {"title": "data-driven neuron allocation for scale aggregation networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Data-Driven_Neuron_Allocation_for_Scale_Aggregation_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "95": {"title": "towards high-fidelity nonlinear 3d face morphable model", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tran_Towards_High-Fidelity_Nonlinear_3D_Face_Morphable_Model_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "431": {"title": "lasot: a high-quality benchmark for large-scale single object tracking", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fan_LaSOT_A_High-Quality_Benchmark_for_Large-Scale_Single_Object_Tracking_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1216": {"title": "l3-net: towards learning based lidar localization for autonomous driving", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Lu_L3-Net_Towards_Learning_Based_LiDAR_Localization_for_Autonomous_Driving_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "289": {"title": "mantra-net: manipulation tracing network for detection and localization of image forgeries with anomalous features", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_ManTra-Net_Manipulation_Tracing_Network_for_Detection_and_Localization_of_Image_CVPR_2019_paper.html", "abstract": "To fight against real-life image forgery, which  commonly involves different types and combined mani\npulations, we propose a unified deep neural architecture called ManTra-Net. Unlike many existing sol\nutions, ManTra-Net is an end-to-end network that performs both detection and localization without ex\ntra preprocessing and postprocessing. \\manifold   is a fully convolutional network and handles image\ns of arbitrary sizes and many known forgery types such splicing, copy-move, removal, enhancement, an\nd even unknown types. This paper has three salient contributions. We design a simple yet effective s\nelf-supervised learning task to learn robust image manipulation traces from classifying 385 image ma\nnipulation types. Further, we formulate the forgery localization problem as a local anomaly detectio\nn problem, design a Z-score feature to capture local anomaly, and propose a novel long short-term me\nmory solution to assess local anomalies. Finally, we carefully conduct ablation experiments to syste\nmatically optimize the proposed network design. Our extensive experimental results demonstrate the g\neneralizability, robustness and superiority of ManTra-Net, not only in single types of manipulations\n/forgeries, but also in their complicated combinations. \r", "cite_num": 1}, "883": {"title": "cross-modal self-attention network for referring image segmentation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ye_Cross-Modal_Self-Attention_Network_for_Referring_Image_Segmentation_CVPR_2019_paper.html", "abstract": "We consider the problem of referring image segmentation. Given an input image and a natural language\n expression, the goal is to segment the object referred by the language expression in the image. Exi\nsting works in this area treat the language expression and the input image separately in their repre\nsentations. They do not sufficiently capture long-range correlations between these two modalities. I\nn this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the lo\nng-range dependencies between linguistic and visual features. Our model can adaptively focus on info\nrmative words in the referring expression and important regions in the input image. In addition, we \npropose a gated multi-level fusion module to selectively integrate self-attentive cross-modal featur\nes corresponding to different levels in the image. This module controls the information flow of feat\nures at different levels. We validate the proposed approach on four evaluation datasets. Our propose\nd approach consistently outperforms existing state-of-the-art methods.", "cite_num": 0}, "1280": {"title": "towards instance-level image-to-image translation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shen_Towards_Instance-Level_Image-To-Image_Translation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "429": {"title": "world from blur", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Qiu_World_From_Blur_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "698": {"title": "lasernet: an efficient probabilistic 3d object detector for autonomous driving", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Meyer_LaserNet_An_Efficient_Probabilistic_3D_Object_Detector_for_Autonomous_Driving_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "819": {"title": "an efficient schmidt-ekf for 3d visual-inertial slam", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Geneva_An_Efficient_Schmidt-EKF_for_3D_Visual-Inertial_SLAM_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "221": {"title": "centripetal sgd for pruning very deep convolutional networks with complicated structure", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Centripetal_SGD_for_Pruning_Very_Deep_Convolutional_Networks_With_Complicated_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "829": {"title": "mixture density generative adversarial networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Eghbal-zadeh_Mixture_Density_Generative_Adversarial_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "912": {"title": "learning to learn from noisy labeled data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_to_Learn_From_Noisy_Labeled_Data_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "70": {"title": "fast object class labelling via speech", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gygli_Fast_Object_Class_Labelling_via_Speech_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1242": {"title": "additive adversarial learning for unbiased authentication", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liang_Additive_Adversarial_Learning_for_Unbiased_Authentication_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1167": {"title": "self-supervised gans via auxiliary rotation loss", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Self-Supervised_GANs_via_Auxiliary_Rotation_Loss_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "803": {"title": "video relationship reasoning using gated spatio-temporal energy graph", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tsai_Video_Relationship_Reasoning_Using_Gated_Spatio-Temporal_Energy_Graph_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "86": {"title": "multi-similarity loss with general pair weighting for deep metric learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "630": {"title": "nm-net: mining reliable neighbors for robust feature correspondences", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_NM-Net_Mining_Reliable_Neighbors_for_Robust_Feature_Correspondences_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "685": {"title": "efficient multi-domain learning by covariance normalization", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Efficient_Multi-Domain_Learning_by_Covariance_Normalization_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "84": {"title": "style transfer by relaxed optimal transport and self-similarity", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kolkin_Style_Transfer_by_Relaxed_Optimal_Transport_and_Self-Similarity_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "822": {"title": "scene memory transformer for embodied agents in long-horizon tasks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fang_Scene_Memory_Transformer_for_Embodied_Agents_in_Long-Horizon_Tasks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "463": {"title": "temporal cycle-consistency learning", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dwibedi_Temporal_Cycle-Consistency_Learning_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "7": {"title": "mitigating information leakage in image representations: a maximum entropy approach", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Roy_Mitigating_Information_Leakage_in_Image_Representations_A_Maximum_Entropy_Approach_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "534": {"title": "disentangling latent space for vae by label relevant/irrelevant dimensions", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Disentangling_Latent_Space_for_VAE_by_Label_RelevantIrrelevant_Dimensions_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "607": {"title": "semantically tied paired cycle consistency for zero-shot sketch-based image retrieval", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Dutta_Semantically_Tied_Paired_Cycle_Consistency_for_Zero-Shot_Sketch-Based_Image_Retrieval_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "521": {"title": "object detection with location-aware deformable convolution and backward attention filtering", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Object_Detection_With_Location-Aware_Deformable_Convolution_and_Backward_Attention_Filtering_CVPR_2019_paper.html", "abstract": "Multi-class and multi-scale object detection for autonomous driving is challenging because of the hi\ngh variation in object scales and the cluttered background in complex street scenes. Context informa\ntion and high-resolution features are the keys to achieve a good performance in multi-scale object d\netection. However, context information is typically unevenly distributed, and the high-resolution fe\nature map also contains distractive low-level features. In this paper, we propose a location-aware d\neformable convolution and a backward attention filtering to improve the detection performance. The l\nocation-aware deformable convolution extracts the unevenly distributed context features by sampling \nthe input from where informative context exists. Different from the original deformable convolution,\n the proposed method applies an individual convolutional layer on each input sampling grid location \nto obtain a wide and unique receptive field for a better offset estimation. Meanwhile, the backward \nattention filtering module filters the high-resolution feature map by highlighting the informative f\neatures and suppressing the distractive features using the semantic features from the deep layers. E\nxtensive experiments are conducted on the KITTI object detection and PASCAL VOC 2007 datasets. The p\nroposed method shows an average 6% performance improvement over the Faster R-CNN baseline, and it ha\ns the top-3 performance on the KITTI leaderboard with the fastest processing speed.\r", "cite_num": 1}, "687": {"title": "fast, diverse and accurate image captioning guided by part-of-speech", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Deshpande_Fast_Diverse_and_Accurate_Image_Captioning_Guided_by_Part-Of-Speech_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "55": {"title": "scene graph generation with external knowledge and image reconstruction", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Gu_Scene_Graph_Generation_With_External_Knowledge_and_Image_Reconstruction_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "149": {"title": "visual question answering as reading comprehension", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Visual_Question_Answering_as_Reading_Comprehension_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1020": {"title": "ip102: a large-scale benchmark dataset for insect pest recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_IP102_A_Large-Scale_Benchmark_Dataset_for_Insect_Pest_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1019": {"title": "pde acceleration for active contours", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yezzi_PDE_Acceleration_for_Active_Contours_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "491": {"title": "bi-directional cascade network for perceptual edge detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/He_Bi-Directional_Cascade_Network_for_Perceptual_Edge_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "749": {"title": "aet vs. aed: unsupervised representation learning by auto-encoding transformations rather than data", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_AET_vs._AED_Unsupervised_Representation_Learning_by_Auto-Encoding_Transformations_Rather_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1291": {"title": "semantic attribute matching networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Semantic_Attribute_Matching_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1090": {"title": "uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yasarla_Uncertainty_Guided_Multi-Scale_Residual_Learning-Using_a_Cycle_Spinning_CNN_for_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "774": {"title": "does learning specific features for related parts help human pose estimation?", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_Does_Learning_Specific_Features_for_Related_Parts_Help_Human_Pose_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1183": {"title": "a-cnn: annularly convolutional neural networks on point clouds", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Komarichev_A-CNN_Annularly_Convolutional_Neural_Networks_on_Point_Clouds_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "312": {"title": "mars: motion-augmented rgb stream for action recognition", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Crasto_MARS_Motion-Augmented_RGB_Stream_for_Action_Recognition_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1201": {"title": "texture mixer: a network for controllable synthesis and interpolation of texture", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Texture_Mixer_A_Network_for_Controllable_Synthesis_and_Interpolation_of_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "41": {"title": "normalized diversification", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Normalized_Diversification_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "379": {"title": "self-supervised representation learning from videos for facial action unit detection", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Self-Supervised_Representation_Learning_From_Videos_for_Facial_Action_Unit_Detection_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "549": {"title": "robustness via curvature regularization, and vice versa", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Moosavi-Dezfooli_Robustness_via_Curvature_Regularization_and_Vice_Versa_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "1240": {"title": "fully learnable group convolution for acceleration of deep neural networks", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Fully_Learnable_Group_Convolution_for_Acceleration_of_Deep_Neural_Networks_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "675": {"title": "few-shot learning via saliency-guided hallucination of samples", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Few-Shot_Learning_via_Saliency-Guided_Hallucination_of_Samples_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "611": {"title": "analysis of feature visibility in non-line-of-sight measurements", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Analysis_of_Feature_Visibility_in_Non-Line-Of-Sight_Measurements_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "141": {"title": "image-to-image translation via group-wise deep whitening-and-coloring transformation", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Cho_Image-To-Image_Translation_via_Group-Wise_Deep_Whitening-And-Coloring_Transformation_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "835": {"title": "3d shape reconstruction from images in the frequency domain", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shen_3D_Shape_Reconstruction_From_Images_in_the_Frequency_Domain_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}, "839": {"title": "characterizing and avoiding negative transfer", "conf": "cvpr", "time": "2019", "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Characterizing_and_Avoiding_Negative_Transfer_CVPR_2019_paper.html", "abstract": "", "cite_num": -1}}